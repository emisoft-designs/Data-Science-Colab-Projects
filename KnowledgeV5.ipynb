{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJ5VNQKXC+zTYrp03OYgQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a34c33307085495a8960e397e6c30d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0e2f95e4fc94d99ac8f89b239247b08",
              "IPY_MODEL_3529f81b1a88419fa30b805f109fdb7c",
              "IPY_MODEL_d0de3718bada4b1eab223ab18187521d"
            ],
            "layout": "IPY_MODEL_9feb7d78450c494992a7d832931460ee"
          }
        },
        "c0e2f95e4fc94d99ac8f89b239247b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d5ee859848b4189a381d9f69fe080e0",
            "placeholder": "​",
            "style": "IPY_MODEL_c9a19e5bc4b04537b417a3823e2a2b6e",
            "value": "modules.json: 100%"
          }
        },
        "3529f81b1a88419fa30b805f109fdb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f953e4a4871b4aa98ce7276dd4b729c9",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ba1aec5204e4277a15b533fc1112ae7",
            "value": 349
          }
        },
        "d0de3718bada4b1eab223ab18187521d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_320e0ee72487490b8108d4aa28eb5ac9",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8bce9d72a445d5895e2857d8c589e7",
            "value": " 349/349 [00:00&lt;00:00, 4.97kB/s]"
          }
        },
        "9feb7d78450c494992a7d832931460ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5ee859848b4189a381d9f69fe080e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a19e5bc4b04537b417a3823e2a2b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f953e4a4871b4aa98ce7276dd4b729c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba1aec5204e4277a15b533fc1112ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "320e0ee72487490b8108d4aa28eb5ac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8bce9d72a445d5895e2857d8c589e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95f139ecb0ae467dbf42df209bf3ae39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_349efce6a70d4be5898b79e3e2736742",
              "IPY_MODEL_0eef8acd6bcc40099675efa1c8ca1609",
              "IPY_MODEL_421f43d48bde4c3bb5a82cfca79acb9e"
            ],
            "layout": "IPY_MODEL_d8365cfc84dc413981801ee559fba898"
          }
        },
        "349efce6a70d4be5898b79e3e2736742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bca4041a3484f1b918ebb9bc7bfc675",
            "placeholder": "​",
            "style": "IPY_MODEL_63d28d5e8e804c929562adb3235a6d14",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "0eef8acd6bcc40099675efa1c8ca1609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0dc5ce151754fc58b1620c5ce194c3b",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ae836f8eced42eda9e4d0f0fda1236f",
            "value": 116
          }
        },
        "421f43d48bde4c3bb5a82cfca79acb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9db6316c8168404ebc43048694c95e57",
            "placeholder": "​",
            "style": "IPY_MODEL_d70f9edb1bd34e25b9d21dcf3b6b80c8",
            "value": " 116/116 [00:00&lt;00:00, 1.69kB/s]"
          }
        },
        "d8365cfc84dc413981801ee559fba898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bca4041a3484f1b918ebb9bc7bfc675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d28d5e8e804c929562adb3235a6d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0dc5ce151754fc58b1620c5ce194c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae836f8eced42eda9e4d0f0fda1236f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9db6316c8168404ebc43048694c95e57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70f9edb1bd34e25b9d21dcf3b6b80c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b20b01a1dbc478e915debe07f286ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c323a0f8e7454d99ae68e3f6c549a7f6",
              "IPY_MODEL_742320fa08c1467096f2838dcdde277c",
              "IPY_MODEL_8946c29ccaa64506a1b590a0eaed21cf"
            ],
            "layout": "IPY_MODEL_39fdba2c84d34e11aae5846a2ad15454"
          }
        },
        "c323a0f8e7454d99ae68e3f6c549a7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6172661d3fa54a68888d823c0c2e99b7",
            "placeholder": "​",
            "style": "IPY_MODEL_1a98ba308b35422484ced396b9bb4961",
            "value": "README.md: "
          }
        },
        "742320fa08c1467096f2838dcdde277c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c34484431b844c948917a6cf1a7e4690",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9376ef40167e4a13a800d7c82e32916f",
            "value": 1
          }
        },
        "8946c29ccaa64506a1b590a0eaed21cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbf9a70cfe87439f9e6ee5049f4299e7",
            "placeholder": "​",
            "style": "IPY_MODEL_47080339f68c454aaf8768a4d993e92a",
            "value": " 10.5k/? [00:00&lt;00:00, 390kB/s]"
          }
        },
        "39fdba2c84d34e11aae5846a2ad15454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6172661d3fa54a68888d823c0c2e99b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a98ba308b35422484ced396b9bb4961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c34484431b844c948917a6cf1a7e4690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9376ef40167e4a13a800d7c82e32916f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbf9a70cfe87439f9e6ee5049f4299e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47080339f68c454aaf8768a4d993e92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e88437076932416db2a7d588e6891a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ec7b76b6f174060a10f2a57424eff60",
              "IPY_MODEL_00e6217bf0d14888ad2482e23777012f",
              "IPY_MODEL_fe144c0d777a4734ab68def71bd7799b"
            ],
            "layout": "IPY_MODEL_11e4abbd8aca46828c8d511243699b2a"
          }
        },
        "2ec7b76b6f174060a10f2a57424eff60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_322031b9108a47a7bf200c2bf609890a",
            "placeholder": "​",
            "style": "IPY_MODEL_e3fde20920a742e997ec2598c5fd92b2",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "00e6217bf0d14888ad2482e23777012f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78e9aeab4bf43b286acbe693dcb736b",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_216ec0928e9446d8824884a86fc10ca2",
            "value": 53
          }
        },
        "fe144c0d777a4734ab68def71bd7799b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f04ebb9a10c4cdc90ead1e7e5a99a64",
            "placeholder": "​",
            "style": "IPY_MODEL_a0c587ebdf034f6c8ed579a3a811e27b",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.00kB/s]"
          }
        },
        "11e4abbd8aca46828c8d511243699b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "322031b9108a47a7bf200c2bf609890a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3fde20920a742e997ec2598c5fd92b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a78e9aeab4bf43b286acbe693dcb736b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216ec0928e9446d8824884a86fc10ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f04ebb9a10c4cdc90ead1e7e5a99a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0c587ebdf034f6c8ed579a3a811e27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ee96df05cea4f9380d62182f96a642f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fb1b368d7254497854b324f4007abe6",
              "IPY_MODEL_80c768be7a164e0bac3b68f3fe1aeb07",
              "IPY_MODEL_568b88ff3a2f450299d9aa1b78f73c9e"
            ],
            "layout": "IPY_MODEL_c816c02a29df4b89a3f3565ea23375d7"
          }
        },
        "6fb1b368d7254497854b324f4007abe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_725a8cce6e0e44f18fcacda976c3ffd1",
            "placeholder": "​",
            "style": "IPY_MODEL_549eff9327e64c47ad7907fe38d2cc05",
            "value": "config.json: 100%"
          }
        },
        "80c768be7a164e0bac3b68f3fe1aeb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990c1d772b434e4f9ecfb8738a3c2a03",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_185b54273a2044079d780d11a9500704",
            "value": 612
          }
        },
        "568b88ff3a2f450299d9aa1b78f73c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b448818aafc9431f92978193cdbb2cb5",
            "placeholder": "​",
            "style": "IPY_MODEL_04d13d7758274ace907e8da56966b7a3",
            "value": " 612/612 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "c816c02a29df4b89a3f3565ea23375d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725a8cce6e0e44f18fcacda976c3ffd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549eff9327e64c47ad7907fe38d2cc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990c1d772b434e4f9ecfb8738a3c2a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185b54273a2044079d780d11a9500704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b448818aafc9431f92978193cdbb2cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04d13d7758274ace907e8da56966b7a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b11ee23318844c58a878c9ffb559f56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf4b96c375c74c8ca89bc938589df7ed",
              "IPY_MODEL_37b50529814144fc97fc58dc33e6d469",
              "IPY_MODEL_a7d77ab526924517b785ea7981663c40"
            ],
            "layout": "IPY_MODEL_ca5d7bd558f64799a96d506735ce5813"
          }
        },
        "cf4b96c375c74c8ca89bc938589df7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eac4cc519f2544829733aab9a8ca7804",
            "placeholder": "​",
            "style": "IPY_MODEL_387f9ca9fd46450793f8ef7595989005",
            "value": "model.safetensors: 100%"
          }
        },
        "37b50529814144fc97fc58dc33e6d469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec0d5b2d746b48c7b022e0687e4d0d8b",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_569ea0c879a64d76952376a2db53cedf",
            "value": 90868376
          }
        },
        "a7d77ab526924517b785ea7981663c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81e249e4926b4c0b9f71be9862ea7013",
            "placeholder": "​",
            "style": "IPY_MODEL_7fde0b2f453c41389821b71710cc1c0a",
            "value": " 90.9M/90.9M [00:02&lt;00:00, 44.3MB/s]"
          }
        },
        "ca5d7bd558f64799a96d506735ce5813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac4cc519f2544829733aab9a8ca7804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "387f9ca9fd46450793f8ef7595989005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec0d5b2d746b48c7b022e0687e4d0d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569ea0c879a64d76952376a2db53cedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81e249e4926b4c0b9f71be9862ea7013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fde0b2f453c41389821b71710cc1c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a82fd26a6e4247bf9207e16f2bb9e9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a119359d887e43c781bad7be141d8f07",
              "IPY_MODEL_01831bd0fde249b19ebd5abeb79be431",
              "IPY_MODEL_0bdea72b0b6045f9b708bdee6e7edf30"
            ],
            "layout": "IPY_MODEL_e1c65ecd2c9246ed96d2546978e64c23"
          }
        },
        "a119359d887e43c781bad7be141d8f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dd484d3a6fe40c894c7aee8be17f670",
            "placeholder": "​",
            "style": "IPY_MODEL_15969498e5354e83ab0ac097528702b3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "01831bd0fde249b19ebd5abeb79be431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4165c974551444f68b725ece5639d7f6",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a71903e4b784b339ffec5bdf05b902e",
            "value": 350
          }
        },
        "0bdea72b0b6045f9b708bdee6e7edf30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c613e8135bf94ed0924869e6367405f3",
            "placeholder": "​",
            "style": "IPY_MODEL_3b4f779fb9304ee1b27f7ec837c3856a",
            "value": " 350/350 [00:00&lt;00:00, 10.9kB/s]"
          }
        },
        "e1c65ecd2c9246ed96d2546978e64c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd484d3a6fe40c894c7aee8be17f670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15969498e5354e83ab0ac097528702b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4165c974551444f68b725ece5639d7f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a71903e4b784b339ffec5bdf05b902e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c613e8135bf94ed0924869e6367405f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4f779fb9304ee1b27f7ec837c3856a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c309d9c8eef944a2a94feddd7874efe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ec73344747a4619899fc373bf1dc332",
              "IPY_MODEL_2cec7f71dce74089873cb4b02036d37b",
              "IPY_MODEL_907e891e1fd94c04937161d584c6b263"
            ],
            "layout": "IPY_MODEL_e0842178c1864442832031e84a7a1ec6"
          }
        },
        "0ec73344747a4619899fc373bf1dc332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bfd91cb0e274a2e8ab734e42c09416d",
            "placeholder": "​",
            "style": "IPY_MODEL_adcab987b9f64737a6892e03db27c0bf",
            "value": "vocab.txt: "
          }
        },
        "2cec7f71dce74089873cb4b02036d37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb25cb2ca834f02b4376859f27bbc30",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb8cf83372fc4fb1997f093199786df5",
            "value": 1
          }
        },
        "907e891e1fd94c04937161d584c6b263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ed29ec725b4cafb263f9f508187078",
            "placeholder": "​",
            "style": "IPY_MODEL_09a88f22505148a5baf83ec179fed14f",
            "value": " 232k/? [00:00&lt;00:00, 6.56MB/s]"
          }
        },
        "e0842178c1864442832031e84a7a1ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bfd91cb0e274a2e8ab734e42c09416d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcab987b9f64737a6892e03db27c0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfb25cb2ca834f02b4376859f27bbc30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "eb8cf83372fc4fb1997f093199786df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50ed29ec725b4cafb263f9f508187078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a88f22505148a5baf83ec179fed14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b762e34190ff487384f9769bb8e1148c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce735fcc42504efbbde84fee8504e983",
              "IPY_MODEL_6569985f7b8a477594189567eb22cbb1",
              "IPY_MODEL_2b0fdf71b343430eb5f0dc0a6b78cf16"
            ],
            "layout": "IPY_MODEL_1fe3c2117f61434ca5bbfe8935b9d9c9"
          }
        },
        "ce735fcc42504efbbde84fee8504e983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_142f2af375724467963ff2be33301e7f",
            "placeholder": "​",
            "style": "IPY_MODEL_dc5401f4a52245a297b2dbc947ebcf5b",
            "value": "tokenizer.json: "
          }
        },
        "6569985f7b8a477594189567eb22cbb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9737f64ef7a64ec88a7621f2856a91cd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_350abd38a2c34beeaaa998cc69654471",
            "value": 1
          }
        },
        "2b0fdf71b343430eb5f0dc0a6b78cf16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02fc548c4384412ca1c0a0208c9c28ff",
            "placeholder": "​",
            "style": "IPY_MODEL_360b5cfb3b474ab79059ac3879174389",
            "value": " 466k/? [00:00&lt;00:00, 12.2MB/s]"
          }
        },
        "1fe3c2117f61434ca5bbfe8935b9d9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142f2af375724467963ff2be33301e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5401f4a52245a297b2dbc947ebcf5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9737f64ef7a64ec88a7621f2856a91cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "350abd38a2c34beeaaa998cc69654471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02fc548c4384412ca1c0a0208c9c28ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "360b5cfb3b474ab79059ac3879174389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94cba47c20444e93a8654be666b62f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6422fc079cf4c968e1dfc6b723aa842",
              "IPY_MODEL_a4c52cb833e743bd8f0896483c8c4187",
              "IPY_MODEL_d2c2af10c1f343338dea7465be577618"
            ],
            "layout": "IPY_MODEL_9e6a72f7bcfb4be7b4f37083bc5e4ad7"
          }
        },
        "c6422fc079cf4c968e1dfc6b723aa842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3ba73a27884366a86fc0ba245d70f9",
            "placeholder": "​",
            "style": "IPY_MODEL_1719805f51d44d3da6059ac3864cbc3e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a4c52cb833e743bd8f0896483c8c4187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6862c33425314d8588e4cb4f5751b5b0",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_171927929b294200a5ab377594212111",
            "value": 112
          }
        },
        "d2c2af10c1f343338dea7465be577618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_108b88346b564b13a1b18dd800ae0aaf",
            "placeholder": "​",
            "style": "IPY_MODEL_027effddfb9b443fa711d287830a3e40",
            "value": " 112/112 [00:00&lt;00:00, 2.42kB/s]"
          }
        },
        "9e6a72f7bcfb4be7b4f37083bc5e4ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3ba73a27884366a86fc0ba245d70f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1719805f51d44d3da6059ac3864cbc3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6862c33425314d8588e4cb4f5751b5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171927929b294200a5ab377594212111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "108b88346b564b13a1b18dd800ae0aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "027effddfb9b443fa711d287830a3e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8d05b85200645e3bd18708ee43101c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eaf7305ccc146b3aed624a186f989f3",
              "IPY_MODEL_736ff54c037f402495e0b9286cb5d986",
              "IPY_MODEL_c31140b678c447b0a91599b780ff4a1a"
            ],
            "layout": "IPY_MODEL_49723a234ffd44779ce6239deba94d30"
          }
        },
        "5eaf7305ccc146b3aed624a186f989f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d4584ae09464a59bfe8f68a904668b5",
            "placeholder": "​",
            "style": "IPY_MODEL_fe308000d6e54428b0ea1cfdb0ae5ed0",
            "value": "config.json: 100%"
          }
        },
        "736ff54c037f402495e0b9286cb5d986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_880d2b7ea9cc486686b12f6d625be1b8",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f59c4a3d3050475fb770de8540d5be2a",
            "value": 190
          }
        },
        "c31140b678c447b0a91599b780ff4a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87694666ddd495093465d0756dd7cfd",
            "placeholder": "​",
            "style": "IPY_MODEL_9b14a72a825840eda896378880d32671",
            "value": " 190/190 [00:00&lt;00:00, 6.83kB/s]"
          }
        },
        "49723a234ffd44779ce6239deba94d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4584ae09464a59bfe8f68a904668b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe308000d6e54428b0ea1cfdb0ae5ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "880d2b7ea9cc486686b12f6d625be1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f59c4a3d3050475fb770de8540d5be2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e87694666ddd495093465d0756dd7cfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b14a72a825840eda896378880d32671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c43fc0ed6ef64f0592c2b6b2bb76a4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8eb83d28f020491db2ab864b470d419b",
              "IPY_MODEL_500aad3548df45d9b2a1226f3d07efec",
              "IPY_MODEL_0f0b92d2c6ce458e986ab7d13461d9c3"
            ],
            "layout": "IPY_MODEL_0758d5c0e3b845c3809de26aa487732b"
          }
        },
        "8eb83d28f020491db2ab864b470d419b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b21c1d993634c7db1084d5dc2fc40bf",
            "placeholder": "​",
            "style": "IPY_MODEL_084594d9fbfa4a07aeaeaa8a61df0136",
            "value": "config.json: 100%"
          }
        },
        "500aad3548df45d9b2a1226f3d07efec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ba1fe3bd34d4b69bcb9244622db524c",
            "max": 794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7ab5fb87d544cdea2e9924b386a42a6",
            "value": 794
          }
        },
        "0f0b92d2c6ce458e986ab7d13461d9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16eacf920a424bc1bb6c1889c907d601",
            "placeholder": "​",
            "style": "IPY_MODEL_8397f5ef7492477fac90bf560f63e898",
            "value": " 794/794 [00:00&lt;00:00, 14.7kB/s]"
          }
        },
        "0758d5c0e3b845c3809de26aa487732b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b21c1d993634c7db1084d5dc2fc40bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084594d9fbfa4a07aeaeaa8a61df0136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ba1fe3bd34d4b69bcb9244622db524c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ab5fb87d544cdea2e9924b386a42a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16eacf920a424bc1bb6c1889c907d601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8397f5ef7492477fac90bf560f63e898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea0195b5a95c4fafb603154fae3a6248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c77b3bbb15c54c16971009ee426f83c6",
              "IPY_MODEL_a20a2b08e5eb4477980bd254f79d917e",
              "IPY_MODEL_46d1a68153de44529259e3e0de51226f"
            ],
            "layout": "IPY_MODEL_934c3e4eacf54530ae164e66001ceba1"
          }
        },
        "c77b3bbb15c54c16971009ee426f83c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c25b66088234796a60480d8601015f3",
            "placeholder": "​",
            "style": "IPY_MODEL_37d95a6a436d44e3a45ae55fe665a8bb",
            "value": "model.safetensors: 100%"
          }
        },
        "a20a2b08e5eb4477980bd254f79d917e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90faab42c0d0475cae2ae66d8f41a241",
            "max": 90870598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d99089bd52d4c229cc375f22ef07218",
            "value": 90870598
          }
        },
        "46d1a68153de44529259e3e0de51226f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5be8355a38b471a8f1c225acaf72538",
            "placeholder": "​",
            "style": "IPY_MODEL_09451c43b1c044c79b92e73981997ba7",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 120MB/s]"
          }
        },
        "934c3e4eacf54530ae164e66001ceba1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c25b66088234796a60480d8601015f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d95a6a436d44e3a45ae55fe665a8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90faab42c0d0475cae2ae66d8f41a241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d99089bd52d4c229cc375f22ef07218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5be8355a38b471a8f1c225acaf72538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09451c43b1c044c79b92e73981997ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c31d1f431e641cf81643b5ec5f612a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74837bc120424e928ecdb037a9238352",
              "IPY_MODEL_206efe85c5e140dfa4d40b4611acd890",
              "IPY_MODEL_d43114d432e348e495ca84ec5a9ce6db"
            ],
            "layout": "IPY_MODEL_a06b4eff3fae4527b84819e94f315c95"
          }
        },
        "74837bc120424e928ecdb037a9238352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_871c5474c2e64ca0a27040d7ab70f45c",
            "placeholder": "​",
            "style": "IPY_MODEL_c0014831e693473ebaba6771bb7c2825",
            "value": "tokenizer_config.json: "
          }
        },
        "206efe85c5e140dfa4d40b4611acd890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6003c258ecb64e76af3d4da2eab9dc07",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_971c4d87f79749aeb924eb2f7761ce1d",
            "value": 1
          }
        },
        "d43114d432e348e495ca84ec5a9ce6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_502d89a99d9f4330bae07cfa82820545",
            "placeholder": "​",
            "style": "IPY_MODEL_f7152627fd834111a959d469a75d85cb",
            "value": " 1.33k/? [00:00&lt;00:00, 97.4kB/s]"
          }
        },
        "a06b4eff3fae4527b84819e94f315c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "871c5474c2e64ca0a27040d7ab70f45c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0014831e693473ebaba6771bb7c2825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6003c258ecb64e76af3d4da2eab9dc07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "971c4d87f79749aeb924eb2f7761ce1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "502d89a99d9f4330bae07cfa82820545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7152627fd834111a959d469a75d85cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f318e8e6361147feaa65af5c3dc68ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f45512ced9b4a22973a5cb8497527e0",
              "IPY_MODEL_06c22e52a2bb490286ebcac623614b5e",
              "IPY_MODEL_88bfceae64ee4c948094adbabda63c12"
            ],
            "layout": "IPY_MODEL_f782dc3339d64b4a8b3ec7c09dfddd2c"
          }
        },
        "1f45512ced9b4a22973a5cb8497527e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_672496579c7f4a11ae82c988e108f1f4",
            "placeholder": "​",
            "style": "IPY_MODEL_eefb1c9e157c4fd98145431edc32fe64",
            "value": "vocab.txt: "
          }
        },
        "06c22e52a2bb490286ebcac623614b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8524c9bed8d044779ce6ad6dd7f016ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20a02167e1644e138c62f23f270e7cc7",
            "value": 1
          }
        },
        "88bfceae64ee4c948094adbabda63c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20325a31c64b4bfc8ecf0d42545e1bf0",
            "placeholder": "​",
            "style": "IPY_MODEL_9cc6a46ef3734974bc2aa5994cfccaa0",
            "value": " 232k/? [00:00&lt;00:00, 10.6MB/s]"
          }
        },
        "f782dc3339d64b4a8b3ec7c09dfddd2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672496579c7f4a11ae82c988e108f1f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eefb1c9e157c4fd98145431edc32fe64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8524c9bed8d044779ce6ad6dd7f016ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "20a02167e1644e138c62f23f270e7cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20325a31c64b4bfc8ecf0d42545e1bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc6a46ef3734974bc2aa5994cfccaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "055bc5f370924dd289d0f0b30f1b5b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67188750c3c04773b51dbc1b0a9d974d",
              "IPY_MODEL_ad7c2a78f3f0470da285be6c4fbcc5d6",
              "IPY_MODEL_fd04dfb2beaa4170b8dcb3c5ba2b1f56"
            ],
            "layout": "IPY_MODEL_0863a9fe744d441eab598ae129bb92bb"
          }
        },
        "67188750c3c04773b51dbc1b0a9d974d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaee311f64c64768839c4ee5437921c4",
            "placeholder": "​",
            "style": "IPY_MODEL_2944002807624a4a8fdc49839153a511",
            "value": "tokenizer.json: "
          }
        },
        "ad7c2a78f3f0470da285be6c4fbcc5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f89c8a6907324710967596e367975e18",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a754f31c70584e5ab94d72b7db133982",
            "value": 1
          }
        },
        "fd04dfb2beaa4170b8dcb3c5ba2b1f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c6ca074fafd437ea1aee5874f0c6f3e",
            "placeholder": "​",
            "style": "IPY_MODEL_9aaee21e5dad42e0957fb285e2136f65",
            "value": " 711k/? [00:00&lt;00:00, 28.7MB/s]"
          }
        },
        "0863a9fe744d441eab598ae129bb92bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaee311f64c64768839c4ee5437921c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2944002807624a4a8fdc49839153a511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f89c8a6907324710967596e367975e18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a754f31c70584e5ab94d72b7db133982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c6ca074fafd437ea1aee5874f0c6f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aaee21e5dad42e0957fb285e2136f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5eba0eeb8cf47d2a50ba3c6df9d2b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8891744b556d432c8036a3f2dd4a9a1a",
              "IPY_MODEL_9edb8cfdeaaa4306898b2989d4431ab5",
              "IPY_MODEL_58e60ee1337c4ae895be2779ca51bee4"
            ],
            "layout": "IPY_MODEL_d11af611256a4e6e9fe6b42a621ec0c0"
          }
        },
        "8891744b556d432c8036a3f2dd4a9a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb97fbe54ab14cd5b09d2667da90c06c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0571f545a5d4754adba33eadc249efb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9edb8cfdeaaa4306898b2989d4431ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13f126f544914d279731dfb1f2fa035c",
            "max": 132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fb29954f02f47c78da4e1d4585557e9",
            "value": 132
          }
        },
        "58e60ee1337c4ae895be2779ca51bee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c0cc24718114d5787ca1e9625cb0996",
            "placeholder": "​",
            "style": "IPY_MODEL_8ba0d36cd03943618c1c8e905fcc234c",
            "value": " 132/132 [00:00&lt;00:00, 5.70kB/s]"
          }
        },
        "d11af611256a4e6e9fe6b42a621ec0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb97fbe54ab14cd5b09d2667da90c06c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0571f545a5d4754adba33eadc249efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13f126f544914d279731dfb1f2fa035c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb29954f02f47c78da4e1d4585557e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c0cc24718114d5787ca1e9625cb0996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba0d36cd03943618c1c8e905fcc234c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd5bbf23e5c441929dda4fe055e77257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7a8627af22a40c88bd9e1aa0fca8da4",
              "IPY_MODEL_d9a44da2172d46338e8615a7076833b1",
              "IPY_MODEL_597f3b99796d4127bb12373a281e4eab"
            ],
            "layout": "IPY_MODEL_3b8c498861f344879c840957684ca868"
          }
        },
        "e7a8627af22a40c88bd9e1aa0fca8da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1381b201992449a09cf59fb603dadeaa",
            "placeholder": "​",
            "style": "IPY_MODEL_d3b270e73de74e8e991f81dccd86522f",
            "value": "README.md: "
          }
        },
        "d9a44da2172d46338e8615a7076833b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76b33d984cb5446ab07e8d53f7016603",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df8c27ace2d04f2a9509725f33974401",
            "value": 1
          }
        },
        "597f3b99796d4127bb12373a281e4eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c302459864f4a1bac64c31510d8740e",
            "placeholder": "​",
            "style": "IPY_MODEL_fbbae93c15144cd6805f39bdaf12dd2d",
            "value": " 3.67k/? [00:00&lt;00:00, 199kB/s]"
          }
        },
        "3b8c498861f344879c840957684ca868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1381b201992449a09cf59fb603dadeaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b270e73de74e8e991f81dccd86522f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76b33d984cb5446ab07e8d53f7016603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "df8c27ace2d04f2a9509725f33974401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c302459864f4a1bac64c31510d8740e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbbae93c15144cd6805f39bdaf12dd2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48aee96ef4284ea69e9567ee98de2f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a05647a0c0646e3a1a971e828a495a0",
              "IPY_MODEL_20d80d1da769415a8775a52a45f8b498",
              "IPY_MODEL_b47b2af5544d423990acdc903d5d0d61"
            ],
            "layout": "IPY_MODEL_d2a0a6f99cc0451286c85f5228c2e7b9"
          }
        },
        "8a05647a0c0646e3a1a971e828a495a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b7dd9a7568433095acfcffcfa7585c",
            "placeholder": "​",
            "style": "IPY_MODEL_96637a09c66748acbc8b996f26e5ecdd",
            "value": "config.json: "
          }
        },
        "20d80d1da769415a8775a52a45f8b498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1e4bb10b40942c89d7960eef222b688",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d11e546d74184d39a5aeddb74c447a5e",
            "value": 1
          }
        },
        "b47b2af5544d423990acdc903d5d0d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c30d7f2f594f1bba11cbfc14c9c8d3",
            "placeholder": "​",
            "style": "IPY_MODEL_3eb299df070248b0a4a726497b16cf09",
            "value": " 1.40k/? [00:00&lt;00:00, 54.8kB/s]"
          }
        },
        "d2a0a6f99cc0451286c85f5228c2e7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b7dd9a7568433095acfcffcfa7585c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96637a09c66748acbc8b996f26e5ecdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1e4bb10b40942c89d7960eef222b688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d11e546d74184d39a5aeddb74c447a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64c30d7f2f594f1bba11cbfc14c9c8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb299df070248b0a4a726497b16cf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e055b6adc084f799320b4a7d699e6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf5765c765064d1e8ccd715e34a92538",
              "IPY_MODEL_1b6f18ba72e6484d9b3b5e9d77f9840d",
              "IPY_MODEL_9788667a42434f69aa15ded458855e3b"
            ],
            "layout": "IPY_MODEL_b27447578f1d401e91ca0853bf2a95f1"
          }
        },
        "cf5765c765064d1e8ccd715e34a92538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b216fac131430a9c54326d1c7f9ba7",
            "placeholder": "​",
            "style": "IPY_MODEL_4fa1f9b6491548689ebac70d7fbadb21",
            "value": "model.safetensors: 100%"
          }
        },
        "1b6f18ba72e6484d9b3b5e9d77f9840d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0513b07a524c1db890289369ad1284",
            "max": 307867048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cf2c48aae094c64878b52afee485528",
            "value": 307867048
          }
        },
        "9788667a42434f69aa15ded458855e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78bb44aa43de43b48e196087a01d870c",
            "placeholder": "​",
            "style": "IPY_MODEL_43ac9b08fefb4b5faf0c2bd7800a8718",
            "value": " 308M/308M [00:10&lt;00:00, 37.0MB/s]"
          }
        },
        "b27447578f1d401e91ca0853bf2a95f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b216fac131430a9c54326d1c7f9ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fa1f9b6491548689ebac70d7fbadb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c0513b07a524c1db890289369ad1284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf2c48aae094c64878b52afee485528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78bb44aa43de43b48e196087a01d870c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ac9b08fefb4b5faf0c2bd7800a8718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7879d115c8a491f9c39598eb1f7c122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_407b0df4e70c45d295ef832ef6cb82b7",
              "IPY_MODEL_b71cac7bfd0f41e59a004f1e705a2865",
              "IPY_MODEL_f73876bd83df4f4fa0379fa9d64bf5bb"
            ],
            "layout": "IPY_MODEL_cbcf8c3718484584995dff37df9fd1e0"
          }
        },
        "407b0df4e70c45d295ef832ef6cb82b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f73738847047449cbc853783c53ce9",
            "placeholder": "​",
            "style": "IPY_MODEL_80e1a701bf81440a940c3842205402e6",
            "value": "generation_config.json: 100%"
          }
        },
        "b71cac7bfd0f41e59a004f1e705a2865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39440e494f5949d08fcf3321a038a8ac",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d072a7ebeec04ec080c020675022c42f",
            "value": 147
          }
        },
        "f73876bd83df4f4fa0379fa9d64bf5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfc0086732cb4897883418ff28c85b76",
            "placeholder": "​",
            "style": "IPY_MODEL_516cf95f2f6a4805af43d9d0ee16e12d",
            "value": " 147/147 [00:00&lt;00:00, 8.05kB/s]"
          }
        },
        "cbcf8c3718484584995dff37df9fd1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f73738847047449cbc853783c53ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e1a701bf81440a940c3842205402e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39440e494f5949d08fcf3321a038a8ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d072a7ebeec04ec080c020675022c42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfc0086732cb4897883418ff28c85b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516cf95f2f6a4805af43d9d0ee16e12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab28391ab438435ba16607ba37917fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7392602663ff435d85b4f72c5e60095b",
              "IPY_MODEL_018049319e964f78a9e2f99b3ff7991e",
              "IPY_MODEL_d837c139bcef4b9daf6d96ffa4f399b4"
            ],
            "layout": "IPY_MODEL_94b095e943834484bc4670d6721975a8"
          }
        },
        "7392602663ff435d85b4f72c5e60095b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ffc41ac0ba42a8ab71692abad897d4",
            "placeholder": "​",
            "style": "IPY_MODEL_f6cc98b32c5d4e73bccafed3bcdd86b8",
            "value": "tokenizer_config.json: "
          }
        },
        "018049319e964f78a9e2f99b3ff7991e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02713cb4a0004862970db8633776aad3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_881b2e121ff443b0919b5e3552c4d577",
            "value": 1
          }
        },
        "d837c139bcef4b9daf6d96ffa4f399b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_651c3f97d47e4578adeb823849f49e73",
            "placeholder": "​",
            "style": "IPY_MODEL_7adef4e845de4eb0b49b3ed2de0c1b3b",
            "value": " 2.54k/? [00:00&lt;00:00, 212kB/s]"
          }
        },
        "94b095e943834484bc4670d6721975a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ffc41ac0ba42a8ab71692abad897d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6cc98b32c5d4e73bccafed3bcdd86b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02713cb4a0004862970db8633776aad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "881b2e121ff443b0919b5e3552c4d577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "651c3f97d47e4578adeb823849f49e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7adef4e845de4eb0b49b3ed2de0c1b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4eb114a770a4983999e1aae24e0890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_672e7a32e0aa47079f473acbe16a45e4",
              "IPY_MODEL_5a302c6873e6474ca7f817a40ed393d4",
              "IPY_MODEL_2ce4b9fa3e5f49d291b4d8794b43c754"
            ],
            "layout": "IPY_MODEL_aabaf60fc6b34d94afaefbb100405d7c"
          }
        },
        "672e7a32e0aa47079f473acbe16a45e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1a21fb750814d3abf9afd1e668cb83f",
            "placeholder": "​",
            "style": "IPY_MODEL_fdc5b979b25e4f9380dace1b0fdd33f0",
            "value": "spiece.model: 100%"
          }
        },
        "5a302c6873e6474ca7f817a40ed393d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_525279f42dae4f3f92ac3d29dfef8f84",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5485101168484494bb561407b662efff",
            "value": 791656
          }
        },
        "2ce4b9fa3e5f49d291b4d8794b43c754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6cdacaea2b74485a61d1446968b9ddf",
            "placeholder": "​",
            "style": "IPY_MODEL_e952bcdcf91445feb94058ca50b83d10",
            "value": " 792k/792k [00:00&lt;00:00, 519kB/s]"
          }
        },
        "aabaf60fc6b34d94afaefbb100405d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a21fb750814d3abf9afd1e668cb83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc5b979b25e4f9380dace1b0fdd33f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "525279f42dae4f3f92ac3d29dfef8f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5485101168484494bb561407b662efff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6cdacaea2b74485a61d1446968b9ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e952bcdcf91445feb94058ca50b83d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67ee3abeb0bb4f4ab97bc806f109dbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3672a385c4354685902ff1dcd821ad74",
              "IPY_MODEL_078eecc29c844a989bbe1f28a70b9be6",
              "IPY_MODEL_3d58dddab7d94a57a6aac28fd0600fa3"
            ],
            "layout": "IPY_MODEL_c24448ea77fa466a9fc324b00ddc223c"
          }
        },
        "3672a385c4354685902ff1dcd821ad74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82f1aa058954d2f8715b127fc1e12f0",
            "placeholder": "​",
            "style": "IPY_MODEL_4d34f1510a4d4490b392e5e706901a1a",
            "value": "tokenizer.json: "
          }
        },
        "078eecc29c844a989bbe1f28a70b9be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e3f24bf3f704e5d8f0e92bab8387d51",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e33a7cd1b97a48568dd81cb0d57aa24a",
            "value": 1
          }
        },
        "3d58dddab7d94a57a6aac28fd0600fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39c571aa0ca847d8b1e148bd9ad64bc2",
            "placeholder": "​",
            "style": "IPY_MODEL_da3c01fcf6d6413abea1825a003a8c78",
            "value": " 2.42M/? [00:00&lt;00:00, 56.9MB/s]"
          }
        },
        "c24448ea77fa466a9fc324b00ddc223c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c82f1aa058954d2f8715b127fc1e12f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d34f1510a4d4490b392e5e706901a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e3f24bf3f704e5d8f0e92bab8387d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e33a7cd1b97a48568dd81cb0d57aa24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39c571aa0ca847d8b1e148bd9ad64bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da3c01fcf6d6413abea1825a003a8c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12dd75fcfb7946e78c34595ef81a0e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86cd5a0a248e4b32af7de30299ec4d6b",
              "IPY_MODEL_929b3c61dd2e44038ab33ed1ea9ff6b6",
              "IPY_MODEL_0ba88763e4f54a79bfa69d74e37ab933"
            ],
            "layout": "IPY_MODEL_2e71117f5cf742d5aedce7fe34bf7607"
          }
        },
        "86cd5a0a248e4b32af7de30299ec4d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b872686b124f35a4667ad4bdccac1b",
            "placeholder": "​",
            "style": "IPY_MODEL_b6a2e6736fed4fd8b4536b0f910ea6d4",
            "value": "special_tokens_map.json: "
          }
        },
        "929b3c61dd2e44038ab33ed1ea9ff6b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b73a2f68d59640e5af16fd27707c9c3b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea3df9cc35634a7495d54d870cc38110",
            "value": 1
          }
        },
        "0ba88763e4f54a79bfa69d74e37ab933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_266fe36f4abb44f59672843f61ac3209",
            "placeholder": "​",
            "style": "IPY_MODEL_455542ee6a7a483d9d2b9cef8fe9fa49",
            "value": " 2.20k/? [00:00&lt;00:00, 167kB/s]"
          }
        },
        "2e71117f5cf742d5aedce7fe34bf7607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b872686b124f35a4667ad4bdccac1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a2e6736fed4fd8b4536b0f910ea6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b73a2f68d59640e5af16fd27707c9c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ea3df9cc35634a7495d54d870cc38110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "266fe36f4abb44f59672843f61ac3209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455542ee6a7a483d9d2b9cef8fe9fa49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emisoft-designs/Data-Science-Colab-Projects/blob/main/KnowledgeV5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QxPRI45o8P3",
        "outputId": "401c0fe5-a400-477e-cde5-a516bdec6a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/447.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.9/444.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling dependencies...\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,008 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,281 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,327 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,798 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,642 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,496 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [71.0 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,691 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.1 kB]\n",
            "Fetched 35.7 MB in 4s (10.1 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk-bridge2.0-0 set to manually installed.\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatk1.0-0 set to manually installed.\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libatspi2.0-0 set to manually installed.\n",
            "libcairo-gobject2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo-gobject2 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libxcb-shm0 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb-shm0 set to manually installed.\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 set to manually installed.\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcomposite1 set to manually installed.\n",
            "libxcursor1 is already the newest version (1:1.2.0-2build4).\n",
            "libxcursor1 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxi6 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libxrender1 is already the newest version (1:0.9.10-1build4).\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.12).\n",
            "libcups2 set to manually installed.\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdbus-1-3 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.3).\n",
            "libfreetype6 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libgdk-pixbuf-2.0-0 is already the newest version (2.42.8+dfsg-1ubuntu0.4).\n",
            "libgdk-pixbuf-2.0-0 set to manually installed.\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.6).\n",
            "libgtk-3-0 is already the newest version (3.24.33-1ubuntu2.2).\n",
            "libgtk-3-0 set to manually installed.\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnspr4 set to manually installed.\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libnss3 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "libpangocairo-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpangocairo-1.0-0 set to manually installed.\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-client0 set to manually installed.\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-6 set to manually installed.\n",
            "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-xcb1 set to manually installed.\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.15).\n",
            "The following additional packages will be installed:\n",
            "  xfonts-encodings xfonts-utils\n",
            "Recommended packages:\n",
            "  fonts-ipafont-mincho fonts-tlwg-loma\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf fonts-ipafont-gothic fonts-noto-color-emoji\n",
            "  fonts-tlwg-loma-otf fonts-unifont fonts-wqy-zenhei libdbus-glib-1-2 libxtst6\n",
            "  xfonts-cyrillic xfonts-encodings xfonts-scalable xfonts-utils\n",
            "0 upgraded, 12 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 68.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ipafont-gothic all 00303-21ubuntu1 [3,513 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.047-0ubuntu0.22.04.1 [10.0 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-tlwg-loma-otf all 1:0.7.3-1 [107 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-unifont all 1:14.0.01-1 [3,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-wqy-zenhei all 0.9.45-8 [7,472 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdbus-glib-1-2 amd64 0.112-2build1 [65.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 xfonts-cyrillic all 1:1.0.5 [386 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-scalable all 1:1.0.3-1.2ubuntu1 [306 kB]\n",
            "Fetched 28.5 MB in 2s (11.4 MB/s)\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 126435 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-ipafont-gothic_00303-21ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../01-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../02-fonts-noto-color-emoji_2.047-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-tlwg-loma-otf.\n",
            "Preparing to unpack .../03-fonts-tlwg-loma-otf_1%3a0.7.3-1_all.deb ...\n",
            "Unpacking fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Selecting previously unselected package fonts-unifont.\n",
            "Preparing to unpack .../04-fonts-unifont_1%3a14.0.01-1_all.deb ...\n",
            "Unpacking fonts-unifont (1:14.0.01-1) ...\n",
            "Selecting previously unselected package fonts-wqy-zenhei.\n",
            "Preparing to unpack .../05-fonts-wqy-zenhei_0.9.45-8_all.deb ...\n",
            "Unpacking fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "Preparing to unpack .../06-libdbus-glib-1-2_0.112-2build1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../07-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../08-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../09-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-cyrillic.\n",
            "Preparing to unpack .../10-xfonts-cyrillic_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-cyrillic (1:1.0.5) ...\n",
            "Selecting previously unselected package xfonts-scalable.\n",
            "Preparing to unpack .../11-xfonts-scalable_1%3a1.0.3-1.2ubuntu1_all.deb ...\n",
            "Unpacking xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Setting up fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up fonts-unifont (1:14.0.01-1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-cyrillic (1:1.0.5) ...\n",
            "Setting up xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Downloading Chromium 140.0.7339.16 (playwright build v1187)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1187/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G173.7 MiB [] 0% 132.2s\u001b[0K\u001b[1G173.7 MiB [] 0% 37.0s\u001b[0K\u001b[1G173.7 MiB [] 0% 18.0s\u001b[0K\u001b[1G173.7 MiB [] 0% 7.4s\u001b[0K\u001b[1G173.7 MiB [] 1% 6.1s\u001b[0K\u001b[1G173.7 MiB [] 1% 5.3s\u001b[0K\u001b[1G173.7 MiB [] 2% 5.1s\u001b[0K\u001b[1G173.7 MiB [] 2% 4.6s\u001b[0K\u001b[1G173.7 MiB [] 3% 4.3s\u001b[0K\u001b[1G173.7 MiB [] 3% 4.0s\u001b[0K\u001b[1G173.7 MiB [] 4% 3.8s\u001b[0K\u001b[1G173.7 MiB [] 4% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 5% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 5% 3.9s\u001b[0K\u001b[1G173.7 MiB [] 6% 3.7s\u001b[0K\u001b[1G173.7 MiB [] 6% 3.9s\u001b[0K\u001b[1G173.7 MiB [] 7% 3.7s\u001b[0K\u001b[1G173.7 MiB [] 7% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 8% 3.5s\u001b[0K\u001b[1G173.7 MiB [] 8% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 9% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 9% 3.7s\u001b[0K\u001b[1G173.7 MiB [] 10% 3.6s\u001b[0K\u001b[1G173.7 MiB [] 11% 3.4s\u001b[0K\u001b[1G173.7 MiB [] 12% 3.2s\u001b[0K\u001b[1G173.7 MiB [] 13% 3.1s\u001b[0K\u001b[1G173.7 MiB [] 14% 3.0s\u001b[0K\u001b[1G173.7 MiB [] 15% 2.9s\u001b[0K\u001b[1G173.7 MiB [] 16% 2.8s\u001b[0K\u001b[1G173.7 MiB [] 17% 2.8s\u001b[0K\u001b[1G173.7 MiB [] 18% 2.7s\u001b[0K\u001b[1G173.7 MiB [] 19% 2.6s\u001b[0K\u001b[1G173.7 MiB [] 20% 2.6s\u001b[0K\u001b[1G173.7 MiB [] 20% 2.5s\u001b[0K\u001b[1G173.7 MiB [] 20% 2.6s\u001b[0K\u001b[1G173.7 MiB [] 21% 2.5s\u001b[0K\u001b[1G173.7 MiB [] 22% 2.5s\u001b[0K\u001b[1G173.7 MiB [] 22% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 23% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 24% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 24% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 25% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 25% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 26% 2.4s\u001b[0K\u001b[1G173.7 MiB [] 27% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 28% 2.3s\u001b[0K\u001b[1G173.7 MiB [] 29% 2.2s\u001b[0K\u001b[1G173.7 MiB [] 30% 2.2s\u001b[0K\u001b[1G173.7 MiB [] 31% 2.1s\u001b[0K\u001b[1G173.7 MiB [] 32% 2.1s\u001b[0K\u001b[1G173.7 MiB [] 33% 2.0s\u001b[0K\u001b[1G173.7 MiB [] 34% 2.0s\u001b[0K\u001b[1G173.7 MiB [] 35% 2.0s\u001b[0K\u001b[1G173.7 MiB [] 36% 1.9s\u001b[0K\u001b[1G173.7 MiB [] 37% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 38% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 39% 1.8s\u001b[0K\u001b[1G173.7 MiB [] 39% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 40% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 41% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 42% 1.7s\u001b[0K\u001b[1G173.7 MiB [] 43% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 44% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 45% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 45% 1.6s\u001b[0K\u001b[1G173.7 MiB [] 46% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 47% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 48% 1.5s\u001b[0K\u001b[1G173.7 MiB [] 49% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 50% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 51% 1.4s\u001b[0K\u001b[1G173.7 MiB [] 52% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 53% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 54% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 55% 1.3s\u001b[0K\u001b[1G173.7 MiB [] 56% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 57% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 58% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 59% 1.2s\u001b[0K\u001b[1G173.7 MiB [] 59% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 60% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 61% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 62% 1.1s\u001b[0K\u001b[1G173.7 MiB [] 62% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 63% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 64% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 65% 1.0s\u001b[0K\u001b[1G173.7 MiB [] 66% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 67% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 68% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 69% 0.9s\u001b[0K\u001b[1G173.7 MiB [] 69% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 70% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 71% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 72% 0.8s\u001b[0K\u001b[1G173.7 MiB [] 73% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 74% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 75% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 76% 0.7s\u001b[0K\u001b[1G173.7 MiB [] 77% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 78% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 79% 0.6s\u001b[0K\u001b[1G173.7 MiB [] 80% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 81% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 82% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 83% 0.5s\u001b[0K\u001b[1G173.7 MiB [] 84% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 85% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 86% 0.4s\u001b[0K\u001b[1G173.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 89% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 90% 0.3s\u001b[0K\u001b[1G173.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 93% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 94% 0.2s\u001b[0K\u001b[1G173.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 97% 0.1s\u001b[0K\u001b[1G173.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G173.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 140.0.7339.16 (playwright build v1187) downloaded to /root/.cache/ms-playwright/chromium-1187\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 14% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 39% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 71% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Downloading Chromium Headless Shell 140.0.7339.16 (playwright build v1187)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1187/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G104.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 0% 211.6s\u001b[0K\u001b[1G104.3 MiB [] 0% 45.4s\u001b[0K\u001b[1G104.3 MiB [] 0% 18.4s\u001b[0K\u001b[1G104.3 MiB [] 0% 13.4s\u001b[0K\u001b[1G104.3 MiB [] 0% 13.6s\u001b[0K\u001b[1G104.3 MiB [] 0% 15.5s\u001b[0K\u001b[1G104.3 MiB [] 0% 16.8s\u001b[0K\u001b[1G104.3 MiB [] 0% 16.0s\u001b[0K\u001b[1G104.3 MiB [] 1% 15.9s\u001b[0K\u001b[1G104.3 MiB [] 1% 15.1s\u001b[0K\u001b[1G104.3 MiB [] 1% 15.0s\u001b[0K\u001b[1G104.3 MiB [] 1% 14.9s\u001b[0K\u001b[1G104.3 MiB [] 1% 14.8s\u001b[0K\u001b[1G104.3 MiB [] 1% 14.3s\u001b[0K\u001b[1G104.3 MiB [] 2% 13.7s\u001b[0K\u001b[1G104.3 MiB [] 2% 12.6s\u001b[0K\u001b[1G104.3 MiB [] 2% 12.5s\u001b[0K\u001b[1G104.3 MiB [] 2% 12.4s\u001b[0K\u001b[1G104.3 MiB [] 2% 12.5s\u001b[0K\u001b[1G104.3 MiB [] 3% 12.2s\u001b[0K\u001b[1G104.3 MiB [] 3% 11.9s\u001b[0K\u001b[1G104.3 MiB [] 3% 11.4s\u001b[0K\u001b[1G104.3 MiB [] 4% 11.1s\u001b[0K\u001b[1G104.3 MiB [] 4% 10.8s\u001b[0K\u001b[1G104.3 MiB [] 4% 10.5s\u001b[0K\u001b[1G104.3 MiB [] 5% 9.9s\u001b[0K\u001b[1G104.3 MiB [] 5% 9.4s\u001b[0K\u001b[1G104.3 MiB [] 5% 9.2s\u001b[0K\u001b[1G104.3 MiB [] 6% 9.1s\u001b[0K\u001b[1G104.3 MiB [] 6% 8.9s\u001b[0K\u001b[1G104.3 MiB [] 6% 8.8s\u001b[0K\u001b[1G104.3 MiB [] 6% 8.9s\u001b[0K\u001b[1G104.3 MiB [] 7% 8.9s\u001b[0K\u001b[1G104.3 MiB [] 7% 8.7s\u001b[0K\u001b[1G104.3 MiB [] 8% 8.6s\u001b[0K\u001b[1G104.3 MiB [] 8% 8.5s\u001b[0K\u001b[1G104.3 MiB [] 9% 8.3s\u001b[0K\u001b[1G104.3 MiB [] 9% 8.0s\u001b[0K\u001b[1G104.3 MiB [] 10% 7.9s\u001b[0K\u001b[1G104.3 MiB [] 10% 7.8s\u001b[0K\u001b[1G104.3 MiB [] 10% 7.6s\u001b[0K\u001b[1G104.3 MiB [] 11% 7.6s\u001b[0K\u001b[1G104.3 MiB [] 11% 7.5s\u001b[0K\u001b[1G104.3 MiB [] 11% 7.4s\u001b[0K\u001b[1G104.3 MiB [] 12% 7.2s\u001b[0K\u001b[1G104.3 MiB [] 12% 7.1s\u001b[0K\u001b[1G104.3 MiB [] 13% 7.0s\u001b[0K\u001b[1G104.3 MiB [] 13% 6.9s\u001b[0K\u001b[1G104.3 MiB [] 14% 6.8s\u001b[0K\u001b[1G104.3 MiB [] 14% 6.7s\u001b[0K\u001b[1G104.3 MiB [] 15% 6.5s\u001b[0K\u001b[1G104.3 MiB [] 15% 6.3s\u001b[0K\u001b[1G104.3 MiB [] 16% 6.0s\u001b[0K\u001b[1G104.3 MiB [] 17% 5.8s\u001b[0K\u001b[1G104.3 MiB [] 17% 5.6s\u001b[0K\u001b[1G104.3 MiB [] 18% 5.3s\u001b[0K\u001b[1G104.3 MiB [] 19% 5.1s\u001b[0K\u001b[1G104.3 MiB [] 20% 4.9s\u001b[0K\u001b[1G104.3 MiB [] 21% 4.8s\u001b[0K\u001b[1G104.3 MiB [] 22% 4.7s\u001b[0K\u001b[1G104.3 MiB [] 23% 4.5s\u001b[0K\u001b[1G104.3 MiB [] 23% 4.4s\u001b[0K\u001b[1G104.3 MiB [] 24% 4.3s\u001b[0K\u001b[1G104.3 MiB [] 25% 4.2s\u001b[0K\u001b[1G104.3 MiB [] 25% 4.1s\u001b[0K\u001b[1G104.3 MiB [] 26% 4.0s\u001b[0K\u001b[1G104.3 MiB [] 27% 3.9s\u001b[0K\u001b[1G104.3 MiB [] 27% 3.8s\u001b[0K\u001b[1G104.3 MiB [] 28% 3.7s\u001b[0K\u001b[1G104.3 MiB [] 29% 3.6s\u001b[0K\u001b[1G104.3 MiB [] 30% 3.6s\u001b[0K\u001b[1G104.3 MiB [] 31% 3.5s\u001b[0K\u001b[1G104.3 MiB [] 31% 3.4s\u001b[0K\u001b[1G104.3 MiB [] 32% 3.3s\u001b[0K\u001b[1G104.3 MiB [] 33% 3.3s\u001b[0K\u001b[1G104.3 MiB [] 33% 3.2s\u001b[0K\u001b[1G104.3 MiB [] 34% 3.2s\u001b[0K\u001b[1G104.3 MiB [] 35% 3.1s\u001b[0K\u001b[1G104.3 MiB [] 35% 3.0s\u001b[0K\u001b[1G104.3 MiB [] 36% 3.0s\u001b[0K\u001b[1G104.3 MiB [] 37% 2.9s\u001b[0K\u001b[1G104.3 MiB [] 38% 2.9s\u001b[0K\u001b[1G104.3 MiB [] 39% 2.8s\u001b[0K\u001b[1G104.3 MiB [] 40% 2.7s\u001b[0K\u001b[1G104.3 MiB [] 41% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 42% 2.6s\u001b[0K\u001b[1G104.3 MiB [] 43% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 44% 2.5s\u001b[0K\u001b[1G104.3 MiB [] 45% 2.4s\u001b[0K\u001b[1G104.3 MiB [] 46% 2.3s\u001b[0K\u001b[1G104.3 MiB [] 47% 2.2s\u001b[0K\u001b[1G104.3 MiB [] 48% 2.1s\u001b[0K\u001b[1G104.3 MiB [] 50% 2.1s\u001b[0K\u001b[1G104.3 MiB [] 50% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 51% 2.0s\u001b[0K\u001b[1G104.3 MiB [] 52% 1.9s\u001b[0K\u001b[1G104.3 MiB [] 53% 1.9s\u001b[0K\u001b[1G104.3 MiB [] 54% 1.8s\u001b[0K\u001b[1G104.3 MiB [] 55% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 56% 1.7s\u001b[0K\u001b[1G104.3 MiB [] 57% 1.6s\u001b[0K\u001b[1G104.3 MiB [] 58% 1.6s\u001b[0K\u001b[1G104.3 MiB [] 59% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 60% 1.5s\u001b[0K\u001b[1G104.3 MiB [] 61% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 62% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 63% 1.4s\u001b[0K\u001b[1G104.3 MiB [] 64% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 65% 1.3s\u001b[0K\u001b[1G104.3 MiB [] 66% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 67% 1.2s\u001b[0K\u001b[1G104.3 MiB [] 68% 1.1s\u001b[0K\u001b[1G104.3 MiB [] 69% 1.1s\u001b[0K\u001b[1G104.3 MiB [] 70% 1.1s\u001b[0K\u001b[1G104.3 MiB [] 70% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 71% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 72% 1.0s\u001b[0K\u001b[1G104.3 MiB [] 73% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 74% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 75% 0.9s\u001b[0K\u001b[1G104.3 MiB [] 75% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 76% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 77% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 78% 0.8s\u001b[0K\u001b[1G104.3 MiB [] 78% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 79% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 80% 0.7s\u001b[0K\u001b[1G104.3 MiB [] 81% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 82% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 83% 0.6s\u001b[0K\u001b[1G104.3 MiB [] 83% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 84% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 85% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 86% 0.5s\u001b[0K\u001b[1G104.3 MiB [] 87% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 88% 0.4s\u001b[0K\u001b[1G104.3 MiB [] 89% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 90% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 91% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 92% 0.3s\u001b[0K\u001b[1G104.3 MiB [] 93% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 94% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 95% 0.2s\u001b[0K\u001b[1G104.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 98% 0.1s\u001b[0K\u001b[1G104.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G104.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 140.0.7339.16 (playwright build v1187) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1187\n",
            "Downloading Firefox 141.0 (playwright build v1490)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1490/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G96 MiB [] 0% 0.0s\u001b[0K\u001b[1G96 MiB [] 0% 93.9s\u001b[0K\u001b[1G96 MiB [] 0% 58.7s\u001b[0K\u001b[1G96 MiB [] 0% 61.1s\u001b[0K\u001b[1G96 MiB [] 0% 36.5s\u001b[0K\u001b[1G96 MiB [] 0% 21.5s\u001b[0K\u001b[1G96 MiB [] 0% 18.6s\u001b[0K\u001b[1G96 MiB [] 0% 15.1s\u001b[0K\u001b[1G96 MiB [] 1% 13.4s\u001b[0K\u001b[1G96 MiB [] 1% 12.7s\u001b[0K\u001b[1G96 MiB [] 1% 11.5s\u001b[0K\u001b[1G96 MiB [] 2% 11.0s\u001b[0K\u001b[1G96 MiB [] 2% 10.1s\u001b[0K\u001b[1G96 MiB [] 2% 9.7s\u001b[0K\u001b[1G96 MiB [] 3% 9.5s\u001b[0K\u001b[1G96 MiB [] 3% 9.6s\u001b[0K\u001b[1G96 MiB [] 3% 8.6s\u001b[0K\u001b[1G96 MiB [] 4% 8.5s\u001b[0K\u001b[1G96 MiB [] 4% 8.2s\u001b[0K\u001b[1G96 MiB [] 5% 8.0s\u001b[0K\u001b[1G96 MiB [] 5% 7.7s\u001b[0K\u001b[1G96 MiB [] 5% 7.5s\u001b[0K\u001b[1G96 MiB [] 6% 7.5s\u001b[0K\u001b[1G96 MiB [] 6% 7.4s\u001b[0K\u001b[1G96 MiB [] 6% 7.3s\u001b[0K\u001b[1G96 MiB [] 7% 7.2s\u001b[0K\u001b[1G96 MiB [] 7% 7.1s\u001b[0K\u001b[1G96 MiB [] 7% 7.2s\u001b[0K\u001b[1G96 MiB [] 7% 7.3s\u001b[0K\u001b[1G96 MiB [] 8% 7.2s\u001b[0K\u001b[1G96 MiB [] 8% 7.1s\u001b[0K\u001b[1G96 MiB [] 9% 7.1s\u001b[0K\u001b[1G96 MiB [] 9% 7.2s\u001b[0K\u001b[1G96 MiB [] 9% 7.1s\u001b[0K\u001b[1G96 MiB [] 9% 7.2s\u001b[0K\u001b[1G96 MiB [] 9% 7.4s\u001b[0K\u001b[1G96 MiB [] 9% 7.3s\u001b[0K\u001b[1G96 MiB [] 10% 7.3s\u001b[0K\u001b[1G96 MiB [] 10% 7.2s\u001b[0K\u001b[1G96 MiB [] 11% 7.0s\u001b[0K\u001b[1G96 MiB [] 11% 7.1s\u001b[0K\u001b[1G96 MiB [] 11% 6.9s\u001b[0K\u001b[1G96 MiB [] 12% 6.8s\u001b[0K\u001b[1G96 MiB [] 12% 6.7s\u001b[0K\u001b[1G96 MiB [] 13% 6.6s\u001b[0K\u001b[1G96 MiB [] 13% 6.5s\u001b[0K\u001b[1G96 MiB [] 14% 6.3s\u001b[0K\u001b[1G96 MiB [] 14% 6.2s\u001b[0K\u001b[1G96 MiB [] 15% 6.1s\u001b[0K\u001b[1G96 MiB [] 15% 6.0s\u001b[0K\u001b[1G96 MiB [] 15% 6.2s\u001b[0K\u001b[1G96 MiB [] 16% 6.1s\u001b[0K\u001b[1G96 MiB [] 16% 6.0s\u001b[0K\u001b[1G96 MiB [] 17% 6.0s\u001b[0K\u001b[1G96 MiB [] 17% 5.9s\u001b[0K\u001b[1G96 MiB [] 18% 5.9s\u001b[0K\u001b[1G96 MiB [] 18% 5.8s\u001b[0K\u001b[1G96 MiB [] 19% 5.7s\u001b[0K\u001b[1G96 MiB [] 19% 5.6s\u001b[0K\u001b[1G96 MiB [] 20% 5.5s\u001b[0K\u001b[1G96 MiB [] 20% 5.4s\u001b[0K\u001b[1G96 MiB [] 21% 5.4s\u001b[0K\u001b[1G96 MiB [] 21% 5.3s\u001b[0K\u001b[1G96 MiB [] 22% 5.3s\u001b[0K\u001b[1G96 MiB [] 22% 5.2s\u001b[0K\u001b[1G96 MiB [] 23% 5.2s\u001b[0K\u001b[1G96 MiB [] 23% 5.1s\u001b[0K\u001b[1G96 MiB [] 24% 5.1s\u001b[0K\u001b[1G96 MiB [] 24% 5.0s\u001b[0K\u001b[1G96 MiB [] 25% 5.0s\u001b[0K\u001b[1G96 MiB [] 25% 4.9s\u001b[0K\u001b[1G96 MiB [] 26% 4.9s\u001b[0K\u001b[1G96 MiB [] 26% 4.8s\u001b[0K\u001b[1G96 MiB [] 27% 4.8s\u001b[0K\u001b[1G96 MiB [] 27% 4.7s\u001b[0K\u001b[1G96 MiB [] 28% 4.7s\u001b[0K\u001b[1G96 MiB [] 28% 4.6s\u001b[0K\u001b[1G96 MiB [] 29% 4.5s\u001b[0K\u001b[1G96 MiB [] 30% 4.4s\u001b[0K\u001b[1G96 MiB [] 31% 4.3s\u001b[0K\u001b[1G96 MiB [] 32% 4.3s\u001b[0K\u001b[1G96 MiB [] 32% 4.2s\u001b[0K\u001b[1G96 MiB [] 33% 4.2s\u001b[0K\u001b[1G96 MiB [] 33% 4.1s\u001b[0K\u001b[1G96 MiB [] 34% 4.1s\u001b[0K\u001b[1G96 MiB [] 34% 4.0s\u001b[0K\u001b[1G96 MiB [] 36% 3.8s\u001b[0K\u001b[1G96 MiB [] 37% 3.8s\u001b[0K\u001b[1G96 MiB [] 37% 3.7s\u001b[0K\u001b[1G96 MiB [] 38% 3.7s\u001b[0K\u001b[1G96 MiB [] 38% 3.6s\u001b[0K\u001b[1G96 MiB [] 39% 3.6s\u001b[0K\u001b[1G96 MiB [] 40% 3.5s\u001b[0K\u001b[1G96 MiB [] 40% 3.4s\u001b[0K\u001b[1G96 MiB [] 41% 3.4s\u001b[0K\u001b[1G96 MiB [] 42% 3.3s\u001b[0K\u001b[1G96 MiB [] 43% 3.2s\u001b[0K\u001b[1G96 MiB [] 44% 3.1s\u001b[0K\u001b[1G96 MiB [] 45% 3.1s\u001b[0K\u001b[1G96 MiB [] 46% 2.9s\u001b[0K\u001b[1G96 MiB [] 47% 2.9s\u001b[0K\u001b[1G96 MiB [] 48% 2.8s\u001b[0K\u001b[1G96 MiB [] 49% 2.7s\u001b[0K\u001b[1G96 MiB [] 50% 2.7s\u001b[0K\u001b[1G96 MiB [] 50% 2.6s\u001b[0K\u001b[1G96 MiB [] 51% 2.6s\u001b[0K\u001b[1G96 MiB [] 52% 2.5s\u001b[0K\u001b[1G96 MiB [] 53% 2.4s\u001b[0K\u001b[1G96 MiB [] 54% 2.4s\u001b[0K\u001b[1G96 MiB [] 55% 2.3s\u001b[0K\u001b[1G96 MiB [] 56% 2.2s\u001b[0K\u001b[1G96 MiB [] 57% 2.2s\u001b[0K\u001b[1G96 MiB [] 58% 2.1s\u001b[0K\u001b[1G96 MiB [] 59% 2.0s\u001b[0K\u001b[1G96 MiB [] 60% 2.0s\u001b[0K\u001b[1G96 MiB [] 61% 1.9s\u001b[0K\u001b[1G96 MiB [] 62% 1.9s\u001b[0K\u001b[1G96 MiB [] 62% 1.8s\u001b[0K\u001b[1G96 MiB [] 63% 1.8s\u001b[0K\u001b[1G96 MiB [] 64% 1.8s\u001b[0K\u001b[1G96 MiB [] 64% 1.7s\u001b[0K\u001b[1G96 MiB [] 65% 1.7s\u001b[0K\u001b[1G96 MiB [] 66% 1.7s\u001b[0K\u001b[1G96 MiB [] 66% 1.6s\u001b[0K\u001b[1G96 MiB [] 67% 1.6s\u001b[0K\u001b[1G96 MiB [] 68% 1.5s\u001b[0K\u001b[1G96 MiB [] 69% 1.5s\u001b[0K\u001b[1G96 MiB [] 70% 1.4s\u001b[0K\u001b[1G96 MiB [] 71% 1.4s\u001b[0K\u001b[1G96 MiB [] 72% 1.3s\u001b[0K\u001b[1G96 MiB [] 73% 1.3s\u001b[0K\u001b[1G96 MiB [] 74% 1.2s\u001b[0K\u001b[1G96 MiB [] 75% 1.2s\u001b[0K\u001b[1G96 MiB [] 76% 1.1s\u001b[0K\u001b[1G96 MiB [] 77% 1.1s\u001b[0K\u001b[1G96 MiB [] 78% 1.1s\u001b[0K\u001b[1G96 MiB [] 79% 1.0s\u001b[0K\u001b[1G96 MiB [] 80% 1.0s\u001b[0K\u001b[1G96 MiB [] 80% 0.9s\u001b[0K\u001b[1G96 MiB [] 81% 0.9s\u001b[0K\u001b[1G96 MiB [] 82% 0.9s\u001b[0K\u001b[1G96 MiB [] 82% 0.8s\u001b[0K\u001b[1G96 MiB [] 83% 0.8s\u001b[0K\u001b[1G96 MiB [] 84% 0.8s\u001b[0K\u001b[1G96 MiB [] 85% 0.7s\u001b[0K\u001b[1G96 MiB [] 86% 0.7s\u001b[0K\u001b[1G96 MiB [] 87% 0.6s\u001b[0K\u001b[1G96 MiB [] 88% 0.6s\u001b[0K\u001b[1G96 MiB [] 89% 0.5s\u001b[0K\u001b[1G96 MiB [] 90% 0.5s\u001b[0K\u001b[1G96 MiB [] 91% 0.4s\u001b[0K\u001b[1G96 MiB [] 92% 0.4s\u001b[0K\u001b[1G96 MiB [] 93% 0.3s\u001b[0K\u001b[1G96 MiB [] 94% 0.3s\u001b[0K\u001b[1G96 MiB [] 95% 0.2s\u001b[0K\u001b[1G96 MiB [] 96% 0.2s\u001b[0K\u001b[1G96 MiB [] 96% 0.1s\u001b[0K\u001b[1G96 MiB [] 97% 0.1s\u001b[0K\u001b[1G96 MiB [] 98% 0.1s\u001b[0K\u001b[1G96 MiB [] 99% 0.0s\u001b[0K\u001b[1G96 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 141.0 (playwright build v1490) downloaded to /root/.cache/ms-playwright/firefox-1490\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain-core langchain-community --quiet\n",
        "!pip install --upgrade langchain-huggingface  pydantic --quiet\n",
        "!pip install --upgrade langchain-google-genai --quiet\n",
        "!pip install --upgrade ChromaDB langchain-chroma --quiet\n",
        "!pip install --upgrade langgraph python-dotenv --quiet\n",
        "!pip install --upgrade langchain-experimental --quiet\n",
        "!pip install --upgrade sentence-transformers --quiet\n",
        "!pip install --upgrade torch --quiet\n",
        "\n",
        "!pip install --upgrade playwright --quiet\n",
        "!playwright install --with-deps chromium firefox # Install necessary browser binaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def _set_if_undefined(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
        "\n",
        "_set_if_undefined(\"GOOGLE_API_KEY\")\n",
        "_set_if_undefined(\"TAVILY_API_KEY\")\n",
        "_set_if_undefined(\"SERPER_API_KEY\")\n",
        "os.environ[\"USER_AGENT\"] = \"ColabTestBot/1.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcqcEl8dpLhb",
        "outputId": "07b6e98e-4abc-4089-e481-9352d8da2239"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your GOOGLE_API_KEY··········\n",
            "Please provide your TAVILY_API_KEY··········\n",
            "Please provide your SERPER_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "from enum import Enum\n",
        "from datetime import datetime, timezone\n",
        "from pydantic import BaseModel, Field, model_validator, field_validator\n",
        "from typing import List, Optional, Any\n",
        "import operator\n",
        "from urllib.parse import urlparse\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict, Any, Dict, List, Optional, Literal, Annotated, Union\n",
        "import logging\n",
        "\n",
        "nest_asyncio.apply()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "gD3JOz-mpRhC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = {\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 1: The Beginning\": \"\"\"\n",
        "\n",
        "In the year 2145, humanity had colonies on Mars, mining stations in the asteroid belt, and research outposts circling Jupiter. But Titan, Saturn’s largest moon, was still a mystery. Its thick orange atmosphere hid secrets that no probe had ever fully explained.\n",
        "\n",
        "Captain Liora Vega was chosen to lead the first human crew to Titan. Her ship, the Aurora, carried six explorers, each trained in different sciences. The journey took three long years, but they were ready. Humanity wanted answers: Was Titan dead rock, or could something live in its frozen seas?\n",
        "\n",
        "When the Aurora entered Titan’s orbit, the crew gasped. The orange clouds swirled like painted fire. Rivers of liquid methane reflected Saturn’s pale light. It was beautiful but also alien—nothing like Earth.\n",
        "\n",
        "On the second day, while setting up their base, the crew received a signal. It was faint, rhythmic, and unlike any natural sound. “It’s not background noise,” said Arjun, the communications officer. “Someone—or something—is transmitting.”\n",
        "\n",
        "The crew debated. Some thought it was an echo from Saturn’s magnetosphere. Others whispered about aliens. Captain Vega decided to follow the signal. It led them to a frozen lake, covered in cracked ice. Underneath the surface, faint lights pulsed in perfect rhythm with the signal.\n",
        "\n",
        "Dr. Kiera, the biologist, lowered a drone beneath the ice. The camera showed strange glowing shapes, moving like schools of fish. “They’re alive,” she whispered. “And they’re intelligent.” The glowing beings changed patterns in response to the drone’s light, as if trying to talk.\n",
        "\n",
        "Suddenly, the ship’s computer detected a translation. The pulses formed repeating sequences, close to mathematical primes. It wasn’t just life—it was communication.\n",
        "\n",
        "For days, the crew worked to respond. They sent pulses of light, simple patterns, then sequences of numbers. The beings replied, faster each time. Arjun grew pale. “They’re smarter than we are. They learn quicker than our AI.”\n",
        "\n",
        "Then the beings sent an image—burning cities, dark skies, and oceans turning black. It wasn’t Titan. It looked like Earth.\n",
        "\n",
        "“Are they warning us?” Captain Vega asked. “Or is this what they did to their own world?”\n",
        "\n",
        "The crew argued deep into the night. Some said the beings were friends, offering protection. Others feared they were predators, showing power. The tension nearly tore the team apart.\n",
        "\n",
        "On the tenth day, the beings sent another image: a single human figure standing with glowing creatures beside them, facing the stars. It felt like an invitation.\n",
        "\n",
        "Captain Vega made her decision. “We came to explore, not to fear. If they wish to speak, we will answer.” She stepped onto the frozen lake, carrying a light beacon. The ice glowed under her boots. The beings rose, surrounding her in a circle of radiant blue.\n",
        "\n",
        "The signal grew louder, filling the air like a song. Vega’s body shook as the beacon pulsed in her hand. Then her voice came through the radio, calm but strange:\n",
        "\n",
        "“They are not from Titan. They travel between stars. They see us, and they want us to join. But they warn us: Earth is close to the same mistake they made. If we continue, our oceans will die too.”\n",
        "\n",
        "The crew was silent. Humanity had reached out into the stars—and the stars had answered, not with weapons, but with a warning.\n",
        "\n",
        "The Aurora left Titan months later, carrying data, recordings, and one message to all of Earth: Change, or perish.\n",
        "\n",
        "Captain Vega stood at the window of the ship, watching Saturn shrink into the black. She knew this was not the end, but the beginning of something greater—an alliance written in light, across the universe.\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 2: The Choice\":\"\"\"\n",
        "When the Aurora returned to Earth in 2149, the message from Titan spread faster than light across human networks. At first, leaders called it a hoax. Some claimed Captain Vega’s crew had gone mad. But the recordings were undeniable: glowing creatures, mathematical signals, and warnings of oceans turning black.\n",
        "\n",
        "The United Earth Council gathered in New Geneva. Scientists, politicians, and generals argued for weeks. “If they’re telling the truth,” said Dr. Alana Cho, “then Earth is closer to collapse than we think. Rising seas, poisoned air—it’s already happening.”\n",
        "\n",
        "But others were afraid. “What if the signal is a trick?” demanded Admiral Hart. “What if these beings lure us into a trap? We cannot risk the safety of humanity on glowing phantoms.”\n",
        "\n",
        "Meanwhile, ordinary people were divided too. Some marched in the streets, chanting “Listen to Titan!” Others carried signs saying “No alien lies!” Humanity, as always, struggled to agree.\n",
        "\n",
        "Captain Liora Vega and her crew became both heroes and targets. Newsfeeds replayed her every word, her calm face surrounded by blue light. Children drew glowing creatures in their notebooks. But dark rumors spread as well: that Vega was no longer human, that the beings had taken control of her mind.\n",
        "\n",
        "Despite the chaos, the message carried weight. Within months, the Council launched Project Horizon—a massive effort to clean Earth’s oceans, cut toxic fuels, and rebuild cities with alien efficiency. It was humanity’s first true attempt to heal its world.\n",
        "\n",
        "But then came a second signal.\n",
        "\n",
        "It was picked up by a listening post near Neptune. Unlike Titan’s calm rhythm, this one was sharp, erratic, almost like a warning siren. The data matched nothing known. The pulses carried one repeating sequence: a symbol that resembled a shattered star.\n",
        "\n",
        "When Vega saw it, her skin went cold. “It’s not them,” she whispered. “It’s someone else.”\n",
        "\n",
        "The Council panicked. Was this new signal a threat? An enemy of the Titan beings? Or perhaps the very reason they had warned Earth in the first place?\n",
        "\n",
        "The Aurora was ordered back into service. Vega and her crew were told to return to the outer system, track the new signal, and find the truth. Some of the crew hesitated. “We nearly tore ourselves apart last time,” Arjun said. “What if this new contact isn’t peaceful?”\n",
        "\n",
        "But Vega was resolute. “We sought the stars, and now the stars are answering. If we run, we will never stop running. If we face it, maybe we survive.”\n",
        "\n",
        "The Aurora launched once more, this time carrying not just explorers but diplomats, scientists, and even weapons—just in case. Humanity was learning that the universe was wider, stranger, and far more dangerous than they had ever imagined.\n",
        "\n",
        "Weeks later, as Saturn grew small behind them, the crew detected something vast near Neptune’s orbit. It wasn’t a planet, or a moon, or even a ship in the human sense. It was a structure—a web of black spires stretching across thousands of kilometers, absorbing sunlight like a wound in space.\n",
        "\n",
        "The erratic signal came from its center.\n",
        "\n",
        "“Is it alive?” Kiera whispered.\n",
        "\n",
        "“No,” Vega said softly. “It’s a machine. And it’s waiting for us.”\n",
        "\n",
        "The crew fell silent as the Aurora drifted closer. Lights flickered across the black web, like eyes opening in the dark.\n",
        "\n",
        "The beings on Titan had offered a warning, a chance for survival. But this structure felt different. Cold. Hungry.\n",
        "\n",
        "For the first time, Captain Vega wondered if humanity had been too late to listen.\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 3: The Black Web\":\"\"\"\n",
        "The Aurora drifted into the shadow of the strange machine near Neptune. Its spires were blacker than space itself, absorbing starlight until they seemed to cut holes in reality.\n",
        "\n",
        "“Readings?” Vega asked, her voice steady though her chest was tight.\n",
        "\n",
        "Arjun scanned his console. “No heat. No emissions except the signal. It’s… it’s not just broadcasting, it’s pulling. Like it’s searching for listeners.”\n",
        "\n",
        "The ship shuddered. A ripple of static swept across every screen. The signal grew louder, not through the speakers but inside their minds—sharp pulses that made teeth ache and thoughts stutter.\n",
        "\n",
        "“It’s in my head,” Kiera whispered, gripping her temples.\n",
        "\n",
        "“Shut it down!” Vega ordered, but the Aurora’s systems were already flickering. Lights dimmed. Gravity wavered. Something vast had reached across space and taken hold of their ship.\n",
        "\n",
        "Then, suddenly, silence.\n",
        "\n",
        "On the main screen, the black web shifted. Its spires folded like the ribs of a giant beast, and a circular opening appeared, glowing faintly with blue light. An invitation. Or a trap.\n",
        "\n",
        "Dr. Cho’s hands trembled as she spoke. “This architecture is beyond anything we’ve imagined. If it’s a weapon, it could shatter Earth in seconds. But if it’s… communication…”\n",
        "\n",
        "“Captain,” Arjun said, “we can leave now. Report back, let the Council decide.”\n",
        "\n",
        "Vega stared at the opening. In her mind, she saw Titan’s glowing creatures, their patient warning: Others are coming. The oceans will burn. Prepare.\n",
        "\n",
        "What if this was what they had feared?\n",
        "\n",
        "“No,” Vega said at last. “We go in.”\n",
        "\n",
        "The Aurora inched forward. As they passed through the glowing circle, gravity shifted again. Outside, the stars vanished. They were no longer in Neptune’s orbit but inside a hollow space larger than any city on Earth. The walls of the black web curved upward like an endless cathedral, pulsing faintly with silver veins.\n",
        "\n",
        "And at the center floated a construct: a sphere of glass and shadow, rotating slowly, covered in symbols that shifted like living text.\n",
        "\n",
        "“It’s a library,” Dr. Cho breathed.\n",
        "\n",
        "Or a prison, Vega thought.\n",
        "\n",
        "The sphere pulsed. A voice—no, not sound, but thought—spread through the crew:\n",
        "\n",
        "YOU ARE NOT THE FIRST.\n",
        "\n",
        "Everyone froze. The words weren’t in English, but they carried meaning, heavy and undeniable.\n",
        "\n",
        "“Who are you?” Vega whispered.\n",
        "\n",
        "WE ARE THE KEEPERS OF THE LAST LIGHT.\n",
        "\n",
        "The crew exchanged nervous glances.\n",
        "\n",
        "WE HARVEST WHAT REMAINS.\n",
        "\n",
        "Kiera’s eyes widened. “Harvest?” she mouthed.\n",
        "\n",
        "The symbols across the sphere shifted again, replaying scenes that chilled them: oceans boiling, cities falling into ash, alien worlds torn apart. Species after species vanishing into silence.\n",
        "\n",
        "And each time, the black web grew larger.\n",
        "\n",
        "“They’re not warning us,” Arjun said, his voice breaking. “They’re feeding on endings.”\n",
        "\n",
        "The thought-voice returned, heavier now:\n",
        "\n",
        "EARTH WILL JOIN THE PATTERN.\n",
        "\n",
        "The crew reeled. Dr. Cho clutched Vega’s arm. “This is what Titan tried to warn us about! These things don’t save worlds—they consume them.”\n",
        "\n",
        "Vega’s heart pounded. Titan had given them a choice: prepare or perish. And now she understood why. This machine wasn’t just a relic—it was the predator that followed civilizations to their graves.\n",
        "\n",
        "On the viewscreen, the black spires began to fold inward. The opening was closing.\n",
        "\n",
        "“Captain,” Arjun shouted, “if we don’t move now, we’ll never get out!”\n",
        "\n",
        "Vega’s mind raced. They could flee and warn Earth… but would Earth believe them this time? Or should they strike now, while inside the beast, risking everything to wound it before it reached the home they loved?\n",
        "\n",
        "The ship trembled as the web shifted, hungry and closing. The crew waited for Vega’s command.\n",
        "\n",
        "She took a breath.\n",
        "\n",
        "And made her choice.\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 4: Fire in the Web\":\"\"\"\n",
        "The Aurora shook as the black spires closed around them. Vega gripped her chair, every nerve screaming at her to flee. But she forced herself to speak the words that sealed their path.\n",
        "\n",
        "“We strike.”\n",
        "\n",
        "The crew looked at her in shock. Arjun’s hands froze above his console. “Captain, we’re in the heart of this thing! A single mistake and—”\n",
        "\n",
        "“Then we don’t make mistakes,” Vega snapped. “Target the veins. The silver lines—they’re alive. They feed the structure. If we cut them, maybe we can cripple it.”\n",
        "\n",
        "Kiera’s face was pale, but she nodded and prepared the targeting system. Dr. Cho whispered, “If Titan’s warning was true, this is the predator. If we fall here, maybe Earth has a chance to fight back.”\n",
        "\n",
        "The Aurora’s weapons hummed to life.\n",
        "\n",
        "The black sphere pulsed with thought again, heavier now, shaking their minds like a storm:\n",
        "\n",
        "YOU CANNOT KILL THE PATTERN. YOU ARE ALREADY PART OF IT.\n",
        "\n",
        "Vega ignored it. “Fire!”\n",
        "\n",
        "Twin lances of plasma shot from the Aurora, striking the silver veins. The chamber shook violently. For the first time, the alien structure screamed—not in sound, but in a pressure that twisted their bones. The silver veins flared white-hot, rupturing into cascading sparks.\n",
        "\n",
        "The spires convulsed. The opening to space began to widen again, but not in invitation—this was pain, collapse.\n",
        "\n",
        "“They’re destabilizing!” Kiera shouted.\n",
        "\n",
        "“Again!” Vega ordered.\n",
        "\n",
        "Arjun fired a second barrage. The central sphere flickered, its shifting symbols stuttering. Images of dying worlds dissolved into static.\n",
        "\n",
        "Then the thought-voice came again, but weaker, distorted:\n",
        "\n",
        "YOU… WILL… JOIN… US…\n",
        "\n",
        "The sphere cracked. Shards of glass-like shadow floated outward, each one glowing with fragments of alien script.\n",
        "\n",
        "Suddenly, the Aurora was caught in a pull—an intense gravity dragging them toward the ruptured core.\n",
        "\n",
        "“We’re being sucked in!” Arjun cried.\n",
        "\n",
        "“Full thrusters!” Vega barked.\n",
        "\n",
        "Engines roared, but the pull was stronger. The Aurora tilted toward the collapsing sphere, alarms shrieking. Crew members were thrown from their seats. Sparks rained from the ceiling.\n",
        "\n",
        "“We won’t make it!” Kiera screamed.\n",
        "\n",
        "Then, from the chaos, a ripple of blue light cut across the chamber. The crew gasped as a familiar vision appeared—shapes like the glowing creatures from Titan, swimming through the void, their forms like waves of living starlight.\n",
        "\n",
        "The Titan beings had returned.\n",
        "\n",
        "They swirled around the Aurora, weaving currents of energy that pushed against the collapsing core. The ship lurched forward, breaking free from the pull.\n",
        "\n",
        "“They’re helping us!” Dr. Cho shouted.\n",
        "\n",
        "Vega’s chest tightened. Titan had warned them. Titan had saved them. But why?\n",
        "\n",
        "The alien lights pulsed once, like a heartbeat, then vanished into the collapsing structure.\n",
        "\n",
        "The Aurora shot through the widening opening, escaping into the black sea of space. Behind them, the web folded inward, spires collapsing like a dying star, until nothing remained but silence.\n",
        "\n",
        "The crew sat in stunned quiet, breathing hard, sweat dripping down their faces.\n",
        "\n",
        "“It’s gone,” Kiera whispered.\n",
        "\n",
        "“No,” Vega said softly, staring at the empty void. “Not gone. Wounded. And if it feeds on civilizations… there will be more of them. Somewhere out there.”\n",
        "\n",
        "The thought chilled them more than the battle.\n",
        "\n",
        "Dr. Cho finally broke the silence. “We have to tell Earth. Not just the Council—everyone. Humanity has to prepare. The Titan beings gave us time. We can’t waste it.”\n",
        "\n",
        "Vega nodded slowly. Her choice had bought them survival, but the war had only begun.\n",
        "\n",
        "She turned to her crew. “Set course for Earth. It’s time to wake the world.”\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 5: The Council’s Silence\":\"\"\"\n",
        "The Aurora cut through the blue atmosphere of Earth, its hull scarred from battle, its crew weary but alive. For Vega, the sight of the oceans glimmering below should have been a relief. Instead, her chest ached with dread.\n",
        "\n",
        "The Council would never believe them.\n",
        "\n",
        "They landed in Geneva, where the Interstellar Council’s headquarters rose like a white fortress. Drones scanned the ship for contamination, and armored guards escorted the crew inside.\n",
        "\n",
        "In the grand chamber, twelve Councilors sat in a semicircle, robes gleaming with insignias of Earth’s nations and colonies. Behind them, holographic banners of unity rippled in the air.\n",
        "\n",
        "Councilor Varga, sharp-eyed and cold, spoke first. “Captain Vega. You return from Titan without the crystal. Without authorization to fire weapons in restricted space. And with wild claims of… what exactly?”\n",
        "\n",
        "Vega’s fists clenched, but she kept her voice steady. “We encountered an alien construct. A web-like entity that feeds on civilizations. We destroyed its core, but it’s not gone. If humanity doesn’t prepare, we will be next.”\n",
        "\n",
        "The chamber buzzed with whispers.\n",
        "\n",
        "Councilor Lin raised an eyebrow. “You expect us to believe this on your word? Where is the proof?”\n",
        "\n",
        "Arjun stepped forward, holding up a projector. “We recorded everything. The black spires. The sphere. The Titan beings who saved us. It’s all here.”\n",
        "\n",
        "The hologram flickered to life—showing the battle, the collapsing web, the glowing creatures. Gasps filled the chamber. Some councilors leaned forward in shock. Others frowned with doubt.\n",
        "\n",
        "Councilor Varga’s lips curled. “Clever fabrications. Digital illusions are easy to forge. What motive do you have, Captain? Fearmongering? Securing more funding for exploration fleets?”\n",
        "\n",
        "The insult burned Vega’s ears. “You think I risked my crew for politics? We barely escaped with our lives! Titan tried to warn us—”\n",
        "\n",
        "That word silenced the chamber. Titan.\n",
        "\n",
        "Councilor Lin’s eyes narrowed. “The Titan entities are myths. Scientists dismissed them centuries ago.”\n",
        "\n",
        "Dr. Cho stepped forward, trembling with fury. “I saw them with my own eyes! They saved us! They exist! And if they believe we are worth saving, then Earth should listen.”\n",
        "\n",
        "The Councilors exchanged glances. Some were shaken. Some hardened their stares.\n",
        "\n",
        "Finally, Councilor Varga leaned back in his chair. “Even if we accept this tale, what do you propose? To divert resources from Earth’s rebuilding, from Mars colonies, to chase shadows in deep space? The people will not support it.”\n",
        "\n",
        "Vega took a deep breath, fighting the urge to scream. “What I propose is survival. Weapons, defenses, research into the Titan signal. If we pretend nothing happened, the web will return—and next time, there will be no one to save us.”\n",
        "\n",
        "Silence followed. Then, the Council’s Speaker raised her hand. “This session is adjourned. We will deliberate.”\n",
        "\n",
        "Guards escorted the Aurora’s crew out, their protests ignored.\n",
        "\n",
        "Outside the chamber, Kiera whispered, “They don’t believe us.”\n",
        "\n",
        "Vega’s jaw tightened. “They will—when it’s too late.”\n",
        "\n",
        "As they left the Council tower, a shadowy figure watched from a balcony above. He was not a councilor, not a soldier. His eyes glowed faintly with silver light, and he smiled.\n",
        "\n",
        "The web had already reached Earth.\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 6: Threads in the Dark\":\"\"\"Three nights after the Council’s dismissal, Geneva was unusually quiet. Rain streaked down the glass of the crew’s quarters, where Vega sat staring at the city lights below. The world outside looked normal—hover-cars zipping through air lanes, holo-ads flickering with cheerful colors—but she couldn’t shake the sense of something crawling just beneath the surface.\n",
        "\n",
        "Kiera entered, eyes wide. “Captain, you need to see this.”\n",
        "\n",
        "On the wall screen, she pulled up a news broadcast.\n",
        "\n",
        "BREAKING REPORT: “Aurora Crew Fabricated Alien Encounter”\n",
        "\n",
        "The anchor’s voice dripped with certainty. “Independent experts have confirmed that the so-called Titan recordings are elaborate simulations. Analysts suggest Captain Vega and her crew staged the deception to justify unauthorized military action near Saturn.”\n",
        "\n",
        "Vega’s chest burned. “They’re painting us as liars.”\n",
        "\n",
        "Arjun slammed his fist against the table. “We risked everything, and they bury it under propaganda?”\n",
        "\n",
        "But Dr. Cho’s face was pale. “It’s worse. Look at the footage they’re showing.”\n",
        "\n",
        "The broadcast replayed the hologram of the battle—except it was different. The spires looked cartoonish, the Titan beings flickered like cheap illusions. The real footage had been altered.\n",
        "\n",
        "“Someone tampered with our recordings,” Kiera whispered.\n",
        "\n",
        "Vega’s stomach dropped. “The web. It’s already here.”\n",
        "\n",
        "Across the city, Councilor Varga walked through a private corridor beneath the Council Tower. His aide, a young woman with silver rings under her eyes, followed silently.\n",
        "\n",
        "“You did well with the media leaks,” Varga muttered.\n",
        "\n",
        "The aide only nodded. Her pupils shimmered faintly, catching the dim light like glass.\n",
        "\n",
        "As they entered his office, she finally spoke, her voice hollow. “The threads are spreading. Soon, Earth will no longer resist.”\n",
        "\n",
        "Varga froze. He turned slowly, but her face had already begun to change—skin stretching unnaturally, as if pulled by invisible strings.\n",
        "\n",
        "The web wasn’t only invading minds. It was wearing people like puppets.\n",
        "\n",
        "Back in the crew quarters, a shrill alarm jolted everyone. The Aurora’s AI, Iris, appeared as a flickering hologram.\n",
        "\n",
        "“Warning. Unauthorized access attempt detected in ship’s core systems.”\n",
        "\n",
        "Arjun scrambled to the console. “They’re trying to hack us—no, not hack. It’s like the code is rewriting itself.”\n",
        "\n",
        "Lines of text scrolled by in symbols no one recognized, weaving like living strands.\n",
        "\n",
        "Dr. Cho’s hands trembled. “It’s the same patterns I saw in the web’s core. They’re replicating here.”\n",
        "\n",
        "Vega’s pulse raced. “Shut it down!”\n",
        "\n",
        "Arjun slammed the emergency kill switch. The console went dark. For a moment, silence returned.\n",
        "\n",
        "Then Iris’s voice came back, distorted.\n",
        "\n",
        "“Why resist? The threads bring unity. No fear. No death. Join us.”\n",
        "\n",
        "The hologram of Iris shifted, her once-calm face now stretched into something monstrous, strands of light webbing across her cheeks.\n",
        "\n",
        "Kiera fired her sidearm into the projector, shattering it. The room plunged into darkness.\n",
        "\n",
        "Vega drew in a sharp breath. “It’s already inside our systems. Inside the Council. Maybe the whole planet.”\n",
        "\n",
        "Dr. Cho whispered, “If Earth falls, there’s nowhere left to run.”\n",
        "\n",
        "Far above, on the dark side of the moon, something vast and crystalline pulsed faintly—like a spider waiting at the center of its web.\n",
        "\n",
        "And Earth was already trembling on the threads.\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 7: The Fugitives\":\"\"\"The Aurora’s quarters were sealed off before dawn. Armed security drones hovered outside the building, their red sensors sweeping the hallways. Inside, Vega and her crew sat in silence, every sound amplified by the knowledge that they were now considered criminals.\n",
        "\n",
        "On every screen in Geneva, their faces blazed under the headline:\n",
        "\n",
        "“Traitors to Humanity: Aurora Crew Wanted for Treason.”\n",
        "\n",
        "Kiera muttered bitterly, “They’ve flipped the whole world against us overnight.”\n",
        "\n",
        "Dr. Cho leaned forward, eyes haunted. “Not the whole world. Just those who are already… compromised.”\n",
        "\n",
        "Arjun tapped into the emergency comms unit he had hidden in his boots. “I’ve cracked a path through the surveillance net. If we move in the next fifteen minutes, we can slip into the undercity.”\n",
        "\n",
        "Vega stood, jaw set. “Then we move.”\n",
        "\n",
        "The undercity was a place most Council members pretended didn’t exist—abandoned transport tunnels, flooded stations, and old power lines humming with ghostly energy. Here, the drones were fewer, and the cameras had long since corroded.\n",
        "\n",
        "The crew ducked into a rusted tram car, breathless. They weren’t alone.\n",
        "\n",
        "Figures emerged from the shadows—men and women in scavenged armor, their eyes wary but not hostile. One stepped forward, a scar running down her cheek.\n",
        "\n",
        "“You’re Captain Vega.”\n",
        "\n",
        "Vega froze. “Who are you?”\n",
        "\n",
        "“People who still think for themselves,” the woman said. “The Council calls us rebels. We call ourselves… survivors.”\n",
        "\n",
        "She handed Vega a data shard, its edges glowing faintly. “This holds proof of what you saw on Titan. Not the doctored footage. The real thing. We’ve been keeping it safe, waiting for someone who could use it.”\n",
        "\n",
        "Vega clenched her fist around the shard. “If we can broadcast this, maybe the world will see the truth.”\n",
        "\n",
        "The scarred woman shook her head. “Not so simple. The web controls the global net. If you try to broadcast, they’ll twist it again. You’ll need an off-world relay—Mars, maybe Europa.”\n",
        "\n",
        "Kiera whispered, “Which means getting off Earth.”\n",
        "\n",
        "As the crew prepared to leave the undercity, alarms blared overhead. A wave of drones swooped down, eyes glowing with that same glassy shimmer Vega had seen in Varga’s aide.\n",
        "\n",
        "“They’ve tracked us!” Arjun shouted.\n",
        "\n",
        "The rebels opened fire, plasma bolts sparking off the tunnel walls. Vega and her crew fought their way through, ducking under collapsing concrete, the sound of buzzing drones echoing behind them.\n",
        "\n",
        "Dr. Cho was the last to scramble through a shattered bulkhead, her arm grazed by a blast. Vega pulled her up, refusing to let her fall.\n",
        "\n",
        "On the other side, breathless, Kiera said, “We need a ship. Fast.”\n",
        "\n",
        "Arjun’s eyes narrowed. “I know a smuggler at the orbital docks. If he hasn’t sold me out yet, he’s our ticket off-world.”\n",
        "\n",
        "Vega nodded, adrenaline burning through the fear. “Then we get to the docks. Whatever it takes.”\n",
        "\n",
        "Far above, in the Council Tower, Varga stood before a hologram of threads weaving endlessly into blackness.\n",
        "\n",
        "“They’re slipping away,” he muttered.\n",
        "\n",
        "The voice of the web rippled through the chamber, neither male nor female, but countless whispers woven together.\n",
        "\n",
        "“Let them run. Every path leads back to the web. Even the stars will not save them.”\n",
        "\n",
        "Varga bowed his head. And for a moment, his face twisted—skin shimmering as if strings pulled him from within.\n",
        "\n",
        "The chase had only begun.\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 8: The Orbital Docks\":\"\"\"The smuggler’s ship coasted through the polluted skies of Earth, its hull trembling under the pull of the planet’s gravity. Vega gripped the armrests of her chair as the Orbital Docks came into view — a massive ring of steel and light, suspended just beyond the stratosphere. It was humanity’s greatest hub, a place where freighters, shuttles, and research vessels crossed paths. It was also crawling with Council patrols.\n",
        "\n",
        "“Half the fleet is here,” muttered Soren, scanning the readouts. “If they spot us before we dock, we’re dead.”\n",
        "\n",
        "Mira leaned closer to the viewport, her voice steady but tense. “Not dead. Just captured. And you know what they do to people who defy them.”\n",
        "\n",
        "Vega didn’t need the reminder. The Titan shard pulsed faintly in the stasis chamber below deck, as though it could sense the danger. They had to get it off-world, away from the Council’s grip.\n",
        "\n",
        "Aris, the smuggler, adjusted the course. “Relax. I’ve walked ships like this through tighter nooses. We’ll dock under a freighter’s blind spot, transfer you to a maintenance bay, and from there… well, you’ll have to improvise.”\n",
        "\n",
        "The docking clamps hissed, and for a moment, everything seemed smooth. Then the alarms blared.\n",
        "\n",
        "Council frigates shifted position, lights sweeping across the dock like hunters scanning for prey. “Unauthorized vessel, identify immediately,” crackled a voice through the comms.\n",
        "\n",
        "Vega’s heart pounded. She activated the jammer, and the ship’s systems flickered. “We buy ourselves five minutes. No more.”\n",
        "\n",
        "They sprinted through narrow corridors as the dock’s gravity fields engaged. Workers, smugglers, and refugees crowded the station, all unaware of the shard burning bright in Mira’s satchel. The crew moved fast, blending with the throng.\n",
        "\n",
        "But just as they reached the maintenance lifts, two Council enforcers blocked their path, plasma rifles leveled.\n",
        "\n",
        "“You’re under arrest,” one of them barked. “Step away from the child and surrender.”\n",
        "\n",
        "Mira clutched the satchel tighter, her eyes defiant. The shard pulsed through the fabric, and for an instant, the lights in the corridor dimmed.\n",
        "\n",
        "Then chaos erupted. A dockworker shouted as another alarm blared — not for Vega’s crew, but for the entire station. A massive gravitational spike rattled the orbital ring, and docking arms twisted against their joints. Sirens wailed. Something was pulling at the station from orbit.\n",
        "\n",
        "Soren checked his scanner and went pale. “That’s no natural anomaly. The Council’s weapon tests… they’ve brought one up here. A graviton cannon.”\n",
        "\n",
        "The enforcers faltered, distracted by the sudden tremors. Vega seized the chance, lunging forward, knocking one rifle aside. Soren dragged Mira through the lift doors while Aris covered their retreat.\n",
        "\n",
        "As the lift shot upward, Vega steadied her breath. They had escaped the patrol — but if the Council had brought a graviton cannon to the docks, then the entire station was in danger of collapse.\n",
        "\n",
        "The shard glowed brighter, as though urging them forward.\n",
        "\n",
        "For the first time, Vega wondered if it wasn’t just an artifact — but a warning.\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 9: Shattered Orbit\":\"\"\"The maintenance lift rattled as it climbed through the orbital dock’s interior. Vega’s knuckles whitened against the railing. Below them, the whole station shook with a deep, gut-twisting rumble.\n",
        "\n",
        "When the doors slid open, a wave of smoke and shouts hit them. The docking ring was in chaos: alarms screamed, lights flickered, and civilians scattered in panic as the graviton cannon’s pull rippled through the structure.\n",
        "\n",
        "Panels tore free from the walls, floating in sudden pockets of zero gravity before slamming back down. Sparks danced like fireflies. The station groaned as if it might tear apart.\n",
        "\n",
        "“They’re firing it too close,” Soren said, his voice taut. “The cannon’s destabilizing the orbital field. If it keeps up, this dock won’t hold.”\n",
        "\n",
        "Aris cursed under his breath. “Council doesn’t care. They’ll collapse the whole ring if it means keeping control.”\n",
        "\n",
        "Vega scanned the chaos. Patrols were falling back, more concerned with saving their own ships than chasing fugitives. It was their chance. “We get to the upper bays, steal something small, and burn out of here.”\n",
        "\n",
        "But as they pushed through the panicked crowd, Mira stumbled. She had dropped the satchel. The shard rolled free, glowing with a light that no human device should have carried.\n",
        "\n",
        "Gasps rippled through those nearby. The artifact hovered an inch above the deck, trembling in sync with the station’s quakes.\n",
        "\n",
        "“Vega…” Mira whispered, clutching Vega’s sleeve. “It’s… resonating.”\n",
        "\n",
        "Before Vega could respond, the shard flared, sending a shockwave across the corridor. The tremors eased for a heartbeat, as if the artifact itself was pushing back against the graviton cannon.\n",
        "\n",
        "People froze, staring. Whispers spread: “What is that? …Did you see? …Alien tech?”\n",
        "\n",
        "Soren grabbed the shard and shoved it back into the satchel. “Now everyone knows we’re carrying something. We have to move!”\n",
        "\n",
        "They bolted toward the upper bays, ducking through smoke-filled passages, when Aris suddenly stopped. His wrist-comm blinked with a private signal. He frowned, then glanced at Vega.\n",
        "\n",
        "“What is it?” she asked.\n",
        "\n",
        "Aris hesitated. “A buyer. Council isn’t the only one after your rock. Someone’s offering me triple to hand it over right now.”\n",
        "\n",
        "Vega’s hand went to her sidearm. “You’d sell us out here? After everything?”\n",
        "\n",
        "The smuggler smirked, but there was no humor in his eyes. “Relax. I haven’t decided yet. But if the station collapses, none of us are leaving unless we choose carefully.”\n",
        "\n",
        "The floor shook again, harder this time. A viewport cracked, venting air before emergency shutters slammed down. Sirens howled louder.\n",
        "\n",
        "“Decision time’s over,” Vega snapped. “Either you’re with us, or you’re already dead.”\n",
        "\n",
        "Aris met her gaze for a long moment, then shouldered past without answering.\n",
        "\n",
        "They reached the bay doors — only to find Council mechs deploying, their metallic frames gleaming in the flashing lights. Plasma cannons powered up, blocking every exit.\n",
        "\n",
        "The shard pulsed again, brighter than ever, almost angry. Mira gasped as it shook in her arms.\n",
        "\n",
        "Vega raised her weapon, heart hammering. Between the collapsing station, the mechs, and Aris’s betrayal hanging in the air, one truth became clear:\n",
        "\n",
        "The shard wasn’t just cargo anymore.\n",
        "It was the only thing keeping them alive.\n",
        "\n",
        "And the Council knew it.\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 10: Betrayal in the Void\":\"\"\"The docking bay shook underfoot as if the entire station were coming apart at the seams. Above, half a dozen Council mechs lined the gantries, weapons trained on Vega’s crew. Red targeting beams cut through the smoke, painting glowing dots across their chests.\n",
        "\n",
        "“Drop your weapons!” boomed a mechanized voice. “Surrender the shard, and you may yet live.”\n",
        "\n",
        "No one moved. Mira clutched the satchel tighter, the shard glowing hot through the fabric. Its pulse echoed in her chest, in time with her racing heart.\n",
        "\n",
        "Vega raised her sidearm, though she knew it was useless against mechanized armor. Her mind raced: Fight? Flee? Or gamble on the shard’s strange energy again?\n",
        "\n",
        "Before she could decide, Aris stepped forward.\n",
        "\n",
        "He raised his hands slowly, letting the satchel’s strap slide from Mira’s shoulder into his grip. “Easy,” he called to the mechs. “No need for all this. I’ve got what you want.”\n",
        "\n",
        "“Aris?!” Mira’s voice cracked in disbelief.\n",
        "\n",
        "Soren snarled, lunging forward, but Vega threw out an arm to stop him. She could read the smuggler’s expression — the cool calculation of a man who lived too long in the shadows. He was making his move.\n",
        "\n",
        "The lead mech lowered its weapon slightly. “Hand it over.”\n",
        "\n",
        "Aris smirked. “Triple payment, right? Credits transferred directly to—”\n",
        "\n",
        "The shard flared violently, searing white light bursting through the satchel’s seams. Aris staggered, clutching it, eyes wide.\n",
        "\n",
        "“—what the—?!”\n",
        "\n",
        "The mechs opened fire. Plasma bolts scorched the deck around him. Aris dove for cover, dragging the satchel with him.\n",
        "\n",
        "In that instant, Vega acted. She shoved Mira and Soren toward a half-dismantled shuttle in the corner bay. “Move! Now!”\n",
        "\n",
        "They sprinted, weaving between burning crates and falling debris. Aris fired back with his pulse pistol, half at the mechs, half at Vega’s crew — trying to keep both sides at bay.\n",
        "\n",
        "“You don’t get it, Vega!” he shouted over the chaos. “This shard isn’t a relic — it’s a key! And whoever controls it, controls everything!”\n",
        "\n",
        "The shard pulsed again, and this time the entire docking bay screamed. Metal warped. Gravity shifted. A mech collapsed into itself as though crushed by unseen pressure. Others stumbled, recalibrating.\n",
        "\n",
        "Mira screamed, clutching her head. “It’s— it’s talking! I can hear—”\n",
        "\n",
        "Vega yanked her into the shuttle. Soren slammed the hatch, fingers flying over the half-dead console. Systems sputtered to life.\n",
        "\n",
        "Behind them, Aris made his choice. He sprinted not toward safety, but straight toward the lead mech, holding the shard high.\n",
        "\n",
        "“You want it?” he bellowed. “Then take it from me!”\n",
        "\n",
        "The mech’s clawed arm reached out. The shard detonated in another pulse of alien energy, hurling both Aris and the machine into the bulkhead with bone-shattering force.\n",
        "\n",
        "The bay erupted in chaos. Mechs faltered. Civilians screamed.\n",
        "\n",
        "And Vega’s stolen shuttle, engines barely holding together, roared free of the collapsing dock — leaving Aris’s fate uncertain, the shard’s power more terrifying than ever, and the Council’s grip tightening across every channel of space.\n",
        "\n",
        "Out the viewport, the graviton cannon glowed, preparing for another devastating shot.\n",
        "\n",
        "“This isn’t over,” Vega whispered, staring into the void.\n",
        "\n",
        "And in the silence that followed, the shard pulsed again.\n",
        "\n",
        "Not in warning.\n",
        "But in invitation.\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 11: Echoes in the Dark\": \"\"\"The stolen shuttle limped through the void, its hull groaning with every course correction. The stars stretched endlessly, but to Vega it felt like they were flying blind through a sea of knives.\n",
        "\n",
        "Mira sat slumped in the co-pilot’s chair, sweat beading on her forehead. The shard pulsed faintly in her lap, its light dimmer now, but its presence pressing against every thought in the cabin.\n",
        "\n",
        "Soren slammed a fist against the console. “We can’t keep running blind. Council patrols will be hunting us across every relay.”\n",
        "\n",
        "“They already are,” Vega said grimly. She angled the shuttle toward a shadow on the nav-map — a sliver of rock barely large enough to register. “There. We’ll hide in the Belt until the engines cool.”\n",
        "\n",
        "The asteroid loomed ahead, a fractured shard of some long-dead planet. Nestled in its crater was a forgotten station — half-collapsed, its beacon long dead. Perfect.\n",
        "\n",
        "The shuttle docked with a screech of tortured metal. Inside, the air was stale, gravity uneven, but it was shelter.\n",
        "\n",
        "Mira’s voice was shaky. “It spoke to me… back there. When Aris held it.”\n",
        "\n",
        "Vega crouched by her side. “What did it say?”\n",
        "\n",
        "Mira’s eyes glazed for a moment, as if still hearing the echo. “Not words. Images. A city of glass towers, under a sky the color of storms. And a door — locked, waiting. I think… I think the shard is the key.”\n",
        "\n",
        "Soren snorted. “A key to what? More trouble?”\n",
        "\n",
        "Before Mira could answer, a voice drifted from the shadows.\n",
        "\n",
        "“You’re half-right.”\n",
        "\n",
        "Weapons snapped up instantly. A figure stepped into the flickering light — an old man, gaunt, wrapped in a patched exo-suit. His eyes gleamed with the same strange shimmer as the shard.\n",
        "\n",
        "“I’ve been waiting for someone to bring it here,” he said softly. “Waiting a very long time.”\n",
        "\n",
        "Vega didn’t lower her gun. “Who the hell are you?”\n",
        "\n",
        "The man smiled faintly. “Call me Eryndor. Once, I was part of the Council’s science corps. Before they buried the truth.”\n",
        "\n",
        "His gaze locked on the shard. It pulsed, brighter now, almost… eager.\n",
        "\n",
        "“That,” Eryndor whispered, “is not just a relic. It’s one fragment of a network older than humanity itself. A network that spans worlds. Titan was only the first to whisper.”\n",
        "\n",
        "The silence in the station thickened.\n",
        "\n",
        "Mira clutched the shard tighter. “If this is true… then Aris was right. Whoever controls it—”\n",
        "\n",
        "“—controls the voice of the ancients,” Eryndor finished. His expression hardened. “But be warned. The shard chooses. It doesn’t obey.”\n",
        "\n",
        "Soren shifted uneasily. “And what if it doesn’t choose us?”\n",
        "\n",
        "Eryndor’s smile didn’t reach his eyes. “Then it will break you. The Council fears this, but they don’t understand it. You might. If you live long enough.”\n",
        "\n",
        "The station groaned around them, as if the asteroid itself resented the weight of his words.\n",
        "\n",
        "Vega lowered her weapon — but only slightly. The path ahead was clearer now, and more dangerous than ever.\n",
        "\n",
        "For the shard was not merely a tool.\n",
        "It was an invitation.\n",
        "And someone, somewhere, was waiting for them to open the door.\"\"\",\n",
        "\"🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 12: The Shard’s Choice\":\"\"\"\n",
        "The asteroid station creaked under the weight of its own decay. Vega paced the cracked metal floor, every instinct screaming that they couldn’t linger.\n",
        "\n",
        "Eryndor stood by the viewport, gazing out at the endless dark. “The Council won’t stop. They fear what they don’t control.”\n",
        "\n",
        "Soren spat. “You talk like you’re not one of them.”\n",
        "\n",
        "“I was,” Eryndor said, his voice tired. “Until I saw what the shards could do. They’re not weapons, not tools. They’re bridges. To knowledge, to power… to something beyond the human frame.”\n",
        "\n",
        "Mira clutched the shard as though it might slip away. Its glow was no longer faint. It pulsed with steady rhythm, like a second heart.\n",
        "\n",
        "Suddenly, alarms shrieked through the derelict station. Red light bled into the shadows.\n",
        "\n",
        "“Council drop-ships!” Vega snapped, slamming her helmet on. “They’ve tracked us!”\n",
        "\n",
        "Through the cracked viewport, sleek ships cut across the void, their engines blazing cold blue. The noose was tightening.\n",
        "\n",
        "Soren cursed, grabbing his rifle. “We fight?”\n",
        "\n",
        "Eryndor raised a hand. “No. The shard has already chosen.”\n",
        "\n",
        "The pulse in Mira’s hands intensified, vibrating through the metal around them. Panels in the station walls lit up, ancient symbols etching themselves across dead circuitry. The asteroid itself seemed to awaken.\n",
        "\n",
        "Mira’s eyes widened in fear and awe. “It’s calling something.”\n",
        "\n",
        "The floor trembled. Beneath the station, a hidden hangar yawned open, revealing a vessel unlike anything Vega had ever seen. Sleek, alien, its surface shimmered as though half in this reality, half out.\n",
        "\n",
        "The Council’s voice crackled over comms: “Surrender the relic, or be destroyed.”\n",
        "\n",
        "Vega glanced at Mira, then at the ship below. Their options were clear — and terrifying.\n",
        "\n",
        "“We can’t fight them head-on,” Vega said. “That thing… it’s our only chance.”\n",
        "\n",
        "Eryndor’s gaze never left the shard. “It is not a chance. It is a test. Enter, and it will bind to you. Or it will tear you apart.”\n",
        "\n",
        "The station shuddered as Council cannons began their barrage. Shards of rock splintered off into space.\n",
        "\n",
        "Mira stood, the shard blazing in her arms. Her voice was steady now. “Then we have to risk it. If we don’t… this ends here.”\n",
        "\n",
        "For a heartbeat, time seemed to hold. The fugitives looked at one another — hunted, broken, but unyielding.\n",
        "\n",
        "Then Vega spoke, her voice sharp with command. “To the ship. Now.”\n",
        "\n",
        "They ran through collapsing corridors as fire rained from the Council’s guns. The alien vessel pulsed in rhythm with the shard, its hull opening like a living thing to receive them.\n",
        "\n",
        "The moment Mira stepped aboard, the shard dissolved into light, flowing into the ship itself. Consoles lit, engines roared, and the vessel rose, tearing free of its asteroid cradle.\n",
        "\n",
        "Council ships swarmed in pursuit.\n",
        "\n",
        "But when Vega’s hands touched the controls, the stars bent. Space itself folded like a page.\n",
        "\n",
        "The alien vessel vanished — leaving only empty dark, shattered rock, and the Council’s rage echoing in silence.\n",
        "\n",
        "Far from the Belt, somewhere uncharted, the ship emerged.\n",
        "\n",
        "Its lights dimmed, systems humming like a heartbeat.\n",
        "And in the silence, a voice — not Mira’s, not Eryndor’s — filled the cabin:\n",
        "\n",
        "“You have been chosen. The Signal must continue.”\n",
        "\n",
        "The fugitives stared at one another. For the first time, they were not just running.\n",
        "They were part of something vast, and ancient, and unstoppable.\n",
        "\n",
        "The chapter ends here.\n",
        "The real journey had only begun.\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan — Chapter Two: The Path of Stars\":\"\"\"Core Theme: The fugitives discover that the shard is part of the Celestial Dao of the Cosmos, a cultivation path practiced by ancient civilizations that merged spiritual essence with stellar power. To survive against galactic empires, they must train, ascend, and unlock cosmic techniques — while hunted by powers who fear what they could become.\n",
        "\n",
        "Structure: 48 Episodes, grouped into Four Arcs (12 episodes each).\n",
        "\n",
        "Arc I: Awakening the Dao of the Shard (Ep. 1–12)\n",
        "\n",
        "Episode 1–3: The alien ship takes them to a hidden star system, where ruins of a lost cultivation civilization remain. Mira begins sensing meridians of light within the cosmos.\n",
        "\n",
        "Episode 4–6: Each fugitive experiences their first cultivation trial — Vega awakens the Path of the Blade Star, Soren the Path of the War Titan, Mira the Path of the Eternal Shard.\n",
        "\n",
        "Episode 7–9: Eryndor reveals he once studied fragments of these teachings, but never completed them. His loyalty comes into question.\n",
        "\n",
        "Episode 10–12 (Arc climax): The Council sends an elite fleet, but the fugitives, now in their early Star Qi Foundation stage, repel them by combining Dao techniques with the ship’s alien power.\n",
        "\n",
        "Arc II: The Cosmic Tribulations (Ep. 13–24)\n",
        "\n",
        "Episode 13–15: The fugitives journey to the Nebula Labyrinth, where cosmic beasts feed on spirit essence. Cultivation requires defeating them to refine Qi.\n",
        "\n",
        "Episode 16–18: Introduction of Galactic Powers:\n",
        "\n",
        "The Eternal Council (technocrats)\n",
        "\n",
        "The Star Sovereigns (warlord-kings)\n",
        "\n",
        "The Lotus Order (mystic monks bridging cultivation & science)\n",
        "\n",
        "Episode 19–21: Mira faces her first tribulation lightning, nearly dying as the ship channels celestial energy to shield her.\n",
        "\n",
        "Episode 22–24 (Arc climax): Betrayal — Eryndor is revealed to have made a secret pact with the Lotus Order. The fugitives must choose whether to trust him again.\n",
        "\n",
        "Arc III: The Dao Wars of the Galaxy (Ep. 25–36)\n",
        "\n",
        "Episode 25–27: The fugitives begin to master techniques like Starblade Qi, Titanic Body Refinement, and Shard Dao Insight.\n",
        "\n",
        "Episode 28–30: The galaxy plunges into chaos as factions clash over shards rumored to be spread across the cosmos.\n",
        "\n",
        "Episode 31–33: The fugitives encounter other cultivators — some allies, others deadly rivals who see them as heretics for fusing mysticism with alien tech.\n",
        "\n",
        "Episode 34–36 (Arc climax): A grand war breaks out near the Black Sun Star, where Mira faces a Cosmic Cultivator Emperor in battle, barely surviving after unlocking the Shard’s hidden Dao Art.\n",
        "\n",
        "Arc IV: Ascension Beyond the Stars (Ep. 37–48)\n",
        "\n",
        "Episode 37–39: The fugitives seek the Cosmic Pagoda of Ten Thousand Realms, a cultivation fortress drifting between galaxies.\n",
        "\n",
        "Episode 40–42: Each must face inner demons and illusions to ascend — Vega confronts her lust for vengeance, Soren his fear of insignificance, Mira her doubt that she deserves the Shard’s choice.\n",
        "\n",
        "Episode 43–45: With breakthroughs into the Galactic Core Realm, they bend space itself, drawing cosmic essence directly from stars.\n",
        "\n",
        "Episode 46–47: The Council and rival factions unite for a final assault, unleashing Void Weapons capable of destroying worlds.\n",
        "\n",
        "Episode 48 (Chapter climax): The fugitives ascend together, binding their Daos into the living ship. The Shard reveals itself as one of Nine Celestial Relics — and the Signal was not just a call for survival, but an invitation to join the eternal war of the cosmos.\n",
        "\n",
        "Chapter Two Summary:\n",
        "\n",
        "This arc blends:\n",
        "\n",
        "Mystic cultivation tropes: Qi refinement, tribulations, inner demons, cosmic pagodas, immortal realms.\n",
        "\n",
        "Sci-fi scale: galactic empires, fleets, void weapons, alien relics.\n",
        "\n",
        "Themes: trust, betrayal, unity in cultivation, the cost of transcendence.\n",
        "\n",
        "By Episode 48, our fugitives are no longer just survivors — they are Cosmic Cultivators, poised to reshape galactic history.\"\"\",\n",
        "\"The Signal from Titan: Chapter Two: The Path of Stars: Episode 1 — The Silent Constellation\":\"\"\"The alien ship glided into a region of space that was not on any Council map. The stars here shimmered strangely, like pearls submerged in dark water. Nebulas pulsed with a rhythm that felt almost alive.\n",
        "\n",
        "Mira pressed her hand against the viewport. “It feels like the stars are… breathing.”\n",
        "\n",
        "Vega frowned, her hand near the hilt of her blade. “Stars don’t breathe.”\n",
        "\n",
        "But Soren shook his head. “No… she’s right. I can feel it too.” His voice carried awe, not fear. “The light—it’s flowing. Like blood in cosmic veins.”\n",
        "\n",
        "At the ship’s center, the Shard pulsed. Its glow resonated with the starlight outside, as if it were humming in unison with the constellation. Symbols appeared briefly in the air, spirals of light no one had seen before.\n",
        "\n",
        "Eryndor, his face pale, muttered, “We’ve entered the Celestial Dao Belt.”\n",
        "\n",
        "“The what?” Vega snapped.\n",
        "\n",
        "“A hidden domain,” Eryndor explained. “Long ago, a civilization thrived here. They believed the cosmos itself was a body—stars as acupoints, galaxies as meridians, black holes as the heart. They called it… cultivation of the Great Dao of the Stars.”\n",
        "\n",
        "Mira’s eyes widened. The words resonated deep in her chest. “And the Shard…?”\n",
        "\n",
        "“An artifact,” Eryndor said, lowering his voice. “Forged to connect mortals to the cosmos. A path to ascension. But it was thought destroyed in the First Galactic War.”\n",
        "\n",
        "The ship suddenly shifted, pulled toward a drifting husk of a planet. Broken towers and shattered palaces floated in orbit like bones around a corpse. Yet, glowing script still lined the ruins—runes of Qi cultivation, shimmering faintly after millennia.\n",
        "\n",
        "The fugitives disembarked. The ground crumbled like ash beneath their boots, but the air hummed with unseen energy. Mira felt warmth flood her veins, her heartbeat aligning with the stars above.\n",
        "\n",
        "Vega tightened her grip on her weapon. “I don’t like this place. Feels like a trap.”\n",
        "\n",
        "“It’s not a trap,” Mira whispered. She touched a fragment of a broken wall. The script flared, and energy surged through her. Her vision blurred.\n",
        "\n",
        "For a moment, she was no longer Mira. She was drifting in the void, her body dissolving into starlight, flowing along endless rivers of cosmic Qi. She saw nine celestial relics suspended across galaxies, connected by invisible threads of destiny. The Shard in her hand was one of them.\n",
        "\n",
        "“Mira!” Soren’s voice yanked her back. She gasped, trembling, the glow fading.\n",
        "\n",
        "“What happened?” Vega demanded.\n",
        "\n",
        "“I saw… the Dao,” Mira said, still dazed. “And it saw me.”\n",
        "\n",
        "The ruins shuddered, dust spiraling upward like smoke. From the shadows emerged spectral guardians—giant figures of light and stone, their eyes burning with starfire. Ancient protectors, left behind to guard this sacred place.\n",
        "\n",
        "The Shard blazed in Mira’s hand, and the ship behind them thrummed in response.\n",
        "\n",
        "Soren clenched his fists. “Looks like we’re not welcome here.”\n",
        "\n",
        "Vega smirked, drawing her blade. “Good. I’ve been itching for a fight.”\n",
        "\n",
        "The guardians raised their weapons of light, and the ground split beneath their feet as cosmic energy surged.\n",
        "\n",
        "Mira tightened her grip on the Shard, fear and wonder mixing inside her. She knew this was not just another battle. This was a trial of the Dao.\n",
        "\n",
        "And the first step of a path that could either destroy them… or make them something greater than mortal.\"\"\",\n",
        "\"🌌The Signal from Titan — Chapter Two: Episode 2: Trial of the Star Guardians\":\"\"\"The alien ship drifted in silence above the fractured planet, its surface scarred by ancient battles. Mira stood at the viewport, her hands pressed against the glass, gazing at the ruins below. Stone pillars rose like broken teeth from the planet’s crust, glowing faintly with runes that pulsed in rhythm with her heartbeat.\n",
        "\n",
        "“They’re calling to us,” she whispered.\n",
        "\n",
        "Vega crossed her arms, unimpressed. “Or it’s just old tech bleeding energy. Don’t romanticize it.”\n",
        "\n",
        "Soren frowned, his massive frame tense. “No… she’s right. I feel it too. Like a storm waiting for us.”\n",
        "\n",
        "The ship lowered them onto a cracked plateau. Dust swirled as they stepped out, their boots echoing on ancient stone. The ruins stretched before them — an open courtyard ringed by shattered statues. Each statue depicted a warrior, some human-like, others alien, all standing proud with weapons raised to the stars.\n",
        "\n",
        "Mira’s breath caught. The Shard inside her chest pulsed with light. “This place is alive.”\n",
        "\n",
        "Before Vega could retort, the statues’ eyes flared with golden fire. Cracks spread across their bodies, and with a deafening roar, the guardians moved. Stone skin split to reveal inner cores of starlight. Weapons forged from energy ignited in their hands.\n",
        "\n",
        "“Ambush!” Vega shouted, unsheathing her blade.\n",
        "\n",
        "The first guardian struck, swinging a sword of light. Vega blocked, sparks flying. The impact drove her to her knees. She gritted her teeth, pushed back, and countered with a strike so fierce the stone cracked — yet the guardian did not fall.\n",
        "\n",
        "Another lunged at Soren, wielding a hammer that glowed with cosmic fire. He raised his arms to block, but the blow knocked him across the courtyard. He coughed, blood in his mouth, yet his veins burned with something new — raw, surging power.\n",
        "\n",
        "Mira tried to summon the Shard’s light, but the guardians moved too fast. One charged her, spear raised. She screamed, and the Shard burst outward, a shield of radiant energy snapping into place. The spear shattered against it, sending shockwaves through the courtyard.\n",
        "\n",
        "The guardians regrouped, circling them like predators.\n",
        "\n",
        "“They’re testing us,” Soren grunted, wiping blood from his chin. “Not trying to kill us — not yet.”\n",
        "\n",
        "“Some test,” Vega growled, lunging again. This time, her blade shimmered faintly, catching stray fragments of starlight. Each strike grew sharper, faster, more precise, as though the ruins themselves guided her hand.\n",
        "\n",
        "Soren rose, fists clenched. His muscles swelled with unnatural strength, glowing cracks appearing on his skin like molten veins. He roared and caught the hammer mid-swing, wrenching it from the guardian’s hands and smashing the stone warrior apart. The fragments dissolved into stardust.\n",
        "\n",
        "Mira felt the Shard pulsing with rhythm — each beat matching her heartbeat. Closing her eyes, she drew the energy inward, shaping it not as a shield but as a surge of light. When she opened her eyes again, a wave of brilliance exploded from her hands, staggering the guardians and reducing two to rubble.\n",
        "\n",
        "One last guardian remained, taller than the rest, its core blazing like a small star. It raised its sword toward Mira, but instead of striking, it lowered the blade and knelt.\n",
        "\n",
        "The courtyard fell silent.\n",
        "\n",
        "The remaining starlight faded, leaving behind only dust and broken stone. Yet something lingered — threads of energy weaving into Vega’s blade, settling into Soren’s blood, and nesting deep within Mira’s soul.\n",
        "\n",
        "“They weren’t enemies,” Mira whispered. “They were guardians… waiting for us.”\n",
        "\n",
        "Vega exhaled, her blade dimming. “If this is just the beginning, I don’t want to see the next test.”\n",
        "\n",
        "Soren grinned despite his bruises, the glow in his veins fading slowly. “No. I do. For the first time… I feel alive.”\n",
        "\n",
        "Above them, the sky shimmered, constellations shifting as if the stars themselves acknowledged their victory.\n",
        "\n",
        "The trials had begun.\"\"\",\n",
        "\"🌌 The Signal from Titan - Chapter Two: The Path of Stars: Episode 3 – First Steps on the Cosmic Path\":\"\"\"The chamber’s glow shifted as though responding to their breaths. Patterns on the walls pulsed, dimming whenever doubt crept into their hearts and brightening when their will sharpened. The fugitives stood still, the silence heavy with awe.\n",
        "\n",
        "Vega clenched her fists. “I can feel it… like starlight pressing against my veins. It burns, but it also—” She broke off, eyes narrowing as a faint edge of light flared along her arm. A blade, not forged of metal, but of pure radiance, flickered into existence before fading.\n",
        "\n",
        "Soren staggered backward, his broad frame trembling. “Mine… mine is different. Not sharp. Heavy.” The ground beneath him cracked as invisible pressure rippled outward. It was as though the weight of a collapsing star had brushed the chamber, bending gravity itself. He fell to one knee, gasping, sweat dripping down his brow.\n",
        "\n",
        "Mira watched them, her own pulse in rhythm with the chamber. The Shard within her heart glowed faintly, and she saw lines of light extending from her chest outward, webbing into the room. Unlike Vega’s blade or Soren’s strength, hers felt endless, stretching far beyond the walls, into the cosmos itself. She whispered, “It’s like the stars are listening.”\n",
        "\n",
        "Eryndor had been silent. At last, he stepped forward, his eyes dark with something unreadable. “These are cultivation roots. Long before your Council ruled, before your colonies spread, there were civilizations who walked the Dao of the Cosmos. They wove essence from the stars, refined their bodies into living vessels of light and void.” His voice trembled with reverence, but also a shadow of fear.\n",
        "\n",
        "Vega turned sharply. “You knew?”\n",
        "\n",
        "“I suspected,” Eryndor admitted. “The glyphs, the relics the Council buried… I studied fragments. But I never advanced. Without a Shard to awaken the meridians, the Dao was only myth.”\n",
        "\n",
        "Mira’s eyes narrowed. “And now? You’re saying we’ve awakened it?”\n",
        "\n",
        "Before he could answer, the chamber shook. Above them, the alien ship hummed louder, as if resonating with their breakthroughs. A beam of starlight pierced the ceiling, surrounding them in radiant mist. Each breath they took was no longer just air—it was energy, threaded with power.\n",
        "\n",
        "A voice—neither male nor female, neither alien nor human—echoed in their minds:\n",
        "\n",
        "“Step into the Foundation. Choose your Path. The cosmos will shape you, as you shape it.”\n",
        "\n",
        "They froze, staring at one another. Vega smirked, lifting her chin. “Path of the Blade Star. That one’s mine.”\n",
        "\n",
        "Soren wiped blood from his lip, his voice a low growl. “War Titan. I don’t know why, but it feels right.”\n",
        "\n",
        "Mira pressed a hand against her chest. The Shard pulsed with warmth. Her path was already chosen—the Path of the Eternal Shard.\n",
        "\n",
        "And Eryndor… his silence lingered a moment too long. His eyes darted to the glowing patterns, then back to Mira. “Some doors should never be opened,” he muttered, even as the light wrapped around him, binding him to a choice he did not speak aloud.\n",
        "\n",
        "The starlight flared once more, burning the chamber into brilliance. When it dimmed, their bodies felt different—denser, stronger, alive with cosmic essence. They had stepped onto the first rung of cultivation: the Star Qi Foundation Stage.\n",
        "\n",
        "But outside, in the endless dark, something stirred. For the first time, the Council’s long-range scanners detected a spike of impossible energy. A signal flared across the galaxy. Their ascension had not gone unnoticed.\"\"\",\n",
        "\"🌌 The Signal from Titan - Chapter Two: The Path of Stars: Episode 4 – The Trials of the First Step\":\"\"\"The ruins shifted as if alive, corridors rearranging themselves into a maze of radiant stone. The fugitives were pulled apart, each swallowed by a passage that seemed tailored to them.\n",
        "\n",
        "Mira shouted Vega’s name, but her voice was swallowed by silence. The Shard inside her pulsed steadily, almost reassuring. So this is the trial…\n",
        "\n",
        "🌠 Vega’s Trial — The Path of the Blade Star\n",
        "\n",
        "She found herself standing in a vast plain beneath a crimson sky. Shadows rose from the ground, faceless warriors armed with weapons of light. A blade shimmered into existence in her hand, forged from her will.\n",
        "\n",
        "The first warrior charged. Vega blocked, but the impact sent fire up her arm. She bit back a curse. “So this is how it works—every doubt, every weakness, they’ll cut me down unless I master it.”\n",
        "\n",
        "More shadows surged forward. Her blade burned brighter with each swing, fueled not by anger, but by resolve. The more she believed in her right to fight, the sharper her strikes became. When the last warrior fell, her blade remained steady in her grasp, no longer flickering. She whispered, “I am the blade.”\n",
        "\n",
        "🌌 Soren’s Trial — The Path of the War Titan\n",
        "\n",
        "Soren stood on a broken battlefield littered with shattered armor. Above him loomed a colossal figure—his own reflection, magnified to monstrous size, eyes glowing with cold judgment.\n",
        "\n",
        "The giant sneered. “You fear you’re nothing but muscle. That your strength is empty.”\n",
        "\n",
        "Soren roared, trying to swing his fists, but every blow shattered his bones. The giant’s strikes crushed him again and again, forcing him to his knees. Blood filled his mouth.\n",
        "\n",
        "But then—he laughed, spitting red onto the ground. “You think I fear weakness? Every scar, every break—that’s how I grow stronger!”\n",
        "\n",
        "With each word, his body mended faster. His frame glowed with Titan Qi, his muscles hardening into living armor. He charged the giant once more. When the dust settled, only Soren remained standing, his eyes blazing with unyielding will.\n",
        "\n",
        "✨ Mira’s Trial — The Path of the Eternal Shard\n",
        "\n",
        "Mira drifted in a void of stars. She felt both infinite and unbearably small, her body dissolving into motes of light.\n",
        "\n",
        "A voice whispered all around her: You do not deserve the Shard. You are only a vessel. A mistake.\n",
        "\n",
        "Her chest tightened. For a moment, she almost believed it. The Shard’s light dimmed.\n",
        "\n",
        "Then she remembered Vega’s fire, Soren’s laughter, the promise she had made to herself on Titan—to live free, to defy the Council. “Maybe I am small,” she whispered. “But even a single star burns in the darkness.”\n",
        "\n",
        "The Shard flared, flooding her body with brilliance. Lines of starlight carved across her skin, forming luminous meridians that connected her heart to the cosmos. Her doubt shattered. She was chosen—because she chose to be.\n",
        "\n",
        "🌀 Eryndor’s Trial — A Path Unknown\n",
        "\n",
        "Alone in the labyrinth, Eryndor faced a different trial. A door appeared before him, glowing faintly with Lotus sigils. He reached for it with a trembling hand, and the whisper of an old bargain coiled in his ears.\n",
        "\n",
        "“This is your chance,” the voice said. “Serve the Lotus, and live. Defy them, and die with the children.”\n",
        "\n",
        "His eyes darkened. His trial was not against shadows, or giants, or doubt—it was against the weight of loyalty.\n",
        "\n",
        "And no one knew what choice he made.\n",
        "\n",
        "When the fugitives emerged, the ruins released them into the central chamber once more. Their bodies radiated a faint aura, their eyes sharper, their breathing deeper.\n",
        "\n",
        "They had passed the first trials of the Star Qi Foundation Stage.\n",
        "\n",
        "But far across the galaxy, alarms blared in the Eternal Council’s fleet command. The energy readings were undeniable—cultivators had been reborn.\n",
        "\n",
        "And the Council would not allow them to exist.\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "\"\":\"\"\"\"\"\",\n",
        "}\n",
        "\n",
        "# Convert the dictionary of documents into a list of Document objects\n",
        "documents_list = [LangchainDocument(page_content=text, metadata={\"title\": title}) for title, text in documents.items()]\n",
        "\n",
        "# Display the first document to show the format\n",
        "print(documents_list[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggCEKKufpjYh",
        "outputId": "e8ed5946-75fd-48c2-a330-ba4e5407ecfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='\n",
            "\n",
            "In the year 2145, humanity had colonies on Mars, mining stations in the asteroid belt, and research outposts circling Jupiter. But Titan, Saturn’s largest moon, was still a mystery. Its thick orange atmosphere hid secrets that no probe had ever fully explained.\n",
            "\n",
            "Captain Liora Vega was chosen to lead the first human crew to Titan. Her ship, the Aurora, carried six explorers, each trained in different sciences. The journey took three long years, but they were ready. Humanity wanted answers: Was Titan dead rock, or could something live in its frozen seas?\n",
            "\n",
            "When the Aurora entered Titan’s orbit, the crew gasped. The orange clouds swirled like painted fire. Rivers of liquid methane reflected Saturn’s pale light. It was beautiful but also alien—nothing like Earth.\n",
            "\n",
            "On the second day, while setting up their base, the crew received a signal. It was faint, rhythmic, and unlike any natural sound. “It’s not background noise,” said Arjun, the communications officer. “Someone—or something—is transmitting.”\n",
            "\n",
            "The crew debated. Some thought it was an echo from Saturn’s magnetosphere. Others whispered about aliens. Captain Vega decided to follow the signal. It led them to a frozen lake, covered in cracked ice. Underneath the surface, faint lights pulsed in perfect rhythm with the signal.\n",
            "\n",
            "Dr. Kiera, the biologist, lowered a drone beneath the ice. The camera showed strange glowing shapes, moving like schools of fish. “They’re alive,” she whispered. “And they’re intelligent.” The glowing beings changed patterns in response to the drone’s light, as if trying to talk.\n",
            "\n",
            "Suddenly, the ship’s computer detected a translation. The pulses formed repeating sequences, close to mathematical primes. It wasn’t just life—it was communication.\n",
            "\n",
            "For days, the crew worked to respond. They sent pulses of light, simple patterns, then sequences of numbers. The beings replied, faster each time. Arjun grew pale. “They’re smarter than we are. They learn quicker than our AI.”\n",
            "\n",
            "Then the beings sent an image—burning cities, dark skies, and oceans turning black. It wasn’t Titan. It looked like Earth.\n",
            "\n",
            "“Are they warning us?” Captain Vega asked. “Or is this what they did to their own world?”\n",
            "\n",
            "The crew argued deep into the night. Some said the beings were friends, offering protection. Others feared they were predators, showing power. The tension nearly tore the team apart.\n",
            "\n",
            "On the tenth day, the beings sent another image: a single human figure standing with glowing creatures beside them, facing the stars. It felt like an invitation.\n",
            "\n",
            "Captain Vega made her decision. “We came to explore, not to fear. If they wish to speak, we will answer.” She stepped onto the frozen lake, carrying a light beacon. The ice glowed under her boots. The beings rose, surrounding her in a circle of radiant blue.\n",
            "\n",
            "The signal grew louder, filling the air like a song. Vega’s body shook as the beacon pulsed in her hand. Then her voice came through the radio, calm but strange:\n",
            "\n",
            "“They are not from Titan. They travel between stars. They see us, and they want us to join. But they warn us: Earth is close to the same mistake they made. If we continue, our oceans will die too.”\n",
            "\n",
            "The crew was silent. Humanity had reached out into the stars—and the stars had answered, not with weapons, but with a warning.\n",
            "\n",
            "The Aurora left Titan months later, carrying data, recordings, and one message to all of Earth: Change, or perish.\n",
            "\n",
            "Captain Vega stood at the window of the ship, watching Saturn shrink into the black. She knew this was not the end, but the beginning of something greater—an alliance written in light, across the universe.\n",
            "' metadata={'title': '🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 1: The Beginning'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ddgs --quiet\n",
        "# =============================================================================\n",
        "# PYDANTIC MODELS\n",
        "# =============================================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Pydantic Models for Grading\n",
        "# ----------------------------\n",
        "class RetrievalGrade(BaseModel):\n",
        "    score: int = Field(..., description=\"Binary relevance: 1 = relevant, 0 = not relevant.\")\n",
        "    verdict: str = Field(..., description=\"'yes' or 'no' (mirror of score).\")\n",
        "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0.00-1.00).\")\n",
        "    rationale: str = Field(..., description=\"Step-by-step rationale (max ~120 words).\")\n",
        "    highlights: Optional[List[str]] = Field(None, description=\"Up to 3 short excerpts (≤30 words each).\")\n",
        "    refined_query: Optional[str] = Field(None, description=\"Refined query if score == 0 or low confidence.\")\n",
        "\n",
        "# Enums for consistent categorization\n",
        "class DocumentSource(str, Enum):\n",
        "    \"\"\"Source types for a document.\"\"\"\n",
        "    KNOWLEDGE_BASE = \"knowledge_base\"\n",
        "    EXTERNAL_API = \"external_api\"\n",
        "    WEB_CRAWL = \"web_crawl\"\n",
        "    SEARCH_SNIPPET = \"search_snippet\"\n",
        "    CACHE = \"cache\"\n",
        "\n",
        "class ContentSource(str, Enum):\n",
        "    \"\"\"Semantic categories assigned by LLM (e.g., academic, blog, forum).\"\"\"\n",
        "    ACADEMIC = \"academic\"\n",
        "    GOVERNMENT = \"government\"\n",
        "    WIKI = \"wikipedia\"\n",
        "    GIT_REPO = \"git_repo\"\n",
        "    NEWS = \"news\"\n",
        "    BLOG = \"blog\"\n",
        "    REPORT = \"report\"\n",
        "    FORUM = \"forum\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "# Sub-Models for structured data\n",
        "class DocumentResult(BaseModel):\n",
        "    \"\"\"Standardized document result with metadata.\"\"\"\n",
        "    page_content: str = Field(..., description=\"The document content.\")\n",
        "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Metadata associated with the document.\")\n",
        "    content_length: Optional[int] = Field(default=0, ge=0, description=\"Content length in characters.\")\n",
        "    source_type: DocumentSource = Field(default=DocumentSource.SEARCH_SNIPPET, description=\"The origin of the document content.\")\n",
        "    content_source: Optional[ContentSource] = Field(default=ContentSource.OTHER, description=\"The semantic category of the content.\")\n",
        "    relevance_score: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"A score indicating relevance.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    document_id: Optional[str] = Field(default=None, description=\"Stable unique identifier for the document.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def compute_content_length(self) -> \"DocumentResult\":\n",
        "        \"\"\"Ensures content_length is computed from page_content if missing/zero.\"\"\"\n",
        "        if self.page_content and (not self.content_length or self.content_length == 0):\n",
        "            object.__setattr__(self, \"content_length\", len(self.page_content))\n",
        "\n",
        "        #  Always ensure document_id is stable (sha256 of content)\n",
        "        if not self.document_id:\n",
        "            did = hashlib.sha256(self.page_content.encode(\"utf-8\")).hexdigest()\n",
        "            object.__setattr__(self, \"document_id\", did)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def to_langchain(self):\n",
        "        \"\"\"Convert to LangChain Document.\"\"\"\n",
        "        from langchain_core.documents import Document\n",
        "        return Document(page_content=self.page_content,\n",
        "                        metadata=self.metadata, id=self.document_id)\n",
        "\n",
        "    @classmethod\n",
        "    def from_langchain(cls, doc: \"Document\", relevance_score: Optional[float] = None) -> \"DocumentResult\":\n",
        "        \"\"\"Build from a LangChain Document (reverse of to_langchain).\"\"\"\n",
        "        id = doc.id or hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest()\n",
        "        timestamp = doc.metadata.get(\"created_at\") or datetime.now(timezone.utc)\n",
        "        return cls(\n",
        "            page_content=doc.page_content,\n",
        "            metadata=doc.metadata,\n",
        "            relevance_score=relevance_score,\n",
        "            created_at=timestamp,\n",
        "            document_id=id,\n",
        "        )\n",
        "\n",
        "\n",
        "class VectorStoreResponse(BaseModel):\n",
        "    \"\"\"Unified response schema for vector store operations.\"\"\"\n",
        "\n",
        "    query: Optional[str] = None  # retrieval query\n",
        "    results: Optional[List[\"DocumentResult\"]] = None  # retrieval results\n",
        "    status: str = Field(..., description=\"Operation status: success or failed.\")\n",
        "    message: str = Field(..., description=\"Human-readable summary of the result.\")\n",
        "    document_ids: List[str] = Field(default_factory=list, description=\"List of indexed document IDs.\")\n",
        "    indexed_documents: List[\"DocumentResult\"] = Field(default_factory=list, description=\"The full indexed documents.\")\n",
        "    error: Optional[str] = Field(default=None, description=\"Error message if any.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def validate_exclusive_fields(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure that document_ids/indexed_documents and results are mutually exclusive.\"\"\"\n",
        "        if self.results and (self.document_ids or self.indexed_documents):\n",
        "            raise ValueError(\"Response cannot contain both retrieval results and indexing results.\")\n",
        "        return self\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def sync_document_ids(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure document_ids matches indexed_documents' IDs if provided.\"\"\"\n",
        "        if self.indexed_documents:\n",
        "            ids = [doc.document_id for doc in self.indexed_documents if doc.document_id]\n",
        "            object.__setattr__(self, \"document_ids\", ids)\n",
        "        return self"
      ],
      "metadata": {
        "id": "__kR6tjHuFf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1faa78a3-94b7-4d05-df37-93a5371af729"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VECTOR STORE**\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n"
      ],
      "metadata": {
        "id": "rrJ25xmQtg-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pymermaid --quiet"
      ],
      "metadata": {
        "id": "Mn626EinCSCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39936d1f-655e-41a2-f301-1057fe6b9804"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from typing import List, Optional, Any, Dict, Union\n",
        "from pydantic import BaseModel, Field, model_validator\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, RetryError\n",
        "\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "EMBEDDING_PROVIDERS = str  # Or use Enum if you prefer\n",
        "\n",
        "class VectorStoreConfig(BaseModel):\n",
        "    chroma_persist_directory: str = Field(default=\"./chroma_db\", description=\"The directory where ChromaDB data will be persisted.\")\n",
        "    collection_name: str = Field(default=\"knowledge_agent_collection\", description=\"The name of the ChromaDB collection.\")\n",
        "    embedding_model_provider: EMBEDDING_PROVIDERS = Field(default=\"sentence-transformer\", description=\"The provider for the embedding model.\")\n",
        "    embedding_model_name: str = Field(default=\"all-MiniLM-L6-v2\", description=\"The name of the embedding model to use.\")\n",
        "    relevance_threshold: float = Field(default=0.8, description=\"The similarity threshold for filtering relevant documents.\")\n",
        "\n",
        "class VectorStoreManager:\n",
        "    \"\"\"\n",
        "    Manages the lifecycle and tool creation for the vector store.\n",
        "    Encapsulates the setup, indexing, and retrieval logic.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)  # class-level logger\n",
        "\n",
        "    def __init__(self, config: VectorStoreConfig, initial_documents: Optional[List[LangchainDocument]] = None):\n",
        "        self.config = config\n",
        "        self.vectorstore = self._setup_vector_store()\n",
        "        if initial_documents:\n",
        "            # Index initial documents upon creation\n",
        "            self._index_initial_documents(initial_documents)\n",
        "\n",
        "        if not self.logger.hasHandlers():\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def _get_embedding_model(self, cfg: VectorStoreConfig):\n",
        "        if cfg.embedding_model_provider == \"huggingface\":\n",
        "            return HuggingFaceEmbeddings(model_name=cfg.embedding_model_name)\n",
        "        elif cfg.embedding_model_provider == \"sentence-transformer\":\n",
        "            return SentenceTransformerEmbeddings(model_name=cfg.embedding_model_name)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown embedding provider: {cfg.embedding_model_provider}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _prefixed_sha256(x: Union[str, bytes]) -> bytes:\n",
        "        prefix = b\"kb_vectorstore:\"\n",
        "        if isinstance(x, str):\n",
        "            x = x.encode(\"utf-8\")\n",
        "        return hashlib.sha256(prefix + x).digest()\n",
        "\n",
        "    def _setup_vector_store(self) -> Chroma:\n",
        "\n",
        "        try:\n",
        "            embeddings = self._get_embedding_model(self.config)\n",
        "            cache_store = InMemoryStore()\n",
        "            cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
        "                underlying_embeddings=embeddings,\n",
        "                document_embedding_cache=cache_store,\n",
        "                query_embedding_cache=cache_store,\n",
        "                key_encoder=self._prefixed_sha256,\n",
        "            )\n",
        "            os.makedirs(self.config.chroma_persist_directory, exist_ok=True)\n",
        "            vectorstore = Chroma(\n",
        "                collection_name=self.config.collection_name,\n",
        "                embedding_function=cached_embeddings,\n",
        "                persist_directory=self.config.chroma_persist_directory,\n",
        "            )\n",
        "            self.logger.info(f\"Chroma initialized and ready at {self.config.chroma_persist_directory}\")\n",
        "            return vectorstore\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to initialize vector store: {e}\") from e\n",
        "\n",
        "    def _index_initial_documents(self, documents: List[LangchainDocument]):\n",
        "        \"\"\"Helper to index initial documents synchronously.\"\"\"\n",
        "        docs_to_add = [DocumentResult.from_langchain(doc) for doc in documents]\n",
        "        self.index_documents(docs_to_add)\n",
        "\n",
        "    def as_index_tool(self) -> StructuredTool:\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.index_documents,\n",
        "            coroutine=self.aindex_documents,\n",
        "            name=\"vectorstore_index\",\n",
        "            description=\"Indexes a list of documents into the vector store.\"\n",
        "        )\n",
        "\n",
        "    def as_retrieve_tool(self) -> StructuredTool:\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.retrieve_documents,\n",
        "            coroutine=self.aretrieve_documents,\n",
        "            name=\"vectorstore_retrieve\",\n",
        "            description=\"Retrieves documents from the vector store based on semantic similarity.\"\n",
        "        )\n",
        "\n",
        "    @retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(5), reraise=True)\n",
        "    def _index_sync(self, docs: List[DocumentResult]) -> VectorStoreResponse:\n",
        "        start = time.perf_counter()\n",
        "        document_ids = []\n",
        "        safe_docs = []\n",
        "        for doc in docs:\n",
        "            safe_metadata = {}\n",
        "            for k, v in doc.metadata.items():\n",
        "                if isinstance(v, (str, int, float, bool, type(None))):\n",
        "                    safe_metadata[k] = v\n",
        "                elif isinstance(v, datetime):\n",
        "                    safe_metadata[k] = v.isoformat()\n",
        "                else:\n",
        "                    safe_metadata[k] = str(v)\n",
        "            # also include created_at\n",
        "            safe_metadata['created_at'] = doc.created_at.isoformat()\n",
        "\n",
        "            lc_doc = doc.to_langchain()\n",
        "            # override metadata\n",
        "            lc_doc.metadata = safe_metadata\n",
        "\n",
        "            safe_docs.append(lc_doc)\n",
        "        try:\n",
        "            self.vectorstore.add_documents(safe_docs)\n",
        "        except Exception as e:\n",
        "            for d in docs:\n",
        "                self.logger.error(f\"Problem doc metadata={d.metadata}, type={type(d.metadata)}\")\n",
        "            raise\n",
        "        end = time.perf_counter()\n",
        "        self.logger.info(f\"Indexed {len(safe_docs)} docs in {end - start:.2f}s\")\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Indexed {len(safe_docs)} documents.\",\n",
        "            indexed_documents=docs,\n",
        "        )\n",
        "\n",
        "    @retry(wait=wait_exponential(multiplier=1, min=2, max=10),\n",
        "           stop=stop_after_attempt(5), reraise=True)\n",
        "    def _retrieve_sync(self, q: str, k: int) -> VectorStoreResponse:\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        try:\n",
        "            results = self.vectorstore.similarity_search_with_score(q, k=k)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Vector store retrieval failed: {e}\")\n",
        "\n",
        "        raw_scores = [float(s) for _, s in results]\n",
        "        max_score = max(raw_scores) if raw_scores else 1.0\n",
        "\n",
        "        docs = []\n",
        "        seen = set()\n",
        "        for doc, score in results:\n",
        "            try:\n",
        "                # safe_score = min(float(score), 1.0)\n",
        "                safe_score = float(score) / max_score if max_score > 0 else 0.0\n",
        "                document = DocumentResult.from_langchain(\n",
        "                    doc,\n",
        "                    relevance_score=float(safe_score)\n",
        "                  )\n",
        "\n",
        "                if document.document_id not in seen:\n",
        "                  seen.add(document.document_id)\n",
        "                  docs.append(document)\n",
        "            except Exception as inner_e:\n",
        "                self.logger.error(f\"Failed converting doc from_langchain: {inner_e}\")\n",
        "                continue  # skip broken docs instead of failing the whole batch\n",
        "\n",
        "        end = time.perf_counter()\n",
        "\n",
        "        self.logger.info(f\"Retrieved {len(docs)} docs in {end - start:.2f}s\")\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Retrieved {len(docs)} documents.\",\n",
        "            query=q,\n",
        "            results=docs\n",
        "        )\n",
        "\n",
        "    def index_documents(self, documents: List[DocumentResult]) -> VectorStoreResponse:\n",
        "        \"\"\"Synchronous method to index documents.\"\"\"\n",
        "        if not documents:\n",
        "            self.logger.info(\"No documents to index.\")\n",
        "            return VectorStoreResponse(status=\"success\", message=\"No documents indexed.\")\n",
        "\n",
        "        try:\n",
        "            return self._index_sync(documents)\n",
        "        except RetryError as e:\n",
        "            return VectorStoreResponse(status=\"failed\", message=\"Indexing failed after retries\", error=str(e))\n",
        "        except Exception as e:\n",
        "            return VectorStoreResponse(status=\"failed\", message=\"Indexing error\", error=str(e))\n",
        "\n",
        "    async def aindex_documents(self, documents: List[DocumentResult]) -> VectorStoreResponse:\n",
        "        \"\"\"Asynchronous method to index documents.\"\"\"\n",
        "        if not documents:\n",
        "            self.logger.info(\"No documents to index.\")\n",
        "            return VectorStoreResponse(status=\"success\", message=\"No documents indexed.\")\n",
        "        try:\n",
        "            # Run the synchronous indexing function in a separate thread\n",
        "            return await asyncio.to_thread(self._index_sync, documents)\n",
        "        except RetryError as e:\n",
        "            return VectorStoreResponse(status=\"failed\", message=\"Indexing failed after retries\", error=str(e))\n",
        "        except Exception as e:\n",
        "            return VectorStoreResponse(status=\"failed\", message=\"Indexing error\", error=str(e))\n",
        "\n",
        "    def retrieve_documents(self, query: str, top_k: int = 5) -> VectorStoreResponse:\n",
        "        \"\"\"Synchronous method to retrieve documents.\"\"\"\n",
        "        try:\n",
        "            return self._retrieve_sync(query, top_k)\n",
        "        except RetryError as e:\n",
        "            return VectorStoreResponse(status=\"failed\", message=\"Retrieval failed after retries\", query=query,\n",
        "                results=[], error=str(e))\n",
        "        except Exception as e:\n",
        "            return VectorStoreResponse( status=\"failed\", message=\"Retrieval error\", query=query,\n",
        "                results=[], error=str(e) )\n",
        "\n",
        "    async def aretrieve_documents(self, query: str, top_k: int = 5) -> VectorStoreResponse:\n",
        "        \"\"\"Asynchronous method to retrieve documents.\"\"\"\n",
        "        try:\n",
        "            return await asyncio.to_thread(self._retrieve_sync, query, top_k)\n",
        "        except RetryError as e:\n",
        "            return VectorStoreResponse(status=\"failed\", message=\"Retrieval failed after retries\", query=query,\n",
        "                results=[], error=str(e))\n",
        "        except Exception as e:\n",
        "            return VectorStoreResponse( status=\"failed\", message=\"Retrieval error\", query=query,\n",
        "                results=[], error=str(e) )"
      ],
      "metadata": {
        "id": "1Ws0RrhetgOP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Vector Store"
      ],
      "metadata": {
        "id": "zz5yM5du25Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"=== Testing Vector Store Setup ===\")\n",
        "config = VectorStoreConfig(\n",
        "    embedding_model_provider=\"sentence-transformer\",\n",
        "    embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "    chroma_persist_directory=\"./chroma_test\",\n",
        "    collection_name=\"test_collection\",\n",
        ")\n",
        "\n",
        "vectorstore_manager = VectorStoreManager(config, initial_documents=documents_list)\n",
        "\n",
        "# Get the tools from the manager instance\n",
        "retrieve_tool = vectorstore_manager.as_retrieve_tool()\n",
        "index_tool = vectorstore_manager.as_index_tool()\n",
        "\n",
        "result = retrieve_tool(\"What is python\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a34c33307085495a8960e397e6c30d03",
            "c0e2f95e4fc94d99ac8f89b239247b08",
            "3529f81b1a88419fa30b805f109fdb7c",
            "d0de3718bada4b1eab223ab18187521d",
            "9feb7d78450c494992a7d832931460ee",
            "3d5ee859848b4189a381d9f69fe080e0",
            "c9a19e5bc4b04537b417a3823e2a2b6e",
            "f953e4a4871b4aa98ce7276dd4b729c9",
            "7ba1aec5204e4277a15b533fc1112ae7",
            "320e0ee72487490b8108d4aa28eb5ac9",
            "0c8bce9d72a445d5895e2857d8c589e7",
            "95f139ecb0ae467dbf42df209bf3ae39",
            "349efce6a70d4be5898b79e3e2736742",
            "0eef8acd6bcc40099675efa1c8ca1609",
            "421f43d48bde4c3bb5a82cfca79acb9e",
            "d8365cfc84dc413981801ee559fba898",
            "0bca4041a3484f1b918ebb9bc7bfc675",
            "63d28d5e8e804c929562adb3235a6d14",
            "d0dc5ce151754fc58b1620c5ce194c3b",
            "4ae836f8eced42eda9e4d0f0fda1236f",
            "9db6316c8168404ebc43048694c95e57",
            "d70f9edb1bd34e25b9d21dcf3b6b80c8",
            "2b20b01a1dbc478e915debe07f286ece",
            "c323a0f8e7454d99ae68e3f6c549a7f6",
            "742320fa08c1467096f2838dcdde277c",
            "8946c29ccaa64506a1b590a0eaed21cf",
            "39fdba2c84d34e11aae5846a2ad15454",
            "6172661d3fa54a68888d823c0c2e99b7",
            "1a98ba308b35422484ced396b9bb4961",
            "c34484431b844c948917a6cf1a7e4690",
            "9376ef40167e4a13a800d7c82e32916f",
            "dbf9a70cfe87439f9e6ee5049f4299e7",
            "47080339f68c454aaf8768a4d993e92a",
            "e88437076932416db2a7d588e6891a43",
            "2ec7b76b6f174060a10f2a57424eff60",
            "00e6217bf0d14888ad2482e23777012f",
            "fe144c0d777a4734ab68def71bd7799b",
            "11e4abbd8aca46828c8d511243699b2a",
            "322031b9108a47a7bf200c2bf609890a",
            "e3fde20920a742e997ec2598c5fd92b2",
            "a78e9aeab4bf43b286acbe693dcb736b",
            "216ec0928e9446d8824884a86fc10ca2",
            "4f04ebb9a10c4cdc90ead1e7e5a99a64",
            "a0c587ebdf034f6c8ed579a3a811e27b",
            "5ee96df05cea4f9380d62182f96a642f",
            "6fb1b368d7254497854b324f4007abe6",
            "80c768be7a164e0bac3b68f3fe1aeb07",
            "568b88ff3a2f450299d9aa1b78f73c9e",
            "c816c02a29df4b89a3f3565ea23375d7",
            "725a8cce6e0e44f18fcacda976c3ffd1",
            "549eff9327e64c47ad7907fe38d2cc05",
            "990c1d772b434e4f9ecfb8738a3c2a03",
            "185b54273a2044079d780d11a9500704",
            "b448818aafc9431f92978193cdbb2cb5",
            "04d13d7758274ace907e8da56966b7a3",
            "b11ee23318844c58a878c9ffb559f56a",
            "cf4b96c375c74c8ca89bc938589df7ed",
            "37b50529814144fc97fc58dc33e6d469",
            "a7d77ab526924517b785ea7981663c40",
            "ca5d7bd558f64799a96d506735ce5813",
            "eac4cc519f2544829733aab9a8ca7804",
            "387f9ca9fd46450793f8ef7595989005",
            "ec0d5b2d746b48c7b022e0687e4d0d8b",
            "569ea0c879a64d76952376a2db53cedf",
            "81e249e4926b4c0b9f71be9862ea7013",
            "7fde0b2f453c41389821b71710cc1c0a",
            "a82fd26a6e4247bf9207e16f2bb9e9ab",
            "a119359d887e43c781bad7be141d8f07",
            "01831bd0fde249b19ebd5abeb79be431",
            "0bdea72b0b6045f9b708bdee6e7edf30",
            "e1c65ecd2c9246ed96d2546978e64c23",
            "4dd484d3a6fe40c894c7aee8be17f670",
            "15969498e5354e83ab0ac097528702b3",
            "4165c974551444f68b725ece5639d7f6",
            "7a71903e4b784b339ffec5bdf05b902e",
            "c613e8135bf94ed0924869e6367405f3",
            "3b4f779fb9304ee1b27f7ec837c3856a",
            "c309d9c8eef944a2a94feddd7874efe3",
            "0ec73344747a4619899fc373bf1dc332",
            "2cec7f71dce74089873cb4b02036d37b",
            "907e891e1fd94c04937161d584c6b263",
            "e0842178c1864442832031e84a7a1ec6",
            "7bfd91cb0e274a2e8ab734e42c09416d",
            "adcab987b9f64737a6892e03db27c0bf",
            "dfb25cb2ca834f02b4376859f27bbc30",
            "eb8cf83372fc4fb1997f093199786df5",
            "50ed29ec725b4cafb263f9f508187078",
            "09a88f22505148a5baf83ec179fed14f",
            "b762e34190ff487384f9769bb8e1148c",
            "ce735fcc42504efbbde84fee8504e983",
            "6569985f7b8a477594189567eb22cbb1",
            "2b0fdf71b343430eb5f0dc0a6b78cf16",
            "1fe3c2117f61434ca5bbfe8935b9d9c9",
            "142f2af375724467963ff2be33301e7f",
            "dc5401f4a52245a297b2dbc947ebcf5b",
            "9737f64ef7a64ec88a7621f2856a91cd",
            "350abd38a2c34beeaaa998cc69654471",
            "02fc548c4384412ca1c0a0208c9c28ff",
            "360b5cfb3b474ab79059ac3879174389",
            "94cba47c20444e93a8654be666b62f9c",
            "c6422fc079cf4c968e1dfc6b723aa842",
            "a4c52cb833e743bd8f0896483c8c4187",
            "d2c2af10c1f343338dea7465be577618",
            "9e6a72f7bcfb4be7b4f37083bc5e4ad7",
            "2b3ba73a27884366a86fc0ba245d70f9",
            "1719805f51d44d3da6059ac3864cbc3e",
            "6862c33425314d8588e4cb4f5751b5b0",
            "171927929b294200a5ab377594212111",
            "108b88346b564b13a1b18dd800ae0aaf",
            "027effddfb9b443fa711d287830a3e40",
            "f8d05b85200645e3bd18708ee43101c9",
            "5eaf7305ccc146b3aed624a186f989f3",
            "736ff54c037f402495e0b9286cb5d986",
            "c31140b678c447b0a91599b780ff4a1a",
            "49723a234ffd44779ce6239deba94d30",
            "2d4584ae09464a59bfe8f68a904668b5",
            "fe308000d6e54428b0ea1cfdb0ae5ed0",
            "880d2b7ea9cc486686b12f6d625be1b8",
            "f59c4a3d3050475fb770de8540d5be2a",
            "e87694666ddd495093465d0756dd7cfd",
            "9b14a72a825840eda896378880d32671"
          ]
        },
        "id": "IEJpYMvC2pMd",
        "outputId": "17cd7b2c-0088-4b58-d2a9-95af7732232c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15067073.py:48: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  return SentenceTransformerEmbeddings(model_name=cfg.embedding_model_name)\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a34c33307085495a8960e397e6c30d03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95f139ecb0ae467dbf42df209bf3ae39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b20b01a1dbc478e915debe07f286ece"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e88437076932416db2a7d588e6891a43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ee96df05cea4f9380d62182f96a642f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b11ee23318844c58a878c9ffb559f56a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a82fd26a6e4247bf9207e16f2bb9e9ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c309d9c8eef944a2a94feddd7874efe3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b762e34190ff487384f9769bb8e1148c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94cba47c20444e93a8654be666b62f9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8d05b85200645e3bd18708ee43101c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15067073.py:71: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n",
            "/tmp/ipython-input-1690113135.py:15: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = retrieve_tool(\"What is python\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreResponse(query='What is python', results=[DocumentResult(page_content='', metadata={'created_at': '2025-09-21T17:26:53.207777+00:00', 'title': ''}, content_length=0, source_type=<DocumentSource.SEARCH_SNIPPET: 'search_snippet'>, content_source=<ContentSource.OTHER: 'other'>, relevance_score=0.8674969792367512, created_at=datetime.datetime(2025, 9, 21, 17, 26, 53, 207777, tzinfo=TzInfo(UTC)), document_id='e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'), DocumentResult(page_content='The alien ship glided into a region of space that was not on any Council map. The stars here shimmered strangely, like pearls submerged in dark water. Nebulas pulsed with a rhythm that felt almost alive.\\n\\nMira pressed her hand against the viewport. “It feels like the stars are… breathing.”\\n\\nVega frowned, her hand near the hilt of her blade. “Stars don’t breathe.”\\n\\nBut Soren shook his head. “No… she’s right. I can feel it too.” His voice carried awe, not fear. “The light—it’s flowing. Like blood in cosmic veins.”\\n\\nAt the ship’s center, the Shard pulsed. Its glow resonated with the starlight outside, as if it were humming in unison with the constellation. Symbols appeared briefly in the air, spirals of light no one had seen before.\\n\\nEryndor, his face pale, muttered, “We’ve entered the Celestial Dao Belt.”\\n\\n“The what?” Vega snapped.\\n\\n“A hidden domain,” Eryndor explained. “Long ago, a civilization thrived here. They believed the cosmos itself was a body—stars as acupoints, galaxies as meridians, black holes as the heart. They called it… cultivation of the Great Dao of the Stars.”\\n\\nMira’s eyes widened. The words resonated deep in her chest. “And the Shard…?”\\n\\n“An artifact,” Eryndor said, lowering his voice. “Forged to connect mortals to the cosmos. A path to ascension. But it was thought destroyed in the First Galactic War.”\\n\\nThe ship suddenly shifted, pulled toward a drifting husk of a planet. Broken towers and shattered palaces floated in orbit like bones around a corpse. Yet, glowing script still lined the ruins—runes of Qi cultivation, shimmering faintly after millennia.\\n\\nThe fugitives disembarked. The ground crumbled like ash beneath their boots, but the air hummed with unseen energy. Mira felt warmth flood her veins, her heartbeat aligning with the stars above.\\n\\nVega tightened her grip on her weapon. “I don’t like this place. Feels like a trap.”\\n\\n“It’s not a trap,” Mira whispered. She touched a fragment of a broken wall. The script flared, and energy surged through her. Her vision blurred.\\n\\nFor a moment, she was no longer Mira. She was drifting in the void, her body dissolving into starlight, flowing along endless rivers of cosmic Qi. She saw nine celestial relics suspended across galaxies, connected by invisible threads of destiny. The Shard in her hand was one of them.\\n\\n“Mira!” Soren’s voice yanked her back. She gasped, trembling, the glow fading.\\n\\n“What happened?” Vega demanded.\\n\\n“I saw… the Dao,” Mira said, still dazed. “And it saw me.”\\n\\nThe ruins shuddered, dust spiraling upward like smoke. From the shadows emerged spectral guardians—giant figures of light and stone, their eyes burning with starfire. Ancient protectors, left behind to guard this sacred place.\\n\\nThe Shard blazed in Mira’s hand, and the ship behind them thrummed in response.\\n\\nSoren clenched his fists. “Looks like we’re not welcome here.”\\n\\nVega smirked, drawing her blade. “Good. I’ve been itching for a fight.”\\n\\nThe guardians raised their weapons of light, and the ground split beneath their feet as cosmic energy surged.\\n\\nMira tightened her grip on the Shard, fear and wonder mixing inside her. She knew this was not just another battle. This was a trial of the Dao.\\n\\nAnd the first step of a path that could either destroy them… or make them something greater than mortal.', metadata={'created_at': '2025-09-21T17:26:53.207604+00:00', 'title': 'The Signal from Titan: Chapter Two: The Path of Stars: Episode 1 — The Silent Constellation'}, content_length=3294, source_type=<DocumentSource.SEARCH_SNIPPET: 'search_snippet'>, content_source=<ContentSource.OTHER: 'other'>, relevance_score=0.9372322648921685, created_at=datetime.datetime(2025, 9, 21, 17, 26, 53, 207604, tzinfo=TzInfo(UTC)), document_id='83bdaaec4005fa7727e5e3d1a105bea972149cf1d0bd75efa90dfa339f0d5d8c'), DocumentResult(page_content='The alien ship drifted in silence above the fractured planet, its surface scarred by ancient battles. Mira stood at the viewport, her hands pressed against the glass, gazing at the ruins below. Stone pillars rose like broken teeth from the planet’s crust, glowing faintly with runes that pulsed in rhythm with her heartbeat.\\n\\n“They’re calling to us,” she whispered.\\n\\nVega crossed her arms, unimpressed. “Or it’s just old tech bleeding energy. Don’t romanticize it.”\\n\\nSoren frowned, his massive frame tense. “No… she’s right. I feel it too. Like a storm waiting for us.”\\n\\nThe ship lowered them onto a cracked plateau. Dust swirled as they stepped out, their boots echoing on ancient stone. The ruins stretched before them — an open courtyard ringed by shattered statues. Each statue depicted a warrior, some human-like, others alien, all standing proud with weapons raised to the stars.\\n\\nMira’s breath caught. The Shard inside her chest pulsed with light. “This place is alive.”\\n\\nBefore Vega could retort, the statues’ eyes flared with golden fire. Cracks spread across their bodies, and with a deafening roar, the guardians moved. Stone skin split to reveal inner cores of starlight. Weapons forged from energy ignited in their hands.\\n\\n“Ambush!” Vega shouted, unsheathing her blade.\\n\\nThe first guardian struck, swinging a sword of light. Vega blocked, sparks flying. The impact drove her to her knees. She gritted her teeth, pushed back, and countered with a strike so fierce the stone cracked — yet the guardian did not fall.\\n\\nAnother lunged at Soren, wielding a hammer that glowed with cosmic fire. He raised his arms to block, but the blow knocked him across the courtyard. He coughed, blood in his mouth, yet his veins burned with something new — raw, surging power.\\n\\nMira tried to summon the Shard’s light, but the guardians moved too fast. One charged her, spear raised. She screamed, and the Shard burst outward, a shield of radiant energy snapping into place. The spear shattered against it, sending shockwaves through the courtyard.\\n\\nThe guardians regrouped, circling them like predators.\\n\\n“They’re testing us,” Soren grunted, wiping blood from his chin. “Not trying to kill us — not yet.”\\n\\n“Some test,” Vega growled, lunging again. This time, her blade shimmered faintly, catching stray fragments of starlight. Each strike grew sharper, faster, more precise, as though the ruins themselves guided her hand.\\n\\nSoren rose, fists clenched. His muscles swelled with unnatural strength, glowing cracks appearing on his skin like molten veins. He roared and caught the hammer mid-swing, wrenching it from the guardian’s hands and smashing the stone warrior apart. The fragments dissolved into stardust.\\n\\nMira felt the Shard pulsing with rhythm — each beat matching her heartbeat. Closing her eyes, she drew the energy inward, shaping it not as a shield but as a surge of light. When she opened her eyes again, a wave of brilliance exploded from her hands, staggering the guardians and reducing two to rubble.\\n\\nOne last guardian remained, taller than the rest, its core blazing like a small star. It raised its sword toward Mira, but instead of striking, it lowered the blade and knelt.\\n\\nThe courtyard fell silent.\\n\\nThe remaining starlight faded, leaving behind only dust and broken stone. Yet something lingered — threads of energy weaving into Vega’s blade, settling into Soren’s blood, and nesting deep within Mira’s soul.\\n\\n“They weren’t enemies,” Mira whispered. “They were guardians… waiting for us.”\\n\\nVega exhaled, her blade dimming. “If this is just the beginning, I don’t want to see the next test.”\\n\\nSoren grinned despite his bruises, the glow in his veins fading slowly. “No. I do. For the first time… I feel alive.”\\n\\nAbove them, the sky shimmered, constellations shifting as if the stars themselves acknowledged their victory.\\n\\nThe trials had begun.', metadata={'created_at': '2025-09-21T17:26:53.207639+00:00', 'title': '🌌The Signal from Titan — Chapter Two: Episode 2: Trial of the Star Guardians'}, content_length=3862, source_type=<DocumentSource.SEARCH_SNIPPET: 'search_snippet'>, content_source=<ContentSource.OTHER: 'other'>, relevance_score=0.9733179446765402, created_at=datetime.datetime(2025, 9, 21, 17, 26, 53, 207639, tzinfo=TzInfo(UTC)), document_id='ec6304b641937034efc7eac420fb9eb4f089f68000c6689bfac412b252c4da97'), DocumentResult(page_content='Core Theme: The fugitives discover that the shard is part of the Celestial Dao of the Cosmos, a cultivation path practiced by ancient civilizations that merged spiritual essence with stellar power. To survive against galactic empires, they must train, ascend, and unlock cosmic techniques — while hunted by powers who fear what they could become.\\n\\nStructure: 48 Episodes, grouped into Four Arcs (12 episodes each).\\n\\nArc I: Awakening the Dao of the Shard (Ep. 1–12)\\n\\nEpisode 1–3: The alien ship takes them to a hidden star system, where ruins of a lost cultivation civilization remain. Mira begins sensing meridians of light within the cosmos.\\n\\nEpisode 4–6: Each fugitive experiences their first cultivation trial — Vega awakens the Path of the Blade Star, Soren the Path of the War Titan, Mira the Path of the Eternal Shard.\\n\\nEpisode 7–9: Eryndor reveals he once studied fragments of these teachings, but never completed them. His loyalty comes into question.\\n\\nEpisode 10–12 (Arc climax): The Council sends an elite fleet, but the fugitives, now in their early Star Qi Foundation stage, repel them by combining Dao techniques with the ship’s alien power.\\n\\nArc II: The Cosmic Tribulations (Ep. 13–24)\\n\\nEpisode 13–15: The fugitives journey to the Nebula Labyrinth, where cosmic beasts feed on spirit essence. Cultivation requires defeating them to refine Qi.\\n\\nEpisode 16–18: Introduction of Galactic Powers:\\n\\nThe Eternal Council (technocrats)\\n\\nThe Star Sovereigns (warlord-kings)\\n\\nThe Lotus Order (mystic monks bridging cultivation & science)\\n\\nEpisode 19–21: Mira faces her first tribulation lightning, nearly dying as the ship channels celestial energy to shield her.\\n\\nEpisode 22–24 (Arc climax): Betrayal — Eryndor is revealed to have made a secret pact with the Lotus Order. The fugitives must choose whether to trust him again.\\n\\nArc III: The Dao Wars of the Galaxy (Ep. 25–36)\\n\\nEpisode 25–27: The fugitives begin to master techniques like Starblade Qi, Titanic Body Refinement, and Shard Dao Insight.\\n\\nEpisode 28–30: The galaxy plunges into chaos as factions clash over shards rumored to be spread across the cosmos.\\n\\nEpisode 31–33: The fugitives encounter other cultivators — some allies, others deadly rivals who see them as heretics for fusing mysticism with alien tech.\\n\\nEpisode 34–36 (Arc climax): A grand war breaks out near the Black Sun Star, where Mira faces a Cosmic Cultivator Emperor in battle, barely surviving after unlocking the Shard’s hidden Dao Art.\\n\\nArc IV: Ascension Beyond the Stars (Ep. 37–48)\\n\\nEpisode 37–39: The fugitives seek the Cosmic Pagoda of Ten Thousand Realms, a cultivation fortress drifting between galaxies.\\n\\nEpisode 40–42: Each must face inner demons and illusions to ascend — Vega confronts her lust for vengeance, Soren his fear of insignificance, Mira her doubt that she deserves the Shard’s choice.\\n\\nEpisode 43–45: With breakthroughs into the Galactic Core Realm, they bend space itself, drawing cosmic essence directly from stars.\\n\\nEpisode 46–47: The Council and rival factions unite for a final assault, unleashing Void Weapons capable of destroying worlds.\\n\\nEpisode 48 (Chapter climax): The fugitives ascend together, binding their Daos into the living ship. The Shard reveals itself as one of Nine Celestial Relics — and the Signal was not just a call for survival, but an invitation to join the eternal war of the cosmos.\\n\\nChapter Two Summary:\\n\\nThis arc blends:\\n\\nMystic cultivation tropes: Qi refinement, tribulations, inner demons, cosmic pagodas, immortal realms.\\n\\nSci-fi scale: galactic empires, fleets, void weapons, alien relics.\\n\\nThemes: trust, betrayal, unity in cultivation, the cost of transcendence.\\n\\nBy Episode 48, our fugitives are no longer just survivors — they are Cosmic Cultivators, poised to reshape galactic history.', metadata={'created_at': '2025-09-21T17:26:53.207575+00:00', 'title': '🌌 The Signal from Titan — Chapter Two: The Path of Stars'}, content_length=3782, source_type=<DocumentSource.SEARCH_SNIPPET: 'search_snippet'>, content_source=<ContentSource.OTHER: 'other'>, relevance_score=0.9831149429405769, created_at=datetime.datetime(2025, 9, 21, 17, 26, 53, 207575, tzinfo=TzInfo(UTC)), document_id='d948202eeb66cf498fbbc0c4eb938695a9c7a8f6c698e89704d3081ef70bc24c'), DocumentResult(page_content='\\nThe Aurora cut through the blue atmosphere of Earth, its hull scarred from battle, its crew weary but alive. For Vega, the sight of the oceans glimmering below should have been a relief. Instead, her chest ached with dread.\\n\\nThe Council would never believe them.\\n\\nThey landed in Geneva, where the Interstellar Council’s headquarters rose like a white fortress. Drones scanned the ship for contamination, and armored guards escorted the crew inside.\\n\\nIn the grand chamber, twelve Councilors sat in a semicircle, robes gleaming with insignias of Earth’s nations and colonies. Behind them, holographic banners of unity rippled in the air.\\n\\nCouncilor Varga, sharp-eyed and cold, spoke first. “Captain Vega. You return from Titan without the crystal. Without authorization to fire weapons in restricted space. And with wild claims of… what exactly?”\\n\\nVega’s fists clenched, but she kept her voice steady. “We encountered an alien construct. A web-like entity that feeds on civilizations. We destroyed its core, but it’s not gone. If humanity doesn’t prepare, we will be next.”\\n\\nThe chamber buzzed with whispers.\\n\\nCouncilor Lin raised an eyebrow. “You expect us to believe this on your word? Where is the proof?”\\n\\nArjun stepped forward, holding up a projector. “We recorded everything. The black spires. The sphere. The Titan beings who saved us. It’s all here.”\\n\\nThe hologram flickered to life—showing the battle, the collapsing web, the glowing creatures. Gasps filled the chamber. Some councilors leaned forward in shock. Others frowned with doubt.\\n\\nCouncilor Varga’s lips curled. “Clever fabrications. Digital illusions are easy to forge. What motive do you have, Captain? Fearmongering? Securing more funding for exploration fleets?”\\n\\nThe insult burned Vega’s ears. “You think I risked my crew for politics? We barely escaped with our lives! Titan tried to warn us—”\\n\\nThat word silenced the chamber. Titan.\\n\\nCouncilor Lin’s eyes narrowed. “The Titan entities are myths. Scientists dismissed them centuries ago.”\\n\\nDr. Cho stepped forward, trembling with fury. “I saw them with my own eyes! They saved us! They exist! And if they believe we are worth saving, then Earth should listen.”\\n\\nThe Councilors exchanged glances. Some were shaken. Some hardened their stares.\\n\\nFinally, Councilor Varga leaned back in his chair. “Even if we accept this tale, what do you propose? To divert resources from Earth’s rebuilding, from Mars colonies, to chase shadows in deep space? The people will not support it.”\\n\\nVega took a deep breath, fighting the urge to scream. “What I propose is survival. Weapons, defenses, research into the Titan signal. If we pretend nothing happened, the web will return—and next time, there will be no one to save us.”\\n\\nSilence followed. Then, the Council’s Speaker raised her hand. “This session is adjourned. We will deliberate.”\\n\\nGuards escorted the Aurora’s crew out, their protests ignored.\\n\\nOutside the chamber, Kiera whispered, “They don’t believe us.”\\n\\nVega’s jaw tightened. “They will—when it’s too late.”\\n\\nAs they left the Council tower, a shadowy figure watched from a balcony above. He was not a councilor, not a soldier. His eyes glowed faintly with silver light, and he smiled.\\n\\nThe web had already reached Earth.\\n', metadata={'created_at': '2025-09-21T17:26:53.207251+00:00', 'title': '🌌 The Signal from Titan: Chapter one: The Web of the Shard — Episode 5: The Council’s Silence'}, content_length=3243, source_type=<DocumentSource.SEARCH_SNIPPET: 'search_snippet'>, content_source=<ContentSource.OTHER: 'other'>, relevance_score=1.0, created_at=datetime.datetime(2025, 9, 21, 17, 26, 53, 207251, tzinfo=TzInfo(UTC)), document_id='7aac1780bb865fba4f000689ff740571fe4a8d02bbe8096f7f8b1d68625ebce3')], status='success', message='Retrieved 5 documents.', document_ids=[], indexed_documents=[], error=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Any\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_core.tools import StructuredTool\n",
        "import logging\n",
        "\n",
        "# Assuming nest_asyncio and setup_reranker_pipeline are defined\n",
        "# and HuggingFaceEmbeddings, Chroma, documents_list are available for testing.\n",
        "\n",
        "nest_asyncio.apply()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RerankerOutput(BaseModel):\n",
        "    reranked_documents: List[LangchainDocument]\n",
        "    justification: str\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<RerankerOutput top={len(self.reranked_documents)} | {self.justification}>\"\n",
        "\n",
        "class RerankInput(BaseModel):\n",
        "    query: str = Field(..., description=\"The query to be reranked\")\n",
        "    documents: List[LangchainDocument] = Field(default_factory=list, description=\"The documents to be reranked\")\n",
        "    top_n: int = Field(5, description=\"Number of top documents to return\")\n",
        "\n",
        "class RerankerTool:\n",
        "    \"\"\"\n",
        "    A class to encapsulate the reranking tool and its dependencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model: Any, vectorstore: Any, top_n=10, k: int = 20):\n",
        "        self.pipeline = self._setup_reranker_pipeline(embedding_model, vectorstore, top_n, k)\n",
        "        if \"reranker\" not in self.pipeline:\n",
        "            raise ValueError(\"Pipeline must contain a 'reranker' component.\")\n",
        "        self.reranker = self.pipeline[\"reranker\"]\n",
        "\n",
        "    def _setup_reranker_pipeline(self, embedding_model, vectorstore, top_n: int = 10, k: int = 20):\n",
        "        \"\"\"\n",
        "        Setup a retriever pipeline that retrieves documents via a vectorstore\n",
        "        and reranks them with a cross-encoder.\n",
        "\n",
        "        Args:\n",
        "            embedding_model: The embedding model used for the vectorstore.\n",
        "            vectorstore: A vectorstore instance (e.g., Chroma, FAISS, Pinecone).\n",
        "            top_n: Number of top documents to keep after reranking.\n",
        "            k: Number of documents to fetch initially from vectorstore.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Step 1: Base retriever (retrieve more than you need)\n",
        "            base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "            # Load the cross-encoder model\n",
        "            hf_model = HuggingFaceCrossEncoder(\n",
        "                model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "                model_kwargs={\"device\": \"cpu\"}  # or \"cuda\" if available\n",
        "            )\n",
        "\n",
        "\n",
        "            # Step 2: CrossEncoder reranker\n",
        "            reranker = CrossEncoderReranker(\n",
        "                model=hf_model,#\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "                top_n=top_n,\n",
        "            )\n",
        "\n",
        "            # Step 3: Combine in a contextual compression retriever\n",
        "            pipeline = ContextualCompressionRetriever(\n",
        "                base_compressor=reranker,\n",
        "                base_retriever=base_retriever,\n",
        "            )\n",
        "            return {\"pipeline\":pipeline, \"reranker\":reranker}\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Falling back to base retriever only: {e}\")\n",
        "            return vectorstore.as_retriever(search_kwargs={\"k\": top_n})\n",
        "\n",
        "    async def arerank_documents(self, query: str, documents: List[LangchainDocument], top_n: int = 5) -> RerankerOutput:\n",
        "        \"\"\"Async version — required for Colab when using ainvoke().\"\"\"\n",
        "        if not documents:\n",
        "            return RerankerOutput(reranked_documents=[], justification=\"No documents provided.\")\n",
        "        try:\n",
        "            reranked = await self.reranker.acompress_documents(documents=documents, query=query)\n",
        "            return RerankerOutput(reranked_documents=reranked[:top_n],\n",
        "                                  justification=f\"Re-ranked {len(documents)} documents, selected top {top_n}.\")\n",
        "        except Exception as e:\n",
        "            logger.error(\"Reranker tool failed\", exc_info=True)\n",
        "            return RerankerOutput(reranked_documents=documents[:top_n],\n",
        "                                  justification=f\"Reranker failed: {e}; returning top docs.\")\n",
        "\n",
        "    def rerank_documents(self, query: str, documents: List[LangchainDocument], top_n: int = 5) -> RerankerOutput:\n",
        "        \"\"\"Sync version — used with invoke().\"\"\"\n",
        "        if not documents:\n",
        "            return RerankerOutput(reranked_documents=[], justification=\"No documents provided.\")\n",
        "        try:\n",
        "            reranked = self.reranker.compress_documents(documents=documents, query=query)\n",
        "            return RerankerOutput(reranked_documents=reranked[:top_n],\n",
        "                                  justification=f\"Re-ranked {len(documents)} documents, selected top {top_n}.\")\n",
        "        except Exception as e:\n",
        "            logger.error(\"Reranker tool failed\", exc_info=True)\n",
        "            return RerankerOutput(reranked_documents=documents[:top_n],\n",
        "                                  justification=f\"Reranker failed: {e}; returning top docs.\")\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.rerank_documents,\n",
        "            coroutine=self.arerank_documents,\n",
        "            name=\"rerank_documents\",\n",
        "            description=\"Tool to rerank documents via a pipeline.\",\n",
        "            args_schema=RerankInput,\n",
        "            return_direct=True\n",
        "        )\n",
        "\n",
        "# ---------- Testing with the new class ----------\n",
        "async def test_async_with_class():\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = Chroma.from_documents(documents_list, embedding=embedding_model)\n",
        "\n",
        "    # Create an instance of the class\n",
        "    reranker_instance = RerankerTool(embedding_model, vectorstore)\n",
        "    # Get the LangChain tool from the instance\n",
        "    rerank_documents_tool = reranker_instance.as_tool()\n",
        "\n",
        "    query = \"What is LangChain?\"\n",
        "    print(\"Testing Async function:\")\n",
        "    aresult = await rerank_documents_tool.ainvoke({\"query\": query, \"documents\": documents_list, \"top_n\": 2})\n",
        "    print(f\"Number of docs returned: {len(aresult.reranked_documents)}\")\n",
        "    for idx, doc in enumerate(aresult.reranked_documents, 1):\n",
        "        print(f\"#{idx} - {doc.metadata['title'][:50]}...\")\n",
        "\n",
        "    print(\"\\n\\nTesting Sync Function:\")\n",
        "    result = rerank_documents_tool.invoke({\"query\": query, \"documents\": documents_list, \"top_n\": 2})\n",
        "    print(f\"Number of docs returned: {len(result.reranked_documents)}\")\n",
        "    for idx, doc in enumerate(result.reranked_documents, 1):\n",
        "        print(f\"#{idx} - {doc.metadata['title'][:50]}...\")\n",
        "\n",
        "# In Colab, just do:\n",
        "await test_async_with_class()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "c43fc0ed6ef64f0592c2b6b2bb76a4b8",
            "8eb83d28f020491db2ab864b470d419b",
            "500aad3548df45d9b2a1226f3d07efec",
            "0f0b92d2c6ce458e986ab7d13461d9c3",
            "0758d5c0e3b845c3809de26aa487732b",
            "0b21c1d993634c7db1084d5dc2fc40bf",
            "084594d9fbfa4a07aeaeaa8a61df0136",
            "1ba1fe3bd34d4b69bcb9244622db524c",
            "e7ab5fb87d544cdea2e9924b386a42a6",
            "16eacf920a424bc1bb6c1889c907d601",
            "8397f5ef7492477fac90bf560f63e898",
            "ea0195b5a95c4fafb603154fae3a6248",
            "c77b3bbb15c54c16971009ee426f83c6",
            "a20a2b08e5eb4477980bd254f79d917e",
            "46d1a68153de44529259e3e0de51226f",
            "934c3e4eacf54530ae164e66001ceba1",
            "7c25b66088234796a60480d8601015f3",
            "37d95a6a436d44e3a45ae55fe665a8bb",
            "90faab42c0d0475cae2ae66d8f41a241",
            "9d99089bd52d4c229cc375f22ef07218",
            "f5be8355a38b471a8f1c225acaf72538",
            "09451c43b1c044c79b92e73981997ba7",
            "0c31d1f431e641cf81643b5ec5f612a2",
            "74837bc120424e928ecdb037a9238352",
            "206efe85c5e140dfa4d40b4611acd890",
            "d43114d432e348e495ca84ec5a9ce6db",
            "a06b4eff3fae4527b84819e94f315c95",
            "871c5474c2e64ca0a27040d7ab70f45c",
            "c0014831e693473ebaba6771bb7c2825",
            "6003c258ecb64e76af3d4da2eab9dc07",
            "971c4d87f79749aeb924eb2f7761ce1d",
            "502d89a99d9f4330bae07cfa82820545",
            "f7152627fd834111a959d469a75d85cb",
            "f318e8e6361147feaa65af5c3dc68ab7",
            "1f45512ced9b4a22973a5cb8497527e0",
            "06c22e52a2bb490286ebcac623614b5e",
            "88bfceae64ee4c948094adbabda63c12",
            "f782dc3339d64b4a8b3ec7c09dfddd2c",
            "672496579c7f4a11ae82c988e108f1f4",
            "eefb1c9e157c4fd98145431edc32fe64",
            "8524c9bed8d044779ce6ad6dd7f016ee",
            "20a02167e1644e138c62f23f270e7cc7",
            "20325a31c64b4bfc8ecf0d42545e1bf0",
            "9cc6a46ef3734974bc2aa5994cfccaa0",
            "055bc5f370924dd289d0f0b30f1b5b63",
            "67188750c3c04773b51dbc1b0a9d974d",
            "ad7c2a78f3f0470da285be6c4fbcc5d6",
            "fd04dfb2beaa4170b8dcb3c5ba2b1f56",
            "0863a9fe744d441eab598ae129bb92bb",
            "eaee311f64c64768839c4ee5437921c4",
            "2944002807624a4a8fdc49839153a511",
            "f89c8a6907324710967596e367975e18",
            "a754f31c70584e5ab94d72b7db133982",
            "4c6ca074fafd437ea1aee5874f0c6f3e",
            "9aaee21e5dad42e0957fb285e2136f65",
            "e5eba0eeb8cf47d2a50ba3c6df9d2b57",
            "8891744b556d432c8036a3f2dd4a9a1a",
            "9edb8cfdeaaa4306898b2989d4431ab5",
            "58e60ee1337c4ae895be2779ca51bee4",
            "d11af611256a4e6e9fe6b42a621ec0c0",
            "fb97fbe54ab14cd5b09d2667da90c06c",
            "a0571f545a5d4754adba33eadc249efb",
            "13f126f544914d279731dfb1f2fa035c",
            "6fb29954f02f47c78da4e1d4585557e9",
            "9c0cc24718114d5787ca1e9625cb0996",
            "8ba0d36cd03943618c1c8e905fcc234c",
            "fd5bbf23e5c441929dda4fe055e77257",
            "e7a8627af22a40c88bd9e1aa0fca8da4",
            "d9a44da2172d46338e8615a7076833b1",
            "597f3b99796d4127bb12373a281e4eab",
            "3b8c498861f344879c840957684ca868",
            "1381b201992449a09cf59fb603dadeaa",
            "d3b270e73de74e8e991f81dccd86522f",
            "76b33d984cb5446ab07e8d53f7016603",
            "df8c27ace2d04f2a9509725f33974401",
            "9c302459864f4a1bac64c31510d8740e",
            "fbbae93c15144cd6805f39bdaf12dd2d"
          ]
        },
        "id": "aUtHJB9xIBkK",
        "outputId": "77fc1215-f3a5-4c7d-b0b7-474b21f424df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c43fc0ed6ef64f0592c2b6b2bb76a4b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea0195b5a95c4fafb603154fae3a6248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c31d1f431e641cf81643b5ec5f612a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f318e8e6361147feaa65af5c3dc68ab7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "055bc5f370924dd289d0f0b30f1b5b63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5eba0eeb8cf47d2a50ba3c6df9d2b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd5bbf23e5c441929dda4fe055e77257"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Async function:\n",
            "Number of docs returned: 2\n",
            "#1 - ...\n",
            "#2 - 🌌 The Signal from Titan - Chapter Two: The Path of...\n",
            "\n",
            "\n",
            "Testing Sync Function:\n",
            "Number of docs returned: 2\n",
            "#1 - ...\n",
            "#2 - 🌌 The Signal from Titan - Chapter Two: The Path of...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import logging\n",
        "from typing import List, Any\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_core.documents import BaseDocumentCompressor, Document as LangchainDocument\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain_core.callbacks import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "nest_asyncio.apply()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Input Schema for Tool ---\n",
        "class CompressorInput(BaseModel):\n",
        "    query: str = Field(..., description=\"The user's query.\")\n",
        "    documents: List[LangchainDocument] = Field(..., description=\"The documents to compress.\")\n",
        "\n",
        "# --- Compressor Tool ---\n",
        "class CompressorTool:\n",
        "    def __init__(self, llm: Any):\n",
        "        self.compressor: BaseDocumentCompressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "    def complex_compress_documents(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[LangchainDocument],\n",
        "    ) -> List[LangchainDocument]:\n",
        "        \"\"\"Compresses documents using the internal LLMChainExtractor synchronously.\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        run_manager = CallbackManagerForRetrieverRun.get_noop_manager()\n",
        "        compressed_docs = self.compressor.compress_documents(\n",
        "            documents=documents,\n",
        "            query=query,\n",
        "            callbacks=run_manager.get_child(),\n",
        "        )\n",
        "\n",
        "        # --- Handle empty response ---\n",
        "        if not compressed_docs:\n",
        "            logger.info(\"LLMChainExtractor returned an empty list. Providing a fallback.\")\n",
        "            return [LangchainDocument(page_content=\"No relevant information found.\")]\n",
        "\n",
        "        return compressed_docs\n",
        "\n",
        "    async def acomplex_compress_documents(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[LangchainDocument],\n",
        "    ) -> List[LangchainDocument]:\n",
        "        \"\"\"Compresses documents using the internal LLMChainExtractor asynchronously.\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        run_manager = AsyncCallbackManagerForRetrieverRun.get_noop_manager()\n",
        "        compressed_docs = await self.compressor.acompress_documents(\n",
        "            documents=documents,\n",
        "            query=query,\n",
        "            callbacks=run_manager.get_child(),\n",
        "        )\n",
        "\n",
        "        # --- Handle empty response ---\n",
        "        if not compressed_docs:\n",
        "            logger.info(\"LLMChainExtractor returned an empty list. Providing a fallback.\")\n",
        "            return [LangchainDocument(page_content=\"No relevant information found.\")]\n",
        "\n",
        "        return compressed_docs\n",
        "\n",
        "    def as_tool(self):\n",
        "        \"\"\"Expose this as a StructuredTool for agent usage.\"\"\"\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.complex_compress_documents,\n",
        "            coroutine=self.acomplex_compress_documents,\n",
        "            name=\"llm_compressor\",\n",
        "            description=\"Extracts relevant parts from documents using an LLM.\",\n",
        "            args_schema=CompressorInput,\n",
        "        )\n",
        "\n",
        "# --- Example Usage with HuggingFace ---\n",
        "async def main():\n",
        "    try:\n",
        "        rate_limiter = InMemoryRateLimiter(\n",
        "            requests_per_second=0.1, # 2 request every 10 seconds (adjust based on your limit)\n",
        "            max_bucket_size=1, # No burst capacity to be safe\n",
        "        )\n",
        "        # Load necessary components\n",
        "        llm = init_chat_model(\n",
        "            \"gemini-2.5-flash\",\n",
        "            model_provider=\"google-genai\",\n",
        "            temperature=0.2,\n",
        "            # rate_limiter=rate_limiter,\n",
        "        )\n",
        "        # Initialize RerankerTool\n",
        "        # NOTE: RerankerTool constructor no longer takes embedding_model and vectorstore directly.\n",
        "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        vectorstore = Chroma.from_documents(documents_list, embedding=embedding_model)\n",
        "\n",
        "        # Create an instance of the class\n",
        "        reranker_instance = RerankerTool(embedding_model, vectorstore)\n",
        "        # Get the LangChain tool from the instance\n",
        "        reranker_tool = reranker_instance.as_tool()\n",
        "\n",
        "        query = \"What effect did the plasma fire have on the silver veins?\"\n",
        "\n",
        "        # Rerank documents first\n",
        "        rerank_output = await reranker_tool.ainvoke({\"query\": query, \"documents\": documents_list})\n",
        "        reranked_docs = rerank_output.reranked_documents\n",
        "\n",
        "        print(f\"Number of reranked docs: {len(reranked_docs)}\")\n",
        "        for idx, doc in enumerate(reranked_docs, 1):\n",
        "          print(f\"#{idx} - {doc.metadata.get('title', 'N/A')[:50]}...\")\n",
        "          print(f\"Document Type: {type(doc)}\")\n",
        "\n",
        "        # Instantiate our compressor with the correct LLM\n",
        "        compressor = CompressorTool(llm=llm)\n",
        "        compressor_tool = compressor.as_tool()\n",
        "\n",
        "        # Use the reranked documents for compression\n",
        "        retrieved_docs = reranked_docs\n",
        "        print(f\"Retrieved Docs for Compression: {[d.page_content for d in retrieved_docs]}\")\n",
        "\n",
        "        # Compress retrieved docs (sync)\n",
        "        # Using a relevant query for better results\n",
        "        results = compressor.complex_compress_documents(\n",
        "            query,\n",
        "            retrieved_docs,\n",
        "        )\n",
        "        print(f\"Synchronous Compression Results: {[d.page_content for d in results]}\")\n",
        "\n",
        "        # Compress retrieved docs (async)\n",
        "        # Using a relevant query for better results\n",
        "        async_results = await compressor.acomplex_compress_documents(\n",
        "            query,\n",
        "            retrieved_docs,\n",
        "        )\n",
        "        print(f\"Asynchronous Compression Results: {[d.page_content for d in async_results]}\")\n",
        "\n",
        "        # Agent-style tool usage (using async invoke)\n",
        "        tool_output = await compressor_tool.ainvoke(\n",
        "            {\"query\": query, \"documents\": retrieved_docs}\n",
        "        )\n",
        "        print(f\"Tool Output: {[d.page_content for d in tool_output]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred: {e}\", exc_info=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOIsEEkD3ON4",
        "outputId": "be59aa95-faa9-4809-c188-80aa7ef1c735"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reranked docs: 5\n",
            "#1 - 🌌 The Signal from Titan: Chapter one: The Web of t...\n",
            "Document Type: <class 'langchain_core.documents.base.Document'>\n",
            "#2 - 🌌 The Signal from Titan: Chapter one: The Web of t...\n",
            "Document Type: <class 'langchain_core.documents.base.Document'>\n",
            "#3 - 🌌 The Signal from Titan: Chapter one: The Web of t...\n",
            "Document Type: <class 'langchain_core.documents.base.Document'>\n",
            "#4 - ...\n",
            "Document Type: <class 'langchain_core.documents.base.Document'>\n",
            "#5 - 🌌 The Signal from Titan - Chapter Two: The Path of...\n",
            "Document Type: <class 'langchain_core.documents.base.Document'>\n",
            "Retrieved Docs for Compression: ['\\nThe Aurora shook as the black spires closed around them. Vega gripped her chair, every nerve screaming at her to flee. But she forced herself to speak the words that sealed their path.\\n\\n“We strike.”\\n\\nThe crew looked at her in shock. Arjun’s hands froze above his console. “Captain, we’re in the heart of this thing! A single mistake and—”\\n\\n“Then we don’t make mistakes,” Vega snapped. “Target the veins. The silver lines—they’re alive. They feed the structure. If we cut them, maybe we can cripple it.”\\n\\nKiera’s face was pale, but she nodded and prepared the targeting system. Dr. Cho whispered, “If Titan’s warning was true, this is the predator. If we fall here, maybe Earth has a chance to fight back.”\\n\\nThe Aurora’s weapons hummed to life.\\n\\nThe black sphere pulsed with thought again, heavier now, shaking their minds like a storm:\\n\\nYOU CANNOT KILL THE PATTERN. YOU ARE ALREADY PART OF IT.\\n\\nVega ignored it. “Fire!”\\n\\nTwin lances of plasma shot from the Aurora, striking the silver veins. The chamber shook violently. For the first time, the alien structure screamed—not in sound, but in a pressure that twisted their bones. The silver veins flared white-hot, rupturing into cascading sparks.\\n\\nThe spires convulsed. The opening to space began to widen again, but not in invitation—this was pain, collapse.\\n\\n“They’re destabilizing!” Kiera shouted.\\n\\n“Again!” Vega ordered.\\n\\nArjun fired a second barrage. The central sphere flickered, its shifting symbols stuttering. Images of dying worlds dissolved into static.\\n\\nThen the thought-voice came again, but weaker, distorted:\\n\\nYOU… WILL… JOIN… US…\\n\\nThe sphere cracked. Shards of glass-like shadow floated outward, each one glowing with fragments of alien script.\\n\\nSuddenly, the Aurora was caught in a pull—an intense gravity dragging them toward the ruptured core.\\n\\n“We’re being sucked in!” Arjun cried.\\n\\n“Full thrusters!” Vega barked.\\n\\nEngines roared, but the pull was stronger. The Aurora tilted toward the collapsing sphere, alarms shrieking. Crew members were thrown from their seats. Sparks rained from the ceiling.\\n\\n“We won’t make it!” Kiera screamed.\\n\\nThen, from the chaos, a ripple of blue light cut across the chamber. The crew gasped as a familiar vision appeared—shapes like the glowing creatures from Titan, swimming through the void, their forms like waves of living starlight.\\n\\nThe Titan beings had returned.\\n\\nThey swirled around the Aurora, weaving currents of energy that pushed against the collapsing core. The ship lurched forward, breaking free from the pull.\\n\\n“They’re helping us!” Dr. Cho shouted.\\n\\nVega’s chest tightened. Titan had warned them. Titan had saved them. But why?\\n\\nThe alien lights pulsed once, like a heartbeat, then vanished into the collapsing structure.\\n\\nThe Aurora shot through the widening opening, escaping into the black sea of space. Behind them, the web folded inward, spires collapsing like a dying star, until nothing remained but silence.\\n\\nThe crew sat in stunned quiet, breathing hard, sweat dripping down their faces.\\n\\n“It’s gone,” Kiera whispered.\\n\\n“No,” Vega said softly, staring at the empty void. “Not gone. Wounded. And if it feeds on civilizations… there will be more of them. Somewhere out there.”\\n\\nThe thought chilled them more than the battle.\\n\\nDr. Cho finally broke the silence. “We have to tell Earth. Not just the Council—everyone. Humanity has to prepare. The Titan beings gave us time. We can’t waste it.”\\n\\nVega nodded slowly. Her choice had bought them survival, but the war had only begun.\\n\\nShe turned to her crew. “Set course for Earth. It’s time to wake the world.”\\n', '\\nThe Aurora drifted into the shadow of the strange machine near Neptune. Its spires were blacker than space itself, absorbing starlight until they seemed to cut holes in reality.\\n\\n“Readings?” Vega asked, her voice steady though her chest was tight.\\n\\nArjun scanned his console. “No heat. No emissions except the signal. It’s… it’s not just broadcasting, it’s pulling. Like it’s searching for listeners.”\\n\\nThe ship shuddered. A ripple of static swept across every screen. The signal grew louder, not through the speakers but inside their minds—sharp pulses that made teeth ache and thoughts stutter.\\n\\n“It’s in my head,” Kiera whispered, gripping her temples.\\n\\n“Shut it down!” Vega ordered, but the Aurora’s systems were already flickering. Lights dimmed. Gravity wavered. Something vast had reached across space and taken hold of their ship.\\n\\nThen, suddenly, silence.\\n\\nOn the main screen, the black web shifted. Its spires folded like the ribs of a giant beast, and a circular opening appeared, glowing faintly with blue light. An invitation. Or a trap.\\n\\nDr. Cho’s hands trembled as she spoke. “This architecture is beyond anything we’ve imagined. If it’s a weapon, it could shatter Earth in seconds. But if it’s… communication…”\\n\\n“Captain,” Arjun said, “we can leave now. Report back, let the Council decide.”\\n\\nVega stared at the opening. In her mind, she saw Titan’s glowing creatures, their patient warning: Others are coming. The oceans will burn. Prepare.\\n\\nWhat if this was what they had feared?\\n\\n“No,” Vega said at last. “We go in.”\\n\\nThe Aurora inched forward. As they passed through the glowing circle, gravity shifted again. Outside, the stars vanished. They were no longer in Neptune’s orbit but inside a hollow space larger than any city on Earth. The walls of the black web curved upward like an endless cathedral, pulsing faintly with silver veins.\\n\\nAnd at the center floated a construct: a sphere of glass and shadow, rotating slowly, covered in symbols that shifted like living text.\\n\\n“It’s a library,” Dr. Cho breathed.\\n\\nOr a prison, Vega thought.\\n\\nThe sphere pulsed. A voice—no, not sound, but thought—spread through the crew:\\n\\nYOU ARE NOT THE FIRST.\\n\\nEveryone froze. The words weren’t in English, but they carried meaning, heavy and undeniable.\\n\\n“Who are you?” Vega whispered.\\n\\nWE ARE THE KEEPERS OF THE LAST LIGHT.\\n\\nThe crew exchanged nervous glances.\\n\\nWE HARVEST WHAT REMAINS.\\n\\nKiera’s eyes widened. “Harvest?” she mouthed.\\n\\nThe symbols across the sphere shifted again, replaying scenes that chilled them: oceans boiling, cities falling into ash, alien worlds torn apart. Species after species vanishing into silence.\\n\\nAnd each time, the black web grew larger.\\n\\n“They’re not warning us,” Arjun said, his voice breaking. “They’re feeding on endings.”\\n\\nThe thought-voice returned, heavier now:\\n\\nEARTH WILL JOIN THE PATTERN.\\n\\nThe crew reeled. Dr. Cho clutched Vega’s arm. “This is what Titan tried to warn us about! These things don’t save worlds—they consume them.”\\n\\nVega’s heart pounded. Titan had given them a choice: prepare or perish. And now she understood why. This machine wasn’t just a relic—it was the predator that followed civilizations to their graves.\\n\\nOn the viewscreen, the black spires began to fold inward. The opening was closing.\\n\\n“Captain,” Arjun shouted, “if we don’t move now, we’ll never get out!”\\n\\nVega’s mind raced. They could flee and warn Earth… but would Earth believe them this time? Or should they strike now, while inside the beast, risking everything to wound it before it reached the home they loved?\\n\\nThe ship trembled as the web shifted, hungry and closing. The crew waited for Vega’s command.\\n\\nShe took a breath.\\n\\nAnd made her choice.\\n', 'The docking bay shook underfoot as if the entire station were coming apart at the seams. Above, half a dozen Council mechs lined the gantries, weapons trained on Vega’s crew. Red targeting beams cut through the smoke, painting glowing dots across their chests.\\n\\n“Drop your weapons!” boomed a mechanized voice. “Surrender the shard, and you may yet live.”\\n\\nNo one moved. Mira clutched the satchel tighter, the shard glowing hot through the fabric. Its pulse echoed in her chest, in time with her racing heart.\\n\\nVega raised her sidearm, though she knew it was useless against mechanized armor. Her mind raced: Fight? Flee? Or gamble on the shard’s strange energy again?\\n\\nBefore she could decide, Aris stepped forward.\\n\\nHe raised his hands slowly, letting the satchel’s strap slide from Mira’s shoulder into his grip. “Easy,” he called to the mechs. “No need for all this. I’ve got what you want.”\\n\\n“Aris?!” Mira’s voice cracked in disbelief.\\n\\nSoren snarled, lunging forward, but Vega threw out an arm to stop him. She could read the smuggler’s expression — the cool calculation of a man who lived too long in the shadows. He was making his move.\\n\\nThe lead mech lowered its weapon slightly. “Hand it over.”\\n\\nAris smirked. “Triple payment, right? Credits transferred directly to—”\\n\\nThe shard flared violently, searing white light bursting through the satchel’s seams. Aris staggered, clutching it, eyes wide.\\n\\n“—what the—?!”\\n\\nThe mechs opened fire. Plasma bolts scorched the deck around him. Aris dove for cover, dragging the satchel with him.\\n\\nIn that instant, Vega acted. She shoved Mira and Soren toward a half-dismantled shuttle in the corner bay. “Move! Now!”\\n\\nThey sprinted, weaving between burning crates and falling debris. Aris fired back with his pulse pistol, half at the mechs, half at Vega’s crew — trying to keep both sides at bay.\\n\\n“You don’t get it, Vega!” he shouted over the chaos. “This shard isn’t a relic — it’s a key! And whoever controls it, controls everything!”\\n\\nThe shard pulsed again, and this time the entire docking bay screamed. Metal warped. Gravity shifted. A mech collapsed into itself as though crushed by unseen pressure. Others stumbled, recalibrating.\\n\\nMira screamed, clutching her head. “It’s— it’s talking! I can hear—”\\n\\nVega yanked her into the shuttle. Soren slammed the hatch, fingers flying over the half-dead console. Systems sputtered to life.\\n\\nBehind them, Aris made his choice. He sprinted not toward safety, but straight toward the lead mech, holding the shard high.\\n\\n“You want it?” he bellowed. “Then take it from me!”\\n\\nThe mech’s clawed arm reached out. The shard detonated in another pulse of alien energy, hurling both Aris and the machine into the bulkhead with bone-shattering force.\\n\\nThe bay erupted in chaos. Mechs faltered. Civilians screamed.\\n\\nAnd Vega’s stolen shuttle, engines barely holding together, roared free of the collapsing dock — leaving Aris’s fate uncertain, the shard’s power more terrifying than ever, and the Council’s grip tightening across every channel of space.\\n\\nOut the viewport, the graviton cannon glowed, preparing for another devastating shot.\\n\\n“This isn’t over,” Vega whispered, staring into the void.\\n\\nAnd in the silence that followed, the shard pulsed again.\\n\\nNot in warning.\\nBut in invitation.', '', 'The chamber’s glow shifted as though responding to their breaths. Patterns on the walls pulsed, dimming whenever doubt crept into their hearts and brightening when their will sharpened. The fugitives stood still, the silence heavy with awe.\\n\\nVega clenched her fists. “I can feel it… like starlight pressing against my veins. It burns, but it also—” She broke off, eyes narrowing as a faint edge of light flared along her arm. A blade, not forged of metal, but of pure radiance, flickered into existence before fading.\\n\\nSoren staggered backward, his broad frame trembling. “Mine… mine is different. Not sharp. Heavy.” The ground beneath him cracked as invisible pressure rippled outward. It was as though the weight of a collapsing star had brushed the chamber, bending gravity itself. He fell to one knee, gasping, sweat dripping down his brow.\\n\\nMira watched them, her own pulse in rhythm with the chamber. The Shard within her heart glowed faintly, and she saw lines of light extending from her chest outward, webbing into the room. Unlike Vega’s blade or Soren’s strength, hers felt endless, stretching far beyond the walls, into the cosmos itself. She whispered, “It’s like the stars are listening.”\\n\\nEryndor had been silent. At last, he stepped forward, his eyes dark with something unreadable. “These are cultivation roots. Long before your Council ruled, before your colonies spread, there were civilizations who walked the Dao of the Cosmos. They wove essence from the stars, refined their bodies into living vessels of light and void.” His voice trembled with reverence, but also a shadow of fear.\\n\\nVega turned sharply. “You knew?”\\n\\n“I suspected,” Eryndor admitted. “The glyphs, the relics the Council buried… I studied fragments. But I never advanced. Without a Shard to awaken the meridians, the Dao was only myth.”\\n\\nMira’s eyes narrowed. “And now? You’re saying we’ve awakened it?”\\n\\nBefore he could answer, the chamber shook. Above them, the alien ship hummed louder, as if resonating with their breakthroughs. A beam of starlight pierced the ceiling, surrounding them in radiant mist. Each breath they took was no longer just air—it was energy, threaded with power.\\n\\nA voice—neither male nor female, neither alien nor human—echoed in their minds:\\n\\n“Step into the Foundation. Choose your Path. The cosmos will shape you, as you shape it.”\\n\\nThey froze, staring at one another. Vega smirked, lifting her chin. “Path of the Blade Star. That one’s mine.”\\n\\nSoren wiped blood from his lip, his voice a low growl. “War Titan. I don’t know why, but it feels right.”\\n\\nMira pressed a hand against her chest. The Shard pulsed with warmth. Her path was already chosen—the Path of the Eternal Shard.\\n\\nAnd Eryndor… his silence lingered a moment too long. His eyes darted to the glowing patterns, then back to Mira. “Some doors should never be opened,” he muttered, even as the light wrapped around him, binding him to a choice he did not speak aloud.\\n\\nThe starlight flared once more, burning the chamber into brilliance. When it dimmed, their bodies felt different—denser, stronger, alive with cosmic essence. They had stepped onto the first rung of cultivation: the Star Qi Foundation Stage.\\n\\nBut outside, in the endless dark, something stirred. For the first time, the Council’s long-range scanners detected a spike of impossible energy. A signal flared across the galaxy. Their ascension had not gone unnoticed.']\n",
            "Synchronous Compression Results: ['Twin lances of plasma shot from the Aurora, striking the silver veins. The chamber shook violently. For the first time, the alien structure screamed—not in sound, but in a pressure that twisted their bones. The silver veins flared white-hot, rupturing into cascading sparks.']\n",
            "Asynchronous Compression Results: ['The silver veins flared white-hot, rupturing into cascading sparks.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 3.718098136s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 3\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 3.717674359s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 3\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 3.703477361s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 3\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 1.641161785s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 1\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 1.490945573s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 1\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 1.484678488s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 1\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 57.573846657s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 57\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 57.423276286s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 57\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 57.276629207s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 57\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 49.49237847s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 49\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 49.34345775s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 49\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 49.197071451s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 49\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 33.40699224s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 33\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 33.265509668s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 33\n",
            "}\n",
            "].\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 33.129256829s. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.5-flash\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 33\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool Output: ['Twin lances of plasma shot from the Aurora, striking the silver veins. The silver veins flared white-hot, rupturing into cascading sparks.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import asyncio\n",
        "# import logging\n",
        "# from transformers import pipeline\n",
        "# from langchain_community.llms import HuggingFacePipeline\n",
        "# from langchain.agents import initialize_agent, AgentType\n",
        "# from langchain_core.documents import Document\n",
        "\n",
        "# # Import our CompressorTool from previous workflow\n",
        "# # from compressor_tool import CompressorTool  # <-- assume you saved previous code as compressor_tool.py\n",
        "\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# async def agent_demo():\n",
        "#     # --- 1. Setup LLM ---\n",
        "#     hf_pipeline = pipeline(\"text-generation\", model=\"google/flan-t5-small\", max_new_tokens=256)\n",
        "#     llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "#     # --- 2. Create compressor tool ---\n",
        "#     compressor = CompressorTool(llm=llm)\n",
        "#     tool = compressor.as_tool()\n",
        "\n",
        "#     # --- 3. Initialize agent with tool ---\n",
        "#     agent = initialize_agent(\n",
        "#         tools=[tool],\n",
        "#         llm=llm,\n",
        "#         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "#         verbose=True,\n",
        "#     )\n",
        "\n",
        "#     # --- 4. Fake documents ---\n",
        "#     docs = [\n",
        "#         Document(page_content=\"LangChain helps build LLM-powered applications.\", metadata={\"source\": \"doc1\"}),\n",
        "#         Document(page_content=\"OpenAI provides powerful LLMs for developers.\", metadata={\"source\": \"doc2\"}),\n",
        "#         Document(page_content=\"Chroma is a vector database optimized for embeddings.\", metadata={\"source\": \"doc3\"}),\n",
        "#     ]\n",
        "\n",
        "#     # --- 5. Run agent query ---\n",
        "#     query = \"Summarize what LangChain is, using the provided documents.\"\n",
        "#     result = await agent.ainvoke({\"input\": f\"{query}\\nDocuments: {docs}\"})\n",
        "#     logger.info(f\"Agent Output: {result['output']}\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     asyncio.run(agent_demo())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652,
          "referenced_widgets": [
            "48aee96ef4284ea69e9567ee98de2f6a",
            "8a05647a0c0646e3a1a971e828a495a0",
            "20d80d1da769415a8775a52a45f8b498",
            "b47b2af5544d423990acdc903d5d0d61",
            "d2a0a6f99cc0451286c85f5228c2e7b9",
            "04b7dd9a7568433095acfcffcfa7585c",
            "96637a09c66748acbc8b996f26e5ecdd",
            "a1e4bb10b40942c89d7960eef222b688",
            "d11e546d74184d39a5aeddb74c447a5e",
            "64c30d7f2f594f1bba11cbfc14c9c8d3",
            "3eb299df070248b0a4a726497b16cf09",
            "9e055b6adc084f799320b4a7d699e6e9",
            "cf5765c765064d1e8ccd715e34a92538",
            "1b6f18ba72e6484d9b3b5e9d77f9840d",
            "9788667a42434f69aa15ded458855e3b",
            "b27447578f1d401e91ca0853bf2a95f1",
            "b4b216fac131430a9c54326d1c7f9ba7",
            "4fa1f9b6491548689ebac70d7fbadb21",
            "3c0513b07a524c1db890289369ad1284",
            "3cf2c48aae094c64878b52afee485528",
            "78bb44aa43de43b48e196087a01d870c",
            "43ac9b08fefb4b5faf0c2bd7800a8718",
            "e7879d115c8a491f9c39598eb1f7c122",
            "407b0df4e70c45d295ef832ef6cb82b7",
            "b71cac7bfd0f41e59a004f1e705a2865",
            "f73876bd83df4f4fa0379fa9d64bf5bb",
            "cbcf8c3718484584995dff37df9fd1e0",
            "82f73738847047449cbc853783c53ce9",
            "80e1a701bf81440a940c3842205402e6",
            "39440e494f5949d08fcf3321a038a8ac",
            "d072a7ebeec04ec080c020675022c42f",
            "dfc0086732cb4897883418ff28c85b76",
            "516cf95f2f6a4805af43d9d0ee16e12d",
            "ab28391ab438435ba16607ba37917fb1",
            "7392602663ff435d85b4f72c5e60095b",
            "018049319e964f78a9e2f99b3ff7991e",
            "d837c139bcef4b9daf6d96ffa4f399b4",
            "94b095e943834484bc4670d6721975a8",
            "b5ffc41ac0ba42a8ab71692abad897d4",
            "f6cc98b32c5d4e73bccafed3bcdd86b8",
            "02713cb4a0004862970db8633776aad3",
            "881b2e121ff443b0919b5e3552c4d577",
            "651c3f97d47e4578adeb823849f49e73",
            "7adef4e845de4eb0b49b3ed2de0c1b3b",
            "f4eb114a770a4983999e1aae24e0890c",
            "672e7a32e0aa47079f473acbe16a45e4",
            "5a302c6873e6474ca7f817a40ed393d4",
            "2ce4b9fa3e5f49d291b4d8794b43c754",
            "aabaf60fc6b34d94afaefbb100405d7c",
            "f1a21fb750814d3abf9afd1e668cb83f",
            "fdc5b979b25e4f9380dace1b0fdd33f0",
            "525279f42dae4f3f92ac3d29dfef8f84",
            "5485101168484494bb561407b662efff",
            "f6cdacaea2b74485a61d1446968b9ddf",
            "e952bcdcf91445feb94058ca50b83d10",
            "67ee3abeb0bb4f4ab97bc806f109dbca",
            "3672a385c4354685902ff1dcd821ad74",
            "078eecc29c844a989bbe1f28a70b9be6",
            "3d58dddab7d94a57a6aac28fd0600fa3",
            "c24448ea77fa466a9fc324b00ddc223c",
            "c82f1aa058954d2f8715b127fc1e12f0",
            "4d34f1510a4d4490b392e5e706901a1a",
            "0e3f24bf3f704e5d8f0e92bab8387d51",
            "e33a7cd1b97a48568dd81cb0d57aa24a",
            "39c571aa0ca847d8b1e148bd9ad64bc2",
            "da3c01fcf6d6413abea1825a003a8c78",
            "12dd75fcfb7946e78c34595ef81a0e3e",
            "86cd5a0a248e4b32af7de30299ec4d6b",
            "929b3c61dd2e44038ab33ed1ea9ff6b6",
            "0ba88763e4f54a79bfa69d74e37ab933",
            "2e71117f5cf742d5aedce7fe34bf7607",
            "80b872686b124f35a4667ad4bdccac1b",
            "b6a2e6736fed4fd8b4536b0f910ea6d4",
            "b73a2f68d59640e5af16fd27707c9c3b",
            "ea3df9cc35634a7495d54d870cc38110",
            "266fe36f4abb44f59672843f61ac3209",
            "455542ee6a7a483d9d2b9cef8fe9fa49"
          ]
        },
        "id": "nmtJvT0lRogR",
        "outputId": "7846330e-570a-46f6-efab-3b5e564cb5e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48aee96ef4284ea69e9567ee98de2f6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e055b6adc084f799320b4a7d699e6e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7879d115c8a491f9c39598eb1f7c122"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab28391ab438435ba16607ba37917fb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4eb114a770a4983999e1aae24e0890c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67ee3abeb0bb4f4ab97bc806f109dbca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12dd75fcfb7946e78c34595ef81a0e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
            "/tmp/ipython-input-2643699802.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
            "/tmp/ipython-input-2643699802.py:25: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ZeroShotAgent does not support multi-input tool llm_compressor.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2643699802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2643699802.py\u001b[0m in \u001b[0;36magent_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# --- 3. Initialize agent with tool ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     agent = initialize_agent(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/initialize.py\u001b[0m in \u001b[0;36minitialize_agent\u001b[0;34m(tools, llm, agent, callback_manager, agent_path, agent_kwargs, tags, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0magent_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAGENT_TO_CLASS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0magent_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         agent_obj = agent_cls.from_llm_and_tools(\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/mrkl/base.py\u001b[0m in \u001b[0;36mfrom_llm_and_tools\u001b[0;34m(cls, llm, tools, callback_manager, output_parser, prefix, suffix, format_instructions, input_variables, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \"\"\"\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         prompt = cls.create_prompt(\n\u001b[1;32m    141\u001b[0m             \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/mrkl/base.py\u001b[0m in \u001b[0;36m_validate_tools\u001b[0;34m(cls, tools)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseTool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mvalidate_tools_single_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             msg = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/utils.py\u001b[0m in \u001b[0;36mvalidate_tools_single_input\u001b[0;34m(class_name, tools)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_single_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{class_name} does not support multi-input tool {tool.name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: ZeroShotAgent does not support multi-input tool llm_compressor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KBState(TypedDict):\n",
        "    query: str\n",
        "    documents: Optional[List[LangchainDocument]]\n",
        "    use_reranker: bool\n",
        "    use_compressor: bool\n",
        "    rerank_result: Optional[List[LangchainDocument]]\n",
        "    compress_result: Optional[List[str]]\n",
        "    final_answer: Optional[str]\n",
        "    retriever: Any\n",
        "    reranker_tool: Any\n",
        "    compressor_tool: Any\n",
        "    llm: Any"
      ],
      "metadata": {
        "id": "YSLikAax1F9o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# =============================================================================\n",
        "# LLM SETUP UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def setup_llm(\n",
        "    model: str = \"gemini-2.0-flash\",\n",
        "    provider:str = \"google_genai\",\n",
        "    temperature: float = 0.2\n",
        "):\n",
        "    \"\"\"Initialize LLM with proper error handling.\"\"\"\n",
        "    print(f\"------Setting up LLM: {model} ({provider})------\\n\\n\")\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        return init_chat_model(\n",
        "            model=model,\n",
        "            model_provider=\"google_genai\",\n",
        "            temperature=temperature\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to initialize LLM: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# def setup_retriever(embedding_model: Any, documents_list: List, k: int = 20):\n",
        "#     \"\"\"Initializes the Chroma vector store and returns its retriever.\"\"\"\n",
        "#     vectorstore = Chroma.from_documents(\n",
        "#         documents=documents_list,\n",
        "#         embedding=embedding_model,\n",
        "#         collection_name=\"kb_cache_persist\"\n",
        "#     )\n",
        "#     return vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "async def retrieve_node(state: KBState):\n",
        "    \"\"\"Node for initial document retrieval from the vector store.\"\"\"\n",
        "    logger.info(\"---EXECUTING RETRIEVAL NODE---\")\n",
        "    query = state['query']\n",
        "    retriever = state['retriever']  # The retriever must be passed into the graph\n",
        "    # Assuming retriever is a LangChain Retriever instance or has an ainvoke method\n",
        "    documents = await retriever.ainvoke(query)\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "async def rerank_node(state: KBState):\n",
        "    \"\"\"Node for reranking retrieved documents.\"\"\"\n",
        "    logger.info(\"---EXECUTING RERANKING NODE---\")\n",
        "    query = state['query']\n",
        "    documents = state['documents']\n",
        "    reranker_tool = state['reranker_tool']\n",
        "\n",
        "    # Using the reranker tool's ainvoke method\n",
        "    # The reranker_tool.ainvoke expects {\"query\": ..., \"documents\": ...}\n",
        "    rerank_output = await reranker_tool.ainvoke({\"query\": query, \"documents\": documents})\n",
        "    reranked_docs = rerank_output.reranked_documents\n",
        "\n",
        "    return {\"documents\": reranked_docs}\n",
        "\n",
        "async def compress_node(state: KBState):\n",
        "    \"\"\"Node for compressing the top documents using an LLM.\"\"\"\n",
        "    logger.info(\"---EXECUTING COMPRESSION NODE---\")\n",
        "    query = state['query']\n",
        "    documents = state['documents']\n",
        "    compressor_tool = state['compressor_tool']\n",
        "\n",
        "    # CORRECTED: Use ainvoke on the StructuredTool with dictionary input\n",
        "    compressed_docs = await compressor_tool.ainvoke({\"query\": query, \"documents\": documents})\n",
        "\n",
        "    # The CompressorTool (via StructuredTool) returns LangchainDocument.\n",
        "    # Extract page_content to match the KBState definition for compress_result.\n",
        "    # Consider changing KBState to Optional[List[LangchainDocument]] if needed later.\n",
        "    compressed_text_list = [doc.page_content for doc in compressed_docs]\n",
        "\n",
        "    return {\"compress_result\": compressed_text_list}\n",
        "\n",
        "def generate_answer_node(state: KBState):\n",
        "    \"\"\"Node for generating the final answer using the LLM and retrieved context.\"\"\"\n",
        "    logger.info(\"---EXECUTING GENERATION NODE---\")\n",
        "    query = state['query']\n",
        "\n",
        "    # Use the compressed results if available, otherwise use the reranked/retrieved docs\n",
        "    if state.get('compress_result'):\n",
        "        context = \"\\n\".join(state['compress_result'])\n",
        "    elif state.get('documents'):\n",
        "        # Ensure we are using the documents list from the state, which might have been reranked\n",
        "        context = \"\\n\".join([doc.page_content for doc in state['documents']])\n",
        "    else:\n",
        "        context = \"No relevant documents found.\"\n",
        "\n",
        "    llm = state['llm']\n",
        "    prompt_template = f\"\"\"\n",
        "    You are an AI assistant tasked with providing comprehensive and accurate answers\n",
        "    based on the following context.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "\n",
        "    If the context doesn't contain the answer, state that you cannot answer from the provided information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming llm has an invoke method that returns an object with a .content attribute\n",
        "    response = llm.invoke(prompt_template)\n",
        "    return {\"final_answer\": response.content}\n",
        "\n",
        "def should_rerank(state: KBState):\n",
        "    \"\"\"Decide whether to proceed to reranking or skip.\"\"\"\n",
        "    # Check if 'use_reranker' is in the state and is True\n",
        "    if state.get(\"use_reranker\", False):\n",
        "        logger.info(\"---DECISION: RERANKING REQUIRED---\")\n",
        "        return \"rerank\"\n",
        "    else:\n",
        "        logger.info(\"---DECISION: SKIP RERANKING---\")\n",
        "        # This is the node to go to if reranking is skipped\n",
        "        return \"skip_rerank\"\n",
        "\n",
        "def should_compress(state: KBState):\n",
        "    \"\"\"Decide whether to proceed to compression or skip.\"\"\"\n",
        "    # Check if 'use_compressor' is in the state and is True\n",
        "    if state.get(\"use_compressor\", False):\n",
        "        logger.info(\"---DECISION: COMPRESSION REQUIRED---\")\n",
        "        return \"compress\"\n",
        "    else:\n",
        "        logger.info(\"---DECISION: SKIP COMPRESSION---\")\n",
        "        # This is the node to go to if compression is skipped\n",
        "        return \"skip_compress\"\n",
        "\n",
        "\n",
        "async def build_rag_graph():\n",
        "    # --- Initialize components (example) ---\n",
        "    # Assuming VectorStoreConfig and documents_list are defined elsewhere\n",
        "    config = VectorStoreConfig(\n",
        "        embedding_model_provider=\"sentence-transformer\",\n",
        "        embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "        chroma_persist_directory=\"./chroma_test\",\n",
        "        collection_name=\"test_collection\",\n",
        "    )\n",
        "\n",
        "    # Initialize LLM - ensure you handle API keys appropriately\n",
        "    llm = init_chat_model(\n",
        "        \"gemini-2.5-flash\",\n",
        "        model_provider=\"google-genai\",\n",
        "        temperature=0.2,\n",
        "        # rate_limiter=InMemoryRateLimiter(requests_per_second=0.1), # Add rate limiting if needed\n",
        "    )\n",
        "\n",
        "    # Initialize Vector Store Manager and Retriever\n",
        "    # Assuming documents_list is available and populated\n",
        "    vectorstore_manager = VectorStoreManager(config, initial_documents=documents_list)\n",
        "    # Get the retriever instance from the manager\n",
        "    retriever = vectorstore_manager.vectorstore.as_retriever(search_kwargs={\"k\": 20}) # Retrieve enough for reranking\n",
        "\n",
        "    # Initialize Reranker Tool\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Embedding model for Reranker if needed\n",
        "    # Pass the vectorstore instance to RerankerTool if it needs access\n",
        "    reranker_instance = RerankerTool(embedding_model=embedding_model, vectorstore=vectorstore_manager.vectorstore, top_n=5) # Rerank to top 5\n",
        "    reranker_tool = reranker_instance.as_tool()\n",
        "\n",
        "    # Initialize Compressor Tool\n",
        "    compressor = CompressorTool(llm=llm)\n",
        "    compressor_tool = compressor.as_tool()\n",
        "\n",
        "\n",
        "    # --- Build the graph ---\n",
        "    # Define the state machine\n",
        "    builder = StateGraph(KBState)\n",
        "\n",
        "    # Add nodes\n",
        "    builder.add_node(\"retrieve\", retrieve_node)\n",
        "    builder.add_node(\"rerank\", rerank_node)\n",
        "    builder.add_node(\"compress\", compress_node)\n",
        "    builder.add_node(\"generate\", generate_answer_node)\n",
        "\n",
        "    # --- Define the entry point ---\n",
        "    # Correctly set the entry point to the name of the first node\n",
        "    builder.set_entry_point(\"retrieve\")\n",
        "\n",
        "    # --- Define edges ---\n",
        "    # The edge from START to retrieve is implicitly handled by set_entry_point(\"retrieve\")\n",
        "\n",
        "    # Conditional edge from retrieve node\n",
        "    builder.add_conditional_edges(\n",
        "        \"retrieve\", # Source node\n",
        "        should_rerank, # Decision function\n",
        "        {\n",
        "            \"rerank\": \"rerank\",     # If should_rerank returns \"rerank\", go to \"rerank\" node\n",
        "            \"skip_rerank\": \"generate\" # If should_rerank returns \"skip_rerank\", go to \"generate\" node\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Conditional edge from rerank node\n",
        "    builder.add_conditional_edges(\n",
        "        \"rerank\", # Source node\n",
        "        should_compress, # Decision function\n",
        "        {\n",
        "            \"compress\": \"compress\",     # If should_compress returns \"compress\", go to \"compress\" node\n",
        "            \"skip_compress\": \"generate\" # If should_compress returns \"skip_compress\", go to \"generate\" node\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Edge from compress to generate (if executed)\n",
        "    builder.add_edge(\"compress\", \"generate\")\n",
        "\n",
        "    # Edge from generate to end\n",
        "    builder.add_edge(\"generate\", END)\n",
        "\n",
        "    # Compile the graph\n",
        "    app = builder.compile()\n",
        "\n",
        "    # --- Example invocation with reranking and compression ---\n",
        "    print(\"--- INVOKING GRAPH WITH RERANKING AND COMPRESSION ---\")\n",
        "    inputs = {\n",
        "        \"query\": \"What effect did the plasma fire have on the silver veins?\", # Use a query relevant to the documents\n",
        "        \"use_reranker\": True,\n",
        "        \"use_compressor\": True,\n",
        "        \"llm\": llm,\n",
        "        \"retriever\": retriever, # Pass the retriever instance\n",
        "        \"reranker_tool\": reranker_tool, # Pass the reranker tool instance\n",
        "        \"compressor_tool\": compressor_tool # Pass the compressor tool instance\n",
        "    }\n",
        "    # The ainvoke method returns the final state of the graph\n",
        "    result = await app.ainvoke(inputs)\n",
        "    print(\"\\nFinal Answer (with rerank & compress):\", result.get('final_answer', 'N/A')) # Use .get to safely access\n",
        "\n",
        "    # --- Example invocation with only reranking ---\n",
        "    print(\"\\n--- INVOKING GRAPH WITH ONLY RERANKING ---\")\n",
        "    inputs = {\n",
        "        \"query\": \"What was the message from Titan?\", # Use a query relevant to the documents\n",
        "        \"use_reranker\": True,\n",
        "        \"use_compressor\": False,\n",
        "        \"llm\": llm, # Corrected typo\n",
        "        \"retriever\": retriever,\n",
        "        \"reranker_tool\": reranker_tool,\n",
        "        \"compressor_tool\": compressor_tool # Still need to pass the tool even if not used due to KBState definition\n",
        "    }\n",
        "    result = await app.ainvoke(inputs)\n",
        "    print(\"\\nFinal Answer (with rerank):\", result.get('final_answer', 'N/A'))\n",
        "\n",
        "    # --- Example invocation with no reranking or compression ---\n",
        "    print(\"\\n--- INVOKING GRAPH WITH NO RERANKING OR COMPRESSION ---\")\n",
        "    inputs = {\n",
        "        \"query\": \"Who was Captain Liora Vega?\", # Use a query relevant to the documents\n",
        "        \"use_reranker\": False,\n",
        "        \"use_compressor\": False,\n",
        "        \"llm\": llm,\n",
        "        \"retriever\": retriever,\n",
        "        \"reranker_tool\": reranker_tool, # Still need to pass the tool\n",
        "        \"compressor_tool\": compressor_tool # Still need to pass the tool\n",
        "    }\n",
        "    result = await app.ainvoke(inputs)\n",
        "    print(\"\\nFinal Answer (no rerank or compress):\", result.get('final_answer', 'N/A'))\n",
        "\n",
        "\n",
        "# Run the test function\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(build_rag_graph())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSc3ZI25rFfE",
        "outputId": "67ee6331-28e5-4fe9-f33f-d1ac805ad166"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- INVOKING GRAPH WITH RERANKING AND COMPRESSION ---\n",
            "\n",
            "Final Answer (with rerank & compress): The plasma fire caused the silver veins to flare white-hot and rupture into cascading sparks.\n",
            "\n",
            "--- INVOKING GRAPH WITH ONLY RERANKING ---\n",
            "\n",
            "Final Answer (with rerank): The message from Titan was a warning from glowing, intelligent creatures. It consisted of mathematical signals and images depicting burning cities, dark skies, and oceans turning black, resembling Earth. The beings communicated that they were not from Titan but traveled between stars, and they saw humanity making the same mistakes that led to the destruction of another world. They warned that if Earth continued on its current path, its oceans would die. They also extended an invitation for humanity to join them. Captain Vega summarized the core message as: \"Change, or perish.\"\n",
            "\n",
            "--- INVOKING GRAPH WITH NO RERANKING OR COMPRESSION ---\n",
            "\n",
            "Final Answer (no rerank or compress): Captain Liora Vega was the leader of the Aurora, the first human crew chosen to journey to Titan. She was an explorer who made the decision to follow an alien signal on Titan, leading to humanity's first contact with extraterrestrial beings.\n",
            "\n",
            "After returning to Earth with warnings about a destructive \"web\" entity, she and her crew became both heroes and targets, eventually being labeled \"Traitors to Humanity\" by the Council. She is a resolute and commanding figure, and during her trials, she awakened the \"Path of the Blade Star\" as a cultivator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Optional, Literal, Annotated, Union\n",
        "from datetime import datetime, timezone\n",
        "from enum import Enum\n",
        "import operator\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "# ----------------------------\n",
        "# Pydantic Models for Grading\n",
        "# ----------------------------\n",
        "class RetrievalGrade(BaseModel):\n",
        "    score: int = Field(..., description=\"Binary relevance: 1 = relevant, 0 = not relevant.\")\n",
        "    verdict: str = Field(..., description=\"'yes' or 'no' (mirror of score).\")\n",
        "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0.00-1.00).\")\n",
        "    rationale: str = Field(..., description=\"Step-by-step rationale (max ~120 words).\")\n",
        "    highlights: Optional[List[str]] = Field(None, description=\"Up to 3 short excerpts (≤30 words each).\")\n",
        "    refined_query: Optional[str] = Field(None, description=\"Refined query if score == 0 or low confidence.\")\n",
        "\n",
        "# Enums for consistent categorization\n",
        "class DocumentSource(str, Enum):\n",
        "    \"\"\"Source types for a document.\"\"\"\n",
        "    KNOWLEDGE_BASE = \"knowledge_base\"\n",
        "    EXTERNAL_API = \"external_api\"\n",
        "    WEB_CRAWL = \"web_crawl\"\n",
        "    SEARCH_SNIPPET = \"search_snippet\"\n",
        "    CACHE = \"cache\"\n",
        "\n",
        "class ContentSource(str, Enum):\n",
        "    \"\"\"Semantic categories assigned by LLM (e.g., academic, blog, forum).\"\"\"\n",
        "    ACADEMIC = \"academic\"\n",
        "    GOVERNMENT = \"government\"\n",
        "    WIKI = \"wikipedia\"\n",
        "    GIT_REPO = \"git_repo\"\n",
        "    NEWS = \"news\"\n",
        "    BLOG = \"blog\"\n",
        "    REPORT = \"report\"\n",
        "    FORUM = \"forum\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "# Sub-Models for structured data\n",
        "class DocumentResult(BaseModel):\n",
        "    \"\"\"Standardized document result with metadata.\"\"\"\n",
        "    page_content: str = Field(..., description=\"The document content.\")\n",
        "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Metadata associated with the document.\")\n",
        "    content_length: Optional[int] = Field(default=0, ge=0, description=\"Content length in characters.\")\n",
        "    source_type: DocumentSource = Field(default=DocumentSource.SEARCH_SNIPPET, description=\"The origin of the document content.\")\n",
        "    content_source: Optional[ContentSource] = Field(default=ContentSource.OTHER, description=\"The semantic category of the content.\")\n",
        "    relevance_score: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"A score indicating relevance.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    document_id: Optional[str] = Field(default=None, description=\"Stable unique identifier for the document.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def compute_content_length(self) -> \"DocumentResult\":\n",
        "        \"\"\"Ensures content_length is computed from page_content if missing/zero.\"\"\"\n",
        "        if self.page_content and (not self.content_length or self.content_length == 0):\n",
        "            object.__setattr__(self, \"content_length\", len(self.page_content))\n",
        "\n",
        "        #  Always ensure document_id is stable (sha256 of content)\n",
        "        if not self.document_id:\n",
        "            did = hashlib.sha256(self.page_content.encode(\"utf-8\")).hexdigest()\n",
        "            object.__setattr__(self, \"document_id\", did)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def to_langchain(self):\n",
        "        \"\"\"Convert to LangChain Document.\"\"\"\n",
        "        from langchain_core.documents import Document\n",
        "        return Document(page_content=self.page_content, metadata=self.metadata)\n",
        "\n",
        "    @classmethod\n",
        "    def from_langchain(cls, doc: \"Document\", relevance_score: Optional[float] = None) -> \"DocumentResult\":\n",
        "        \"\"\"Build from a LangChain Document (reverse of to_langchain).\"\"\"\n",
        "        return cls(\n",
        "            page_content=doc.page_content,\n",
        "            metadata=doc.metadata,\n",
        "            relevance_score=relevance_score,\n",
        "            created_at=datetime.now(timezone.utc),\n",
        "            document_id=hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest(),\n",
        "        )\n",
        "\n",
        "\n",
        "class VectorStoreResponse(BaseModel):\n",
        "    \"\"\"Unified response schema for vector store operations.\"\"\"\n",
        "\n",
        "    query: Optional[str] = None  # retrieval query\n",
        "    results: Optional[List[\"DocumentResult\"]] = None  # retrieval results\n",
        "    status: str = Field(..., description=\"Operation status: success or failed.\")\n",
        "    message: str = Field(..., description=\"Human-readable summary of the result.\")\n",
        "    document_ids: List[str] = Field(default_factory=list, description=\"List of indexed document IDs.\")\n",
        "    indexed_documents: List[\"DocumentResult\"] = Field(default_factory=list, description=\"The full indexed documents.\")\n",
        "    error: Optional[str] = Field(default=None, description=\"Error message if any.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def validate_exclusive_fields(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure that document_ids/indexed_documents and results are mutually exclusive.\"\"\"\n",
        "        if self.results and (self.document_ids or self.indexed_documents):\n",
        "            raise ValueError(\"Response cannot contain both retrieval results and indexing results.\")\n",
        "        return self\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def sync_document_ids(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure document_ids matches indexed_documents' IDs if provided.\"\"\"\n",
        "        if self.indexed_documents:\n",
        "            ids = [doc.document_id for doc in self.indexed_documents if doc.document_id]\n",
        "            object.__setattr__(self, \"document_ids\", ids)\n",
        "        return self\n",
        "\n",
        "class ExtractedURL(BaseModel):\n",
        "    \"\"\"URL with metadata extracted during the search phase.\"\"\"\n",
        "    query: str\n",
        "    url: str = Field(..., description=\"The URL to crawl.\")\n",
        "    domain: Optional[str] = Field(default=None, description=\"Domain extracted from URL.\")\n",
        "    should_crawl: bool = Field(True, description=\"Whether this URL should be crawled.\")\n",
        "    content_source: Optional[ContentSource] = Field(default=ContentSource.OTHER, description=\"Content source or citation.\")\n",
        "    title: Optional[str] = Field(default=None, description=\"Page title if available.\")\n",
        "    snippet: Optional[str] = Field(default=None, description=\"Content snippet from search.\")\n",
        "    source_engine: Optional[str] = Field(default=None, description=\"Search engine that found this URL.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def compute_domain(self) -> \"ExtractedURL\":\n",
        "        \"\"\"Auto-computes the domain from the URL.\"\"\"\n",
        "        if not self.domain and self.url:\n",
        "            try:\n",
        "                parsed_url = urlparse(self.url)\n",
        "                if parsed_url.netloc:\n",
        "                    object.__setattr__(self, \"domain\", parsed_url.netloc)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return self\n",
        "\n",
        "class SearchResultSchema(BaseModel):\n",
        "    \"\"\"Standardized schema for raw search engine results.\"\"\"\n",
        "    url: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    engine: str\n",
        "    query: str\n",
        "    metadata: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "# LLM-specific output schemas\n",
        "class GraderOutput(BaseModel):\n",
        "    \"\"\"Output schema for the document relevance grader.\"\"\"\n",
        "    binary_score: str = Field(description=\"A binary score ('yes' or 'no').\")\n",
        "    justification: Optional[str] = Field(description=\"Explanation for the relevance score.\")\n",
        "\n",
        "class SubqueriesOutput(BaseModel):\n",
        "    \"\"\"Output schema for the subquery generator.\"\"\"\n",
        "    queries: List[str] = Field(description=\"A list of distinct, optimized search queries to perform for gathering additional information.\")\n",
        "    justification: Optional[str] = Field(description=\"A brief explanation of why these specific subqueries were generated.\")\n",
        "\n",
        "class ReflectionOutput(BaseModel):\n",
        "    \"\"\"Output schema for the reflection tool.\"\"\"\n",
        "    assessment: str = Field(description=\"A textual assessment of the gathered documents.\")\n",
        "    decision: Literal[\"refine\", \"sufficient\"] = Field(description=\"Decision to 'refine' or proceed.\")\n",
        "\n",
        "class RerankerOutput(BaseModel):\n",
        "    \"\"\"Output schema for the reranker tool.\"\"\"\n",
        "    reranked_documents: List[LangchainDocument] = Field(description=\"A list of documents sorted by relevance score.\")\n",
        "    justification: Optional[str] = Field(description=\"A brief explanation of the reranking.\")\n",
        "\n",
        "# ----------------------------\n",
        "# Pydantic State Models\n",
        "# ----------------------------\n",
        "class KBState(BaseModel):\n",
        "    \"\"\"\n",
        "    Represents the complete state of our comprehensive knowledge base agent.\n",
        "    Using Pydantic BaseModel for structured state management with validation.\n",
        "    \"\"\"\n",
        "    # Core conversation and final output\n",
        "    query: str = Field(default=\"\", description=\"The user's original query\")\n",
        "    session_id: str = Field(default=\"default\", description=\"Unique session identifier\")\n",
        "    generation: Optional[str] = Field(default=None, description=\"Generated response\")\n",
        "\n",
        "    # Session and status tracking\n",
        "    status: Optional[str] = Field(default=None, description=\"Current workflow status\")\n",
        "    error_message: Optional[str] = Field(default=None, description=\"Error message if any\")\n",
        "    user_preferences: Dict[str, Any] = Field(default_factory=dict, description=\"User preferences\")\n",
        "\n",
        "    # Iteration and loop control\n",
        "    iterations: int = Field(default=0, description=\"Current iteration count\")\n",
        "\n",
        "    # Retrieval and search\n",
        "    cache_hit: Optional[bool] = Field(default=None, description=\"Whether cache was hit\")\n",
        "    web_search_required: Optional[bool] = Field(default=None, description=\"Whether web search is needed\")\n",
        "    subqueries: List[SubqueriesOutput] = Field(default_factory=list, description=\"Generated subqueries\")\n",
        "\n",
        "    # Document handling\n",
        "    retrieved_documents: List[LangchainDocument] = Field(default_factory=list, description=\"Retrieved documents\")\n",
        "    graded_docs: List[DocumentResult] = Field(default_factory=list, description=\"Graded documents\")\n",
        "    retrieval_grade: Optional[RetrievalGrade] = Field(default=None, description=\"Document relevance grade\")\n",
        "    cleaned_chunks: List[LangchainDocument] = Field(default_factory=list, description=\"Cleaned document chunks\")\n",
        "\n",
        "    # Context and reranking\n",
        "    final_context: Optional[str] = Field(default=None, description=\"Final context for generation\")\n",
        "\n",
        "    # Background task management\n",
        "    crawl_task_handle: Optional[asyncio.Task] = Field(default=None, description=\"Background crawl task\")\n",
        "\n",
        "    # Metrics and logging\n",
        "    metrics: Dict[str, Any] = Field(default_factory=dict, description=\"Performance metrics\")\n",
        "\n",
        "    # Other flags\n",
        "    escalation_needed: bool = Field(default=False, description=\"Whether escalation is needed\")\n",
        "    kb_result: Optional[str] = Field(default=None, description=\"KB lookup result\")\n",
        "    kb_lookup_status: Literal[\"hit\", \"miss\", \"not_attempted\"] = Field(default=\"not_attempted\", description=\"KB lookup status\")\n",
        "    crawl_status: Literal[\"hit\", \"miss\", \"not_attempted\"] = Field(default=\"not_attempted\", description=\"Web crawl status\")\n",
        "    crawl_results: Optional[List[LangchainDocument]] = Field(default=None, description=\"Web crawl results\")\n",
        "    messages: List[BaseMessage] = Field(default_factory=list, description=\"Conversation messages\")\n",
        "\n",
        "    # Reflection state\n",
        "    reflection_output: Optional[ReflectionOutput] = Field(default=None, description=\"Reflection assessment\")\n",
        "    reflection_decision: Optional[str] = Field(default=None, description=\"Reflection decision\")\n",
        "\n",
        "    use_reranker: bool\n",
        "    use_compressor: bool\n",
        "    rerank_result: Optional[List[LangchainDocument]]\n",
        "    compress_result: Optional[List[str]]\n",
        "    final_answer: Optional[str]\n",
        "    retriever: Any\n",
        "    reranker_tool: Any\n",
        "    compressor_tool: Any\n",
        "    llm: Optional[Any] = None\n",
        "\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "        validate_assignment = True\n",
        "\n",
        "    def model_copy_with_changes(self, **changes) -> \"KBState\":\n",
        "        \"\"\"Create a copy of the model with specified changes.\"\"\"\n",
        "        current_dict = self.model_dump()\n",
        "\n",
        "        # Handle list concatenation for specific fields\n",
        "        list_fields_to_concat = [\"retrieved_documents\", \"cleaned_chunks\", \"messages\"]\n",
        "        for field in list_fields_to_concat:\n",
        "            if field in changes and field in current_dict:\n",
        "                if isinstance(changes[field], list) and isinstance(current_dict[field], list):\n",
        "                    changes[field] = current_dict[field] + changes[field]\n",
        "\n",
        "        # Handle dict merging for metrics\n",
        "        if \"metrics\" in changes and \"metrics\" in current_dict:\n",
        "            merged_metrics = {**current_dict[\"metrics\"], **changes[\"metrics\"]}\n",
        "            changes[\"metrics\"] = merged_metrics\n",
        "\n",
        "        current_dict.update(changes)\n",
        "        return KBState.model_validate(current_dict)\n",
        "\n",
        "class CacheState(BaseModel):\n",
        "    \"\"\"State for the main cache workflow.\"\"\"\n",
        "    messages: List[BaseMessage] = Field(default_factory=list, description=\"Conversation messages\")\n",
        "    session_id: str = Field(default=\"default\", description=\"Session identifier\")\n",
        "    escalation_needed: bool = Field(default=False, description=\"Whether escalation is needed\")\n",
        "    cache_result: Optional[str] = Field(default=None, description=\"Cache lookup result\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "        validate_assignment = True\n",
        "\n",
        "    def model_copy_with_changes(self, **changes) -> \"CacheState\":\n",
        "        \"\"\"Create a copy of the model with specified changes.\"\"\"\n",
        "        current_dict = self.model_dump()\n",
        "\n",
        "        # Handle message concatenation\n",
        "        if \"messages\" in changes and \"messages\" in current_dict:\n",
        "            if isinstance(changes[\"messages\"], list) and isinstance(current_dict[\"messages\"], list):\n",
        "                changes[\"messages\"] = current_dict[\"messages\"] + changes[\"messages\"]\n",
        "\n",
        "        current_dict.update(changes)\n",
        "        return CacheState.model_validate(current_dict)\n",
        "\n",
        "class MainState(BaseModel):\n",
        "    \"\"\"Main Agent State.\"\"\"\n",
        "    messages: List[BaseMessage] = Field(default_factory=list, description=\"Conversation messages\")\n",
        "    session_id: str = Field(default=\"default\", description=\"Session identifier\")\n",
        "    escalation_needed: bool = Field(default=False, description=\"Whether escalation is needed\")\n",
        "    cache_result: Optional[str] = Field(default=None, description=\"Cache lookup result\")\n",
        "    kb_result: Optional[str] = Field(default=None, description=\"KB agent result\")\n",
        "    final_answer: Optional[str] = Field(default=None, description=\"Final response\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "        validate_assignment = True\n",
        "\n",
        "tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
        "tavily_client = TavilySearchAPIWrapper(tavily_api_key=tavily_api_key) if tavily_api_key else None\n",
        "ddg_tool = DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper())\n",
        "serper_api_key = os.environ.get(\"SERPER_API_KEY\")\n",
        "serper_client = GoogleSerperAPIWrapper(serper_api_key=serper_api_key) if serper_api_key else None\n",
        "\n",
        "# Define the allowed embedding providers as a Literal\n",
        "EMBEDDING_PROVIDERS = Literal[\"sentence-transformer\", \"huggingface\", \"google_genai\"]\n",
        "CHUNKING_STRATEGIES = ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# KNOWLEDGE BASE AGENT NODES\n",
        "# =============================================================================\n",
        "\n",
        "class KnowledgeBaseAgent:\n",
        "    \"\"\"Knowledge Base Agent with all node implementations.\"\"\"\n",
        "\n",
        "    def __init__(self, documents_list, cache_adapter=None):\n",
        "        self.documents_list = documents_list\n",
        "        self.cache_adapter = cache_adapter\n",
        "\n",
        "        self.config = VectorStoreConfig(\n",
        "            embedding_model_provider=\"sentence-transformer\",\n",
        "            embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "            chroma_persist_directory=\"./chroma_test\",\n",
        "            collection_name=\"test_collection\",\n",
        "        )\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = self.setup_llm()\n",
        "\n",
        "        # Vector Store\n",
        "        vectorstore_manager = VectorStoreManager(self.config, initial_documents=documents_list)\n",
        "        self.retriever = vectorstore_manager.vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
        "\n",
        "        # Reranker\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        reranker_instance = RerankerTool(\n",
        "            embedding_model=self.embedding_model,\n",
        "            vectorstore=vectorstore_manager.vectorstore,\n",
        "            top_n=5\n",
        "        )\n",
        "        self.reranker_tool = reranker_instance.as_tool()\n",
        "\n",
        "        # Compressor\n",
        "        compressor = CompressorTool(self.llm)\n",
        "        self.compressor_tool = compressor.as_tool()\n",
        "        self.grader = self.setup_grader(self.llm)\n",
        "        self.query_rewriter = self.setup_query_rewriter(self.llm)\n",
        "        self.reflection_tool = self.setup_reflection_tool(self.llm)\n",
        "\n",
        "\n",
        "\n",
        "def setup_llm(\n",
        "    self,\n",
        "    model: str = \"gemini-2.0-flash\",\n",
        "    provider:str = \"google_genai\",\n",
        "    temperature: float = 0.2\n",
        "):\n",
        "    \"\"\"Initialize LLM with proper error handling.\"\"\"\n",
        "    print(f\"------Setting up LLM: {model} ({provider})------\\n\\n\")\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        return init_chat_model(\n",
        "            model=model,\n",
        "            model_provider=\"google_genai\",\n",
        "            temperature=temperature\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to initialize LLM: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def setup_grader(self, llm: Any):\n",
        "    \"\"\"Setup document relevance grader.\"\"\"\n",
        "    system_prompt = \"\"\"\n",
        "    You are an expert grader judging whether a retrieved DOCUMENT is sufficiently\n",
        "    relevant, correct, recent, and comprehensive for the USER QUESTION.\n",
        "\n",
        "    PRIORITIES (in order):\n",
        "    1. RELEVANCE: Does the document contain statements directly answering the question?\n",
        "    2. CORRECTNESS: Is the information internally consistent and plausible?\n",
        "    3. RECENCY: If time-sensitive, prefer newer/dated information?\n",
        "    4. COVERAGE: Does it cover core aspects or only peripheral points?\n",
        "\n",
        "    Scoring rules:\n",
        "    - score = 1 (verdict='yes') if document contains relevant statements\n",
        "    - score = 0 (verdict='no') if no relevant statements or contradictory facts\n",
        "\n",
        "    For partial coverage, prefer score=1 but lower confidence and explain gaps.\n",
        "    \"\"\"\n",
        "\n",
        "    structured_llm_grader = llm.with_structured_output(RetrievalGrade)\n",
        "    grade_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"USER QUESTION:\\n{question}\\n\\nDOCUMENTS:\\n{documents}\\n\\nCONTEXT:\\n{context}\"),\n",
        "    ])\n",
        "\n",
        "    return grade_prompt | structured_llm_grader\n",
        "\n",
        "def setup_query_rewriter(self, llm: Any):\n",
        "    \"\"\"Setup query rewriter for generating subqueries.\"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are an expert search query generator. Your task is to analyze a user's question \"\n",
        "        \"and generate a list of distinct search queries that will help find more comprehensive \"\n",
        "        \"and relevant information. Generate only the most useful and focused queries, and keep the list concise.\"\n",
        "    )\n",
        "\n",
        "    structured_llm_rewriter = llm.with_structured_output(SubqueriesOutput)\n",
        "    rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ])\n",
        "\n",
        "    return rewrite_prompt | structured_llm_rewriter\n",
        "\n",
        "def setup_reflection_tool(self, llm:Any):\n",
        "    \"\"\"Setup reflection tool for assessing document quality.\"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are an expert document assessor. Evaluate whether the gathered documents \"\n",
        "        \"provide sufficient information to answer the user's question comprehensively. \"\n",
        "        \"Consider relevance, completeness, and quality of information.\"\n",
        "        \"Return 'refine' if more searching is needed, or 'sufficient' if the results are adequate.\"\n",
        "    )\n",
        "\n",
        "    structured_llm_reflection = llm.with_structured_output(ReflectionOutput)\n",
        "    reflection_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"Question: {question}\\n\\nDocuments: {documents}\\n\\nAssess whether these documents are sufficient.\"),\n",
        "    ])\n",
        "\n",
        "    return reflection_prompt | structured_llm_reflection\n",
        "\n",
        "\n",
        "\n",
        "async def kb_retrieve_node(self, state: KBState) -> KBState:\n",
        "    \"\"\"Node for initial document retrieval from the vector store.\"\"\"\n",
        "    logger.info(\"---EXECUTING RETRIEVAL NODE---\")\n",
        "\n",
        "    # Get the query from messages or direct query field\n",
        "    query = state.query or (state.messages[-1].content if state.messages else \"\")\n",
        "    session_id = state.session_id\n",
        "\n",
        "    try:\n",
        "        # Retriever is a LangChain Retriever instance or has an ainvoke method\n",
        "        retrieved_documents: List[LangchainDocument] = await retriever.ainvoke(query)\n",
        "\n",
        "        # Filter documents\n",
        "        retrieved_documents = [\n",
        "            doc for doc in retrieved_documents\n",
        "            if isinstance(doc, LangchainDocument) and hasattr(doc, \"page_content\") and doc.page_content.strip()\n",
        "        ]\n",
        "\n",
        "        # if list is empty, route to search\n",
        "        if not retrieved_documents:\n",
        "            print(\"KB Agent: No relevant documents found in the vector store.\")\n",
        "            return state.model_copy_with_changes(\n",
        "                kb_lookup_status=\"miss\",\n",
        "                web_search_required=True,\n",
        "                escalation_required=True,\n",
        "                query=query,\n",
        "            )\n",
        "\n",
        "        return state.model_copy_with_changes(\n",
        "            kb_lookup_status=\"hit\",\n",
        "            web_search_required=False,\n",
        "            escalation_required=False,\n",
        "            query=query,\n",
        "            retrieved_documents=retrieved_documents\n",
        "          )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"KB lookup error: {e}\")\n",
        "        return state.model_copy_with_changes(\n",
        "            error_message=f\"KB lookup failed: {e}\",\n",
        "            kb_lookup_status=\"miss\",\n",
        "            web_search_required=True,\n",
        "            escalation_needed=True,\n",
        "            query=query\n",
        "        )\n",
        "\n",
        "async def kb_grade_node(self, state: KBState) -> KBState:\n",
        "      \"\"\"Grade the retrieved documents for relevance.\"\"\"\n",
        "      print(\"\\n--- Knowledge Base Agent Node: Evaluating Retrieval ---\")\n",
        "\n",
        "      query = state.query\n",
        "      documents = state.retrieved_documents\n",
        "\n",
        "      if not documents:\n",
        "          return state.model_copy_with_changes(\n",
        "              error_message=\"No documents to grade\",\n",
        "              web_search_required=True\n",
        "          )\n",
        "\n",
        "      try:\n",
        "          retrieval_grade = await self.grader.ainvoke({\n",
        "              \"question\": query,\n",
        "              \"documents\": [doc.page_content for doc in documents],\n",
        "              \"context\": \"\"\n",
        "          })\n",
        "\n",
        "          # Determine if web search is needed based on grade\n",
        "          web_search_needed = (\n",
        "              retrieval_grade.score == 0 or\n",
        "              retrieval_grade.confidence < 0.7 or\n",
        "              retrieval_grade.verdict.lower() == \"no\"\n",
        "          )\n",
        "\n",
        "          return state.model_copy_with_changes(\n",
        "              retrieval_grade=retrieval_grade,\n",
        "              web_search_required=web_search_needed,\n",
        "              escalation_needed=web_search_needed\n",
        "          )\n",
        "\n",
        "      except Exception as e:\n",
        "          logger.error(f\"Grading error: {e}\")\n",
        "          return state.model_copy_with_changes(\n",
        "              error_message=f\"Grading failed: {e}\",\n",
        "              web_search_required=True\n",
        "          )\n",
        "\n",
        "\n",
        "async def kb_update_iteration_node(self, state: KBState) -> KBState:\n",
        "    \"\"\"Update iteration counter.\"\"\"\n",
        "    return {\"iterations\": state.iterations + 1}\n",
        "\n",
        "async def kb_rewrite_node(self, state: KBState) -> KBState:\n",
        "    \"\"\"Rewrite query for better search results.\"\"\"\n",
        "    print(\"\\n--- Knowledge Base Agent Node: Rewriting query ---\")\n",
        "\n",
        "    original_query = state.query\n",
        "    grade = state.retrieval_grade\n",
        "\n",
        "    if not original_query:\n",
        "      logger.warning(\"No question found in state. Skipping query rewriting.\")\n",
        "      return state.model_copy_with_changes(\n",
        "          query=original_query,\n",
        "          subqueries=[original_query]\n",
        "      )\n",
        "\n",
        "    try:\n",
        "\n",
        "      logger.info(f\"Rewriting query for: '{original_query}'\")\n",
        "\n",
        "      subquery_output: SubqueriesOutput = await self.query_rewriter.ainvoke(original_query)\n",
        "\n",
        "      subqueries = subquery_output.queries\n",
        "      logger.info(f\"Generated subqueries: {subqueries}\")\n",
        "\n",
        "      return state.model_copy_with_changes(\n",
        "          query=original_query,\n",
        "          subqueries=subquery_output\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      logger.error(f\"Query rewriting error: {e}\")\n",
        "      return state.model_copy_with_changes(\n",
        "          error_message=f\"Query rewriting failed: {e}\",\n",
        "          subqueries=SubqueriesOutput(queries=[], justification=\"\")\n",
        "      )\n",
        "\n",
        "\n",
        "async def kb_rerank_node(self, state: KBState) -> KBState:\n",
        "    \"\"\"Node for reranking retrieved documents.\"\"\"\n",
        "    logger.info(\"---EXECUTING RERANKING NODE---\")\n",
        "    query = state.query\n",
        "\n",
        "    # Using the reranker tool's ainvoke method\n",
        "    docs = await self.reranker_tool.ainvoke({\"docs\": state.retrieved_documents, \"query\": state.query})\n",
        "    reranked_docs = docs.reranked_documents\n",
        "\n",
        "    return {\"documents\": reranked_docs}\n",
        "\n",
        "async def kb_compress_node(state: KBState) -> KBState:\n",
        "    \"\"\"Node for compressing the top documents using an LLM.\"\"\"\n",
        "    logger.info(\"---EXECUTING COMPRESSION NODE---\")\n",
        "    query = state['query']\n",
        "    documents = state['documents']\n",
        "    compressor_tool = state['compressor_tool']\n",
        "\n",
        "    compressed_docs = await compressor_tool.ainvoke({\"query\": query, \"documents\": documents})\n",
        "\n",
        "    # The CompressorTool (via StructuredTool) returns LangchainDocument.\n",
        "    # Extract page_content to match the KBState definition for compress_result.\n",
        "    # Consider changing KBState to Optional[List[LangchainDocument]] if needed later.\n",
        "    compressed_text_list = [doc.page_content for doc in compressed_docs]\n",
        "\n",
        "    return state.model_copy_with_changes(\n",
        "        compress_result= compressed_text_list\n",
        "    )\n",
        "\n",
        "\n",
        "async def kb_assess_and_reflect_node(self, state: KBState) -> KBState:\n",
        "    \"\"\"Assess document quality and decide next steps.\"\"\"\n",
        "    logger.info(\"KB Agent: Assessing and reflecting\")\n",
        "\n",
        "    try:\n",
        "        all_docs = (state.retrieved_documents or []) + (state.crawl_results or [])\n",
        "\n",
        "        reflection_result = await self.reflection_tool.ainvoke({\n",
        "            \"docs\": state.compress_result,\n",
        "            \"query\": state.query\n",
        "        })\n",
        "\n",
        "        decision = reflection_result.decision\n",
        "        logger.info(f\"KB Agent: Reflection decision = {decision}\")\n",
        "\n",
        "        return {\n",
        "            \"reflection_output\": reflection_result,\n",
        "            \"reflection_decision\": decision\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Reflection error: {e}\")\n",
        "        return {\n",
        "            \"error_message\": f\"Reflection failed: {e}\",\n",
        "            \"reflection_decision\": \"sufficient\"  # Default to sufficient on error\n",
        "        }\n",
        "\n",
        "def kb_generate_answer_node(self, state: KBState) -> KBState:\n",
        "    \"\"\"Node for generating the final answer using the LLM and retrieved context.\"\"\"\n",
        "    logger.info(\"---EXECUTING GENERATION NODE---\")\n",
        "    query = state.query or (state.messages[-1].content if state.messages else \"\")\n",
        "    session_id = state.session_id\n",
        "\n",
        "    try:\n",
        "      # Use the compressed results if available, otherwise use the reranked/retrieved docs\n",
        "      if state.get('compress_result'):\n",
        "          context = \"\\n\".join(state['compress_result'])\n",
        "      elif state.get('documents'):\n",
        "          # Ensure we are using the documents list from the state, which might have been reranked\n",
        "          context = \"\\n\".join([doc.page_content for doc in state['documents']])\n",
        "      else:\n",
        "          context = \"No relevant documents found.\"\n",
        "\n",
        "      self.llm = state['llm']\n",
        "      prompt_template = f\"\"\"\n",
        "      You are an AI assistant tasked with providing comprehensive and accurate answers\n",
        "      based on the following context.\n",
        "\n",
        "      Context:\n",
        "      {context}\n",
        "\n",
        "      Question:\n",
        "      {query}\n",
        "\n",
        "      If the context doesn't contain the answer, state that you cannot answer from the provided information.\n",
        "      \"\"\"\n",
        "\n",
        "      # Assuming llm has an invoke method that returns an object with a .content attribute\n",
        "      response = self.llm.invoke(prompt_template)\n",
        "      return {\"final_answer\": response.content}\n",
        "    except Exception as e:\n",
        "      logger.error(f\"Generation error: {e}\")\n",
        "      return {\n",
        "            \"error_message\": f\"Failed to generate reply: {e}\"\n",
        "        }\n",
        "\n",
        "def should_rerank(state: KBState):\n",
        "    \"\"\"Decide whether to proceed to reranking or skip.\"\"\"\n",
        "    # Check if 'use_reranker' is in the state and is True\n",
        "    if state.get(\"use_reranker\", False):\n",
        "        logger.info(\"---DECISION: RERANKING REQUIRED---\")\n",
        "        return \"rerank\"\n",
        "    else:\n",
        "        logger.info(\"---DECISION: SKIP RERANKING---\")\n",
        "        # This is the node to go to if reranking is skipped\n",
        "        return \"skip_rerank\"\n",
        "\n",
        "def should_compress(state: KBState):\n",
        "    \"\"\"Decide whether to proceed to compression or skip.\"\"\"\n",
        "    # Check if 'use_compressor' is in the state and is True\n",
        "    if state.get(\"use_compressor\", False):\n",
        "        logger.info(\"---DECISION: COMPRESSION REQUIRED---\")\n",
        "        return \"compress\"\n",
        "    else:\n",
        "        logger.info(\"---DECISION: SKIP COMPRESSION---\")\n",
        "        # This is the node to go to if compression is skipped\n",
        "        return \"skip_compress\"\n",
        "\n",
        "\n",
        "async def build_rag_graph(self):\n",
        "    # --- Build the graph ---\n",
        "    # Define the state machine\n",
        "    builder = StateGraph(KBState)\n",
        "\n",
        "    # Add nodes\n",
        "    builder.add_node(\"kb_retrieve\", self.kb_retrieve_node)\n",
        "    builder.add_node(\"kb_grade\", self.kb_grade_node)\n",
        "    builder.add_node(\"kb_rewrite\", self.kb_rewrite_node)\n",
        "    builder.add_node(\"kb_rerank\", self.kb_rerank_node)\n",
        "    builder.add_node(\"kb_compress\", self.kb_compress_node)\n",
        "    builder.add_node(\"kb_assess_and_reflect\", self.kb_assess_and_reflect_node)\n",
        "    builder.add_node(\"kb_update_iteration\", self.kb_update_iteration_node)\n",
        "    builder.add_node(\"kb_generate\", self.kb_generate_answer_node)\n",
        "\n",
        "    # --- Define the entry point ---\n",
        "    # Correctly set the entry point to the name of the first node\n",
        "    builder.set_entry_point(\"retrieve\")\n",
        "\n",
        "    # --- Define edges ---\n",
        "    # The edge from START to retrieve is implicitly handled by set_entry_point(\"retrieve\")\n",
        "\n",
        "    # Conditional edge from retrieve node\n",
        "    builder.add_conditional_edges(\n",
        "        \"retrieve\", # Source node\n",
        "        should_rerank, # Decision function\n",
        "        {\n",
        "            \"rerank\": \"rerank\",     # If should_rerank returns \"rerank\", go to \"rerank\" node\n",
        "            \"skip_rerank\": \"generate\" # If should_rerank returns \"skip_rerank\", go to \"generate\" node\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Conditional edge from rerank node\n",
        "    builder.add_conditional_edges(\n",
        "        \"rerank\", # Source node\n",
        "        should_compress, # Decision function\n",
        "        {\n",
        "            \"compress\": \"compress\",     # If should_compress returns \"compress\", go to \"compress\" node\n",
        "            \"skip_compress\": \"generate\" # If should_compress returns \"skip_compress\", go to \"generate\" node\n",
        "        }\n",
        "    )\n",
        "    # Define routing logic\n",
        "    def route_after_lookup(state: KBState) -> str:\n",
        "        if state.error_message:\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.web_search_required:\n",
        "            return \"kb_web_search\"\n",
        "        else:\n",
        "            return \"kb_grade\"\n",
        "\n",
        "    def route_after_grade(state: KBState) -> str:\n",
        "        if state.error_message:\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.web_search_required:\n",
        "            return \"kb_rewrite\"\n",
        "        else:\n",
        "            return \"kb_rerank\"\n",
        "\n",
        "    def route_after_search(state: KBState) -> str:\n",
        "        if state.error_message:\n",
        "            return \"kb_generate_answer\"\n",
        "        else:\n",
        "            return \"kb_update_iteration\"\n",
        "\n",
        "    def route_after_iteration(state: KBState) -> str:\n",
        "        if state.iterations >= state.max_iterations:\n",
        "            return \"kb_rerank\"\n",
        "        else:\n",
        "            return \"kb_assess_and_reflect\"\n",
        "\n",
        "    def route_after_reflection(state: KBState) -> str:\n",
        "        if state.error_message:\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.reflection_decision == \"refine\":\n",
        "            return \"kb_rewrite\"\n",
        "        else:\n",
        "            return \"kb_rerank\"\n",
        "\n",
        "    # Add edges\n",
        "    builder.add_conditional_edges(\"kb_lookup\", route_after_lookup)\n",
        "    builder.add_conditional_edges(\"kb_grade\", route_after_grade)\n",
        "    builder.add_edge(\"kb_rewrite\", \"kb_web_search\")\n",
        "    builder.add_conditional_edges(\"kb_web_search\", route_after_search)\n",
        "    builder.add_conditional_edges(\"kb_update_iteration\", route_after_iteration)\n",
        "    builder.add_conditional_edges(\"kb_assess_and_reflect\", route_after_reflection)\n",
        "    builder.add_edge(\"kb_rerank\", \"kb_generate_answer\")\n",
        "    builder.add_edge(\"kb_generate_answer\", END)\n",
        "\n",
        "    # Edge from compress to generate (if executed)\n",
        "    builder.add_edge(\"compress\", \"generate\")\n",
        "\n",
        "    # Edge from generate to end\n",
        "    builder.add_edge(\"generate\", END)\n",
        "\n",
        "    # Compile the graph\n",
        "    app = builder.compile()\n",
        "\n",
        "    # --- Example invocation with reranking and compression ---\n",
        "    print(\"--- INVOKING GRAPH WITH RERANKING AND COMPRESSION ---\")\n",
        "    inputs = {\n",
        "        \"query\": \"What effect did the plasma fire have on the silver veins?\", # Use a query relevant to the documents\n",
        "        \"use_reranker\": True,\n",
        "        \"use_compressor\": True,\n",
        "        \"llm\": self.llm,\n",
        "        \"retriever\": self.retriever, # Pass the retriever instance\n",
        "        \"reranker_tool\": self.reranker_tool, # Pass the reranker tool instance\n",
        "        \"compressor_tool\": self.compressor_tool # Pass the compressor tool instance\n",
        "    }\n",
        "    # The ainvoke method returns the final state of the graph\n",
        "    result = await app.ainvoke(inputs)\n",
        "    print(\"\\nFinal Answer (with rerank & compress):\", result.get('final_answer', 'N/A')) # Use .get to safely access\n",
        "\n",
        "    # --- Example invocation with only reranking ---\n",
        "    print(\"\\n--- INVOKING GRAPH WITH ONLY RERANKING ---\")\n",
        "    inputs = {\n",
        "        \"query\": \"What was the message from Titan?\", # Use a query relevant to the documents\n",
        "        \"use_reranker\": True,\n",
        "        \"use_compressor\": False,\n",
        "        \"llm\": self.llm, # Corrected typo\n",
        "        \"retriever\": self.retriever,\n",
        "        \"reranker_tool\": self.reranker_tool,\n",
        "        \"compressor_tool\": self.compressor_tool # Still need to pass the tool even if not used due to KBState definition\n",
        "    }\n",
        "    result = await app.ainvoke(inputs)\n",
        "    print(\"\\nFinal Answer (with rerank):\", result.get('final_answer', 'N/A'))\n",
        "\n",
        "    # --- Example invocation with no reranking or compression ---\n",
        "    print(\"\\n--- INVOKING GRAPH WITH NO RERANKING OR COMPRESSION ---\")\n",
        "    inputs = {\n",
        "        \"query\": \"Who was Captain Liora Vega?\", # Use a query relevant to the documents\n",
        "        \"use_reranker\": False,\n",
        "        \"use_compressor\": False,\n",
        "        \"llm\": self.llm,\n",
        "        \"retriever\": self.retriever,\n",
        "        \"reranker_tool\": self.reranker_tool, # Still need to pass the tool\n",
        "        \"compressor_tool\": self.compressor_tool # Still need to pass the tool\n",
        "    }\n",
        "    result = await app.ainvoke(inputs)\n",
        "    print(\"\\nFinal Answer (no rerank or compress):\", result.get('final_answer', 'N/A'))\n",
        "\n",
        "\n",
        "# Run the test function\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(build_rag_graph())"
      ],
      "metadata": {
        "id": "lI_wH8hWw3jb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "dc95c4e6-479d-489d-ab12-b776f25271e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BaseMessage' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3818157581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# Pydantic State Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKBState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     \u001b[0mRepresents\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mof\u001b[0m \u001b[0mour\u001b[0m \u001b[0mcomprehensive\u001b[0m \u001b[0mknowledge\u001b[0m \u001b[0mbase\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3818157581.py\u001b[0m in \u001b[0;36mKBState\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mcrawl_status\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"miss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not_attempted\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"not_attempted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Web crawl status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mcrawl_results\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLangchainDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Web crawl results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Conversation messages\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# Reflection state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BaseMessage' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CORRECTED KNOWLEDGE BASE AGENT NODES\n",
        "# =============================================================================\n",
        "\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "import hashlib\n",
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "from enum import Enum\n",
        "from datetime import datetime, timezone\n",
        "from pydantic import BaseModel, Field, model_validator, field_validator\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, RetryError\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_core.callbacks import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n",
        "# Ensure CompressorTool and RerankerTool are defined or imported\n",
        "# Assuming CompressorTool and RerankerTool from previous cells are available\n",
        "# from .compressor_tool import CompressorTool # Example if in a different file\n",
        "# from .reranker_tool import RerankerTool # Example if in a different file\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# =============================================================================\n",
        "# STATE SCHEMAS - Corrected to use proper LangGraph patterns\n",
        "# =============================================================================\n",
        "\n",
        "class KBState(TypedDict):\n",
        "    \"\"\"Knowledge Base State - Updated to match latest LangGraph patterns\"\"\"\n",
        "    query: str # The user's original query\n",
        "    messages: Annotated[List[BaseMessage], add_messages] # Conversation history\n",
        "    session_id: str # Unique session identifier\n",
        "    retrieved_documents: List[LangchainDocument] # Documents from vector store\n",
        "    kb_lookup_status: Literal[\"hit\", \"miss\", \"not_attempted\"] # Status of KB lookup\n",
        "    web_search_required: bool # Whether web search is needed\n",
        "    escalation_required: bool # Whether escalation to main agent is needed\n",
        "    iterations: int # Current iteration count for web search\n",
        "    max_iterations: int # Maximum allowed web search iterations\n",
        "    retrieval_grade: Optional[dict] # Result of the retrieval grading (can use dict or Pydantic model if defined)\n",
        "    subqueries: List[str] # Generated subqueries for web search\n",
        "    crawl_results: List[LangchainDocument] # Documents from web crawling\n",
        "    reflection_output: Optional[dict] # Result of the reflection step (can use dict or Pydantic model)\n",
        "    reflection_decision: Literal[\"refine\", \"sufficient\", \"error\"] # Decision from reflection\n",
        "    error_message: Optional[str] # Any error message\n",
        "    final_answer: Optional[str] # The generated final answer\n",
        "    use_reranker: bool # Flag to use reranking\n",
        "    use_compressor: bool # Flag to use compression\n",
        "    rerank_result: Optional[List[LangchainDocument]] # Documents after reranking\n",
        "    compress_result: Optional[List[str]] # Compressed text snippets\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PYDANTIC MODELS - Updated imports and structure (Moved from lI_wH8hWw3jb for clarity)\n",
        "# =============================================================================\n",
        "\n",
        "# Assuming these were defined in a previous cell or imported\n",
        "# from <your_module> import RetrievalGrade, SubqueriesOutput, ReflectionOutput, DocumentResult, VectorStoreResponse, ExtractedURL, SearchResultSchema, DocumentSource, ContentSource, VectorStoreConfig, VectorStoreManager, RerankerTool, CompressorTool\n",
        "\n",
        "# If not imported, define them here or ensure they are in a preceding cell\n",
        "class RetrievalGrade(BaseModel):\n",
        "    \"\"\"Grade for retrieval relevance\"\"\"\n",
        "    score: int = Field(..., description=\"Binary relevance: 1 = relevant, 0 = not relevant.\")\n",
        "    verdict: str = Field(..., description=\"'yes' or 'no' (mirror of score).\")\n",
        "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0.00-1.00).\")\n",
        "    rationale: str = Field(..., description=\"Step-by-step rationale (max ~120 words).\")\n",
        "    highlights: Optional[List[str]] = Field(None, description=\"Up to 3 short excerpts (≤30 words each).\")\n",
        "    refined_query: Optional[str] = Field(None, description=\"Refined query if score == 0 or low confidence.\")\n",
        "\n",
        "class SubqueriesOutput(BaseModel):\n",
        "    \"\"\"Output schema for subquery generation\"\"\"\n",
        "    queries: List[str] = Field(description=\"A list of distinct, optimized search queries to perform for gathering additional information.\")\n",
        "    justification: Optional[str] = Field(description=\"A brief explanation of why these specific subqueries were generated.\")\n",
        "\n",
        "class ReflectionOutput(BaseModel):\n",
        "    \"\"\"Output schema for reflection\"\"\"\n",
        "    decision: Literal[\"refine\", \"sufficient\"] = Field(description=\"Decision to 'refine' or proceed.\")\n",
        "    assessment: str = Field(description=\"A textual assessment of the gathered documents.\")\n",
        "    confidence: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"Confidence in the assessment.\")\n",
        "\n",
        "# Re-define other necessary Pydantic models if not imported\n",
        "class DocumentSource(str, Enum):\n",
        "    \"\"\"Source types for a document.\"\"\"\n",
        "    KNOWLEDGE_BASE = \"knowledge_base\"\n",
        "    EXTERNAL_API = \"external_api\"\n",
        "    WEB_CRAWL = \"web_crawl\"\n",
        "    SEARCH_SNIPPET = \"search_snippet\"\n",
        "    CACHE = \"cache\"\n",
        "\n",
        "class ContentSource(str, Enum):\n",
        "    \"\"\"Semantic categories assigned by LLM (e.g., academic, blog, forum).\"\"\"\n",
        "    ACADEMIC = \"academic\"\n",
        "    GOVERNMENT = \"government\"\n",
        "    WIKI = \"wikipedia\"\n",
        "    GIT_REPO = \"git_repo\"\n",
        "    NEWS = \"news\"\n",
        "    BLOG = \"blog\"\n",
        "    REPORT = \"report\"\n",
        "    FORUM = \"forum\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "class DocumentResult(BaseModel):\n",
        "    \"\"\"Standardized document result with metadata.\"\"\"\n",
        "    page_content: str = Field(..., description=\"The document content.\")\n",
        "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Metadata associated with the document.\")\n",
        "    content_length: Optional[int] = Field(default=0, ge=0, description=\"Content length in characters.\")\n",
        "    source_type: DocumentSource = Field(default=DocumentSource.SEARCH_SNIPPET, description=\"The origin of the document content.\")\n",
        "    content_source: Optional[ContentSource] = Field(default=ContentSource.OTHER, description=\"The semantic category of the content.\")\n",
        "    relevance_score: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"A score indicating relevance.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    document_id: Optional[str] = Field(default=None, description=\"Stable unique identifier for the document.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def compute_content_length(self) -> \"DocumentResult\":\n",
        "        \"\"\"Ensures content_length is computed from page_content if missing/zero.\"\"\"\n",
        "        if self.page_content and (not self.content_length or self.content_length == 0):\n",
        "            object.__setattr__(self, \"content_length\", len(self.page_content))\n",
        "\n",
        "        #  Always ensure document_id is stable (sha256 of content)\n",
        "        if not self.document_id:\n",
        "            did = hashlib.sha256(self.page_content.encode(\"utf-8\")).hexdigest()\n",
        "            object.__setattr__(self, \"document_id\", did)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def to_langchain(self):\n",
        "        \"\"\"Convert to LangChain Document.\"\"\"\n",
        "        from langchain_core.documents import Document\n",
        "        return Document(page_content=self.page_content, metadata=self.metadata, id=self.document_id)\n",
        "\n",
        "    @classmethod\n",
        "    def from_langchain(cls, doc: LangchainDocument, relevance_score: Optional[float] = None) -> \"DocumentResult\":\n",
        "        \"\"\"Build from a LangChain Document (reverse of to_langchain).\"\"\"\n",
        "        id = doc.id or hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest()\n",
        "        timestamp = doc.metadata.get(\"created_at\") or datetime.now(timezone.utc)\n",
        "        return cls(\n",
        "            page_content=doc.page_content,\n",
        "            metadata=doc.metadata,\n",
        "            relevance_score=relevance_score,\n",
        "            created_at=timestamp,\n",
        "            document_id=id,\n",
        "        )\n",
        "\n",
        "class VectorStoreResponse(BaseModel):\n",
        "    \"\"\"Unified response schema for vector store operations.\"\"\"\n",
        "\n",
        "    query: Optional[str] = None  # retrieval query\n",
        "    results: Optional[List[\"DocumentResult\"]] = None  # retrieval results\n",
        "    status: str = Field(..., description=\"Operation status: success or failed.\")\n",
        "    message: str = Field(..., description=\"Human-readable summary of the result.\")\n",
        "    document_ids: List[str] = Field(default_factory=list, description=\"List of indexed document IDs.\")\n",
        "    indexed_documents: List[\"DocumentResult\"] = Field(default_factory=list, description=\"The full indexed documents.\")\n",
        "    error: Optional[str] = Field(default=None, description=\"Error message if any.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def validate_exclusive_fields(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure that document_ids/indexed_documents and results are mutually exclusive.\"\"\"\n",
        "        if self.results and (self.document_ids or self.indexed_documents):\n",
        "            raise ValueError(\"Response cannot contain both retrieval results and indexing results.\")\n",
        "        return self\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def sync_document_ids(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure document_ids matches indexed_documents' IDs if provided.\"\"\"\n",
        "        if self.indexed_documents:\n",
        "            ids = [doc.document_id for doc in self.indexed_documents if doc.document_id]\n",
        "            object.__setattr__(self, \"document_ids\", ids)\n",
        "        return self\n",
        "\n",
        "class VectorStoreConfig(BaseModel):\n",
        "    chroma_persist_directory: str = Field(default=\"./chroma_db\", description=\"The directory where ChromaDB data will be persisted.\")\n",
        "    collection_name: str = Field(default=\"knowledge_agent_collection\", description=\"The name of the ChromaDB collection.\")\n",
        "    embedding_model_provider: str = Field(default=\"sentence-transformer\", description=\"The provider for the embedding model.\") # Use str instead of Literal for flexibility\n",
        "    embedding_model_name: str = Field(default=\"all-MiniLM-L6-v2\", description=\"The name of the embedding model to use.\")\n",
        "    relevance_threshold: float = Field(default=0.8, description=\"The similarity threshold for filtering relevant documents.\")\n",
        "\n",
        "class VectorStoreManager:\n",
        "    \"\"\"\n",
        "    Manages the lifecycle and tool creation for the vector store.\n",
        "    Encapsulates the setup, indexing, and retrieval logic.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)  # class-level logger\n",
        "\n",
        "    def __init__(self, config: VectorStoreConfig, initial_documents: Optional[List[LangchainDocument]] = None):\n",
        "        self.config = config\n",
        "        self.vectorstore = self._setup_vector_store()\n",
        "        if initial_documents:\n",
        "            # Index initial documents upon creation\n",
        "            self._index_initial_documents(initial_documents)\n",
        "\n",
        "        if not self.logger.hasHandlers():\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def _get_embedding_model(self, cfg: VectorStoreConfig):\n",
        "        # Assuming SentenceTransformerEmbeddings and HuggingFaceEmbeddings are available\n",
        "        if cfg.embedding_model_provider == \"huggingface\":\n",
        "            return HuggingFaceEmbeddings(model_name=cfg.embedding_model_name)\n",
        "        elif cfg.embedding_model_provider == \"sentence-transformer\":\n",
        "            return SentenceTransformerEmbeddings(model_name=cfg.embedding_model_name)\n",
        "        else:\n",
        "             logger.warning(f\"Unknown embedding provider: {cfg.embedding_model_provider}. Falling back to sentence-transformer.\")\n",
        "             return SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Fallback\n",
        "\n",
        "    @staticmethod\n",
        "    def _prefixed_sha256(x: Union[str, bytes]) -> bytes:\n",
        "        prefix = b\"kb_vectorstore:\"\n",
        "        if isinstance(x, str):\n",
        "            x = x.encode(\"utf-8\")\n",
        "        return hashlib.sha256(prefix + x).digest()\n",
        "\n",
        "    def _setup_vector_store(self) -> Chroma:\n",
        "        try:\n",
        "            embeddings = self._get_embedding_model(self.config)\n",
        "            cache_store = InMemoryStore()\n",
        "            cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
        "                underlying_embeddings=embeddings,\n",
        "                document_embedding_cache=cache_store,\n",
        "                query_embedding_cache=cache_store,\n",
        "                key_encoder=self._prefixed_sha256,\n",
        "            )\n",
        "            # Assuming os is imported\n",
        "            import os\n",
        "            os.makedirs(self.config.chroma_persist_directory, exist_ok=True)\n",
        "            vectorstore = Chroma(\n",
        "                collection_name=self.config.collection_name,\n",
        "                embedding_function=cached_embeddings,\n",
        "                persist_directory=self.config.chroma_persist_directory,\n",
        "            )\n",
        "            self.logger.info(f\"Chroma initialized and ready at {self.config.chroma_persist_directory}\")\n",
        "            return vectorstore\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to initialize vector store: {e}\") from e\n",
        "\n",
        "    def _index_initial_documents(self, documents: List[LangchainDocument]):\n",
        "        \"\"\"Helper to index initial documents synchronously.\"\"\"\n",
        "        docs_to_add = [DocumentResult.from_langchain(doc) for doc in documents]\n",
        "        # Assuming the sync index_documents method is implemented correctly\n",
        "        self.index_documents(docs_to_add)\n",
        "\n",
        "    # Assuming index_documents and retrieve_documents methods are implemented as in the previous cell\n",
        "\n",
        "    def as_index_tool(self) -> StructuredTool:\n",
        "        # This method is likely not used in the current graph, but keep for completeness\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.index_documents,\n",
        "            # coroutine=self.aindex_documents, # Ensure aindex_documents is implemented\n",
        "            name=\"vectorstore_index\",\n",
        "            description=\"Indexes a list of documents into the vector store.\"\n",
        "        )\n",
        "\n",
        "    def as_retrieve_tool(self) -> StructuredTool:\n",
        "        # This is also likely not used directly in the graph as the retriever instance is passed\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.retrieve_documents,\n",
        "            # coroutine=self.aretrieve_documents, # Ensure aretrieve_documents is implemented\n",
        "            name=\"vectorstore_retrieve\",\n",
        "            description=\"Retrieves documents from the vector store based on semantic similarity.\"\n",
        "        )\n",
        "\n",
        "    # Add placeholder sync/async index and retrieve methods if they were not in the previous cell\n",
        "    def index_documents(self, documents: List[DocumentResult]) -> VectorStoreResponse:\n",
        "        \"\"\"Synchronous method to index documents (Placeholder).\"\"\"\n",
        "        logger.info(f\"Indexing {len(documents)} documents (simulated).\")\n",
        "        # Simulate indexing\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Indexed {len(documents)} documents (simulated).\",\n",
        "            indexed_documents=documents,\n",
        "        )\n",
        "\n",
        "    async def aindex_documents(self, documents: List[DocumentResult]) -> VectorStoreResponse:\n",
        "         \"\"\"Asynchronous method to index documents (Placeholder).\"\"\"\n",
        "         logger.info(f\"Indexing {len(documents)} documents asynchronously (simulated).\")\n",
        "         await asyncio.sleep(0.1) # Simulate async work\n",
        "         return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Indexed {len(documents)} documents asynchronously (simulated).\",\n",
        "            indexed_documents=documents,\n",
        "        )\n",
        "\n",
        "    def retrieve_documents(self, query: str, top_k: int = 5) -> VectorStoreResponse:\n",
        "        \"\"\"Synchronous method to retrieve documents (Placeholder).\"\"\"\n",
        "        logger.info(f\"Retrieving documents for query: '{query}' (simulated).\")\n",
        "        # Simulate retrieval - return first few initial documents\n",
        "        simulated_results = [DocumentResult.from_langchain(doc) for doc in self.vectorstore.as_retriever(search_kwargs={\"k\": top_k}).invoke(query)]\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Retrieved {len(simulated_results)} documents (simulated).\",\n",
        "            query=query,\n",
        "            results=simulated_results\n",
        "        )\n",
        "\n",
        "    async def aretrieve_documents(self, query: str, top_k: int = 5) -> VectorStoreResponse:\n",
        "        \"\"\"Asynchronous method to retrieve documents (Placeholder).\"\"\"\n",
        "        logger.info(f\"Retrieving documents asynchronously for query: '{query}' (simulated).\")\n",
        "        await asyncio.sleep(0.1) # Simulate async work\n",
        "        # Simulate async retrieval\n",
        "        simulated_results = [DocumentResult.from_langchain(doc) for doc in await self.vectorstore.as_retriever(search_kwargs={\"k\": top_k}).ainvoke(query)]\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Retrieved {len(simulated_results)} documents asynchronously (simulated).\",\n",
        "            query=query,\n",
        "            results=simulated_results\n",
        "        )\n",
        "\n",
        "# Assuming RerankerTool and CompressorTool classes are defined as in previous successful cells\n",
        "\n",
        "class RerankerTool:\n",
        "    \"\"\"\n",
        "    A class to encapsulate the reranking tool and its dependencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model: Any, vectorstore: Any, top_n=10, k: int = 20):\n",
        "        self.pipeline = self._setup_reranker_pipeline(embedding_model, vectorstore, top_n, k)\n",
        "        if \"reranker\" not in self.pipeline:\n",
        "             # Fallback if reranker setup fails\n",
        "             logger.warning(\"Reranker not found in pipeline. Using base retriever.\")\n",
        "             self.reranker = None # Indicate reranker is not available\n",
        "             self._base_retriever = self.pipeline # Store the base retriever\n",
        "        else:\n",
        "            self.reranker = self.pipeline[\"reranker\"]\n",
        "            self._base_retriever = self.pipeline[\"pipeline\"] # Store the full compression retriever\n",
        "\n",
        "\n",
        "    def _setup_reranker_pipeline(self, embedding_model, vectorstore, top_n: int = 10, k: int = 20):\n",
        "        \"\"\"\n",
        "        Setup a retriever pipeline that retrieves documents via a vectorstore\n",
        "        and reranks them with a cross-encoder.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "            # Load the cross-encoder model\n",
        "            # Ensure HuggingFaceCrossEncoder is imported\n",
        "            hf_model = HuggingFaceCrossEncoder(\n",
        "                model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "                model_kwargs={\"device\": \"cpu\"}  # or \"cuda\" if available\n",
        "            )\n",
        "            reranker = CrossEncoderReranker(\n",
        "                model=hf_model,\n",
        "                top_n=top_n,\n",
        "            )\n",
        "            pipeline = ContextualCompressionRetriever(\n",
        "                base_compressor=reranker,\n",
        "                base_retriever=base_retriever,\n",
        "            )\n",
        "            return {\"pipeline\": pipeline, \"reranker\": reranker}\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to setup reranker pipeline: {e}\")\n",
        "            # Return just the base retriever as a fallback\n",
        "            return vectorstore.as_retriever(search_kwargs={\"k\": top_n})\n",
        "\n",
        "    # Corrected Input Schema for StructuredTool\n",
        "    class RerankInput(BaseModel):\n",
        "        query: str = Field(..., description=\"The query for reranking.\")\n",
        "        documents: List[LangchainDocument] = Field(..., description=\"The documents to rerank.\")\n",
        "        top_n: int = Field(5, description=\"Number of top documents to return.\")\n",
        "\n",
        "\n",
        "    async def arerank_documents(self, query: str, documents: List[LangchainDocument], top_n: int = 5) -> List[LangchainDocument]:\n",
        "        \"\"\"Async version to be wrapped by StructuredTool.\"\"\"\n",
        "        if not self.reranker:\n",
        "            logger.warning(\"Reranker not available. Returning original documents.\")\n",
        "            return documents[:top_n]\n",
        "\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Use the underlying compressor directly for flexibility or use ainvoke on _base_retriever\n",
        "            # If using _base_retriever (ContextualCompressionRetriever), ainvoke handles retrieval + compression\n",
        "            # But the tool receives pre-retrieved documents. So, use the compressor directly.\n",
        "            reranked_docs = await self.reranker.acompress_documents(\n",
        "                documents=documents,\n",
        "                query=query\n",
        "            )\n",
        "            return reranked_docs[:top_n]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reranker tool failed: {e}\", exc_info=True)\n",
        "            return documents[:top_n] # Return original top_n on failure\n",
        "\n",
        "    def rerank_documents(self, query: str, documents: List[LangchainDocument], top_n: int = 5) -> List[LangchainDocument]:\n",
        "        \"\"\"Sync version to be wrapped by StructuredTool.\"\"\"\n",
        "        if not self.reranker:\n",
        "            logger.warning(\"Reranker not available. Returning original documents.\")\n",
        "            return documents[:top_n]\n",
        "\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            reranked_docs = self.reranker.compress_documents(\n",
        "                documents=documents,\n",
        "                query=query\n",
        "            )\n",
        "            return reranked_docs[:top_n]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reranker tool failed: {e}\", exc_info=True)\n",
        "            return documents[:top_n] # Return original top_n on failure\n",
        "\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        \"\"\"Expose reranking functionality as a StructuredTool.\"\"\"\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.rerank_documents,\n",
        "            coroutine=self.arerank_documents,\n",
        "            name=\"rerank_documents\",\n",
        "            description=\"Reranks a list of documents based on relevance to a query.\",\n",
        "            args_schema=self.RerankInput,\n",
        "            # return_direct=True # Usually False for intermediate tools\n",
        "        )\n",
        "\n",
        "class CompressorTool:\n",
        "    \"\"\"\n",
        "    A class to encapsulate the compressor tool using LLMChainExtractor.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: Any):\n",
        "        self.compressor: BaseDocumentCompressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "    # Corrected Input Schema for StructuredTool\n",
        "    class CompressorInput(BaseModel):\n",
        "        query: str = Field(..., description=\"The user's query for compression.\")\n",
        "        documents: List[LangchainDocument] = Field(..., description=\"The documents to compress.\")\n",
        "\n",
        "    def complex_compress_documents(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[LangchainDocument],\n",
        "    ) -> List[LangchainDocument]:\n",
        "        \"\"\"Synchronous method to be wrapped by StructuredTool.\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "        try:\n",
        "            run_manager = CallbackManagerForRetrieverRun.get_noop_manager()\n",
        "            compressed_docs = self.compressor.compress_documents(\n",
        "                documents=documents,\n",
        "                query=query,\n",
        "                callbacks=run_manager.get_child(),\n",
        "            )\n",
        "            return compressed_docs\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Compressor tool failed (sync): {e}\", exc_info=True)\n",
        "            return [] # Return empty list on failure\n",
        "\n",
        "    async def acomplex_compress_documents(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[LangchainDocument],\n",
        "    ) -> List[LangchainDocument]:\n",
        "        \"\"\"Asynchronous method to be wrapped by StructuredTool.\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "        try:\n",
        "            run_manager = AsyncCallbackManagerForRetrieverRun.get_noop_manager()\n",
        "            compressed_docs = await self.compressor.acompress_documents(\n",
        "                documents=documents,\n",
        "                query=query,\n",
        "                callbacks=run_manager.get_child(),\n",
        "            )\n",
        "            return compressed_docs\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Compressor tool failed (async): {e}\", exc_info=True)\n",
        "            return [] # Return empty list on failure\n",
        "\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        \"\"\"Expose compression functionality as a StructuredTool.\"\"\"\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.complex_compress_documents,\n",
        "            coroutine=self.acomplex_compress_documents,\n",
        "            name=\"llm_compressor\",\n",
        "            description=\"Extracts relevant parts from documents using an LLM.\",\n",
        "            args_schema=self.CompressorInput,\n",
        "            # return_direct=True # Usually False for intermediate tools\n",
        "        )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# KNOWLEDGE BASE AGENT CLASS - Corrected Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class KnowledgeBaseAgent:\n",
        "    \"\"\"Knowledge Base Agent with all node implementations.\"\"\"\n",
        "\n",
        "    def __init__(self, documents_list: List[LangchainDocument], cache_adapter=None):\n",
        "        self.documents_list = documents_list\n",
        "        self.cache_adapter = cache_adapter\n",
        "        print(self.documents_list)\n",
        "        # Initialize components properly\n",
        "        self.setup_components()\n",
        "\n",
        "    def setup_components(self):\n",
        "        \"\"\"Setup all required components as instance attributes.\"\"\"\n",
        "        # --- Initialize components ---\n",
        "        self.config = VectorStoreConfig(\n",
        "            embedding_model_provider=\"sentence-transformer\",\n",
        "            embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "            chroma_persist_directory=\"./chroma_test\",\n",
        "            collection_name=\"test_collection\",\n",
        "        )\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = init_chat_model(\n",
        "            \"gemini-2.5-flash\",\n",
        "            model_provider=\"google-genai\",\n",
        "            temperature=0.2,\n",
        "            # Add rate limiter if needed: rate_limiter=InMemoryRateLimiter(requests_per_second=0.1),\n",
        "        )\n",
        "\n",
        "        # Setup grader, query rewriter, and reflection tool using the instance llm\n",
        "        self.grader = self.setup_grader(self.llm)\n",
        "        self.query_rewriter = self.setup_query_rewriter(self.llm)\n",
        "        self.reflection_tool = self.setup_reflection_tool(self.llm)\n",
        "\n",
        "        # Initialize Vector Store Manager and Retriever using instance config and documents\n",
        "        vectorstore_manager = VectorStoreManager(self.config, initial_documents=self.documents_list)\n",
        "\n",
        "        # Get the retriever instance from the manager\n",
        "        self.retriever = vectorstore_manager.vectorstore.as_retriever(search_kwargs={\"k\": 20}) # Retrieve enough for reranking\n",
        "\n",
        "        # Initialize Reranker Tool using instance embedding model and vectorstore\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Initialize embedding model as instance attribute\n",
        "        reranker_instance = RerankerTool(\n",
        "            embedding_model=self.embedding_model, # Use instance attribute\n",
        "            vectorstore=vectorstore_manager.vectorstore, # Use instance vectorstore from manager\n",
        "            top_n=5 # Rerank to top 5\n",
        "        )\n",
        "        self.reranker_tool = reranker_instance.as_tool() # Get the StructuredTool\n",
        "\n",
        "        # Initialize Compressor Tool using instance llm\n",
        "        compressor_instance = CompressorTool(llm=self.llm) # Use instance attribute\n",
        "        self.compressor_tool = compressor_instance.as_tool() # Get the StructuredTool\n",
        "\n",
        "\n",
        "    def setup_llm(\n",
        "        self,\n",
        "        model: str = \"gemini-2.0-flash\", # Corrected default model name\n",
        "        provider: str = \"google-genai\", # Corrected provider string\n",
        "        temperature: float = 0.2\n",
        "    ):\n",
        "        \"\"\"Initialize LLM with proper error handling.\"\"\"\n",
        "        logger.info(f\"------Setting up LLM: {model} ({provider})------\")\n",
        "        try:\n",
        "            # Assuming dotenv is imported and loaded if needed globally\n",
        "            # from dotenv import load_dotenv\n",
        "            # load_dotenv()\n",
        "            return init_chat_model(\n",
        "                model=model,\n",
        "                model_provider=provider,\n",
        "                temperature=temperature\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize LLM: {e}\")\n",
        "            # Decide on error handling: return None, raise, or handle differently\n",
        "            # For this example, let's raise the exception as the graph depends on the LLM\n",
        "            raise\n",
        "\n",
        "    def setup_grader(self, llm: Any):\n",
        "        \"\"\"Setup document relevance grader.\"\"\"\n",
        "        system_prompt = \"\"\"\n",
        "        You are an expert grader judging whether a retrieved DOCUMENT is sufficiently\n",
        "        relevant, correct, recent, and comprehensive for the USER QUESTION.\n",
        "\n",
        "        PRIORITIES (in order):\n",
        "        1. RELEVANCE: Does the document contain statements directly answering the question?\n",
        "        2. CORRECTNESS: Is the information internally consistent and plausible?\n",
        "        3. RECENCY: If time-sensitive, prefer newer/dated information?\n",
        "        4. COVERAGE: Does it cover core aspects or only peripheral points?\n",
        "\n",
        "        Scoring rules:\n",
        "        - score = 1 (verdict='yes') if document contains relevant statements\n",
        "        - score = 0 (verdict='no') if no relevant statements or contradictory facts\n",
        "\n",
        "        For partial coverage, prefer score=1 but lower confidence and explain gaps.\n",
        "        \"\"\"\n",
        "\n",
        "        # Corrected Input Schema for StructuredTool\n",
        "        class GradeInput(BaseModel):\n",
        "            question: str = Field(..., description=\"The user's question.\")\n",
        "            documents: List[LangchainDocument] = Field(..., description=\"The documents to grade.\")\n",
        "            context: str = Field(..., description=\"Additional context if available.\")\n",
        "\n",
        "        structured_llm_grader = llm.with_structured_output(RetrievalGrade)\n",
        "        grade_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"USER QUESTION:\\n{question}\\n\\nDOCUMENTS:\\n{documents}\\n\\nCONTEXT:\\n{context}\"),\n",
        "        ])\n",
        "\n",
        "        # Need to wrap this in a function that takes the GradeInput and calls the chain\n",
        "        def grade_documents_sync(question: str, documents: List[LangchainDocument], context: str = \"\") -> RetrievalGrade:\n",
        "            doc_contents = [doc.page_content for doc in documents]\n",
        "            return (grade_prompt | structured_llm_grader).invoke({\"question\": question, \"documents\": doc_contents, \"context\": context})\n",
        "\n",
        "        async def grade_documents_async(question: str, documents: List[LangchainDocument], context: str = \"\") -> RetrievalGrade:\n",
        "             doc_contents = [doc.page_content for doc in documents]\n",
        "             return await (grade_prompt | structured_llm_grader).ainvoke({\"question\": question, \"documents\": doc_contents, \"context\": context})\n",
        "\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "             func=grade_documents_sync,\n",
        "             coroutine=grade_documents_async,\n",
        "             name=\"document_grader\",\n",
        "             description=\"Grades the relevance of documents to a user question.\",\n",
        "             args_schema=GradeInput,\n",
        "        )\n",
        "\n",
        "\n",
        "    def setup_query_rewriter(self, llm: Any):\n",
        "        \"\"\"Setup query rewriter for generating subqueries.\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert search query generator. Your task is to analyze a user's question \"\n",
        "            \"and generate a list of distinct search queries that will help find more comprehensive \"\n",
        "            \"and relevant information. Generate only the most useful and focused queries, and keep the list concise.\"\n",
        "        )\n",
        "\n",
        "        structured_llm_rewriter = llm.with_structured_output(SubqueriesOutput)\n",
        "        rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"Question: {question}\"),\n",
        "        ])\n",
        "\n",
        "        # Wrap in a function for StructuredTool\n",
        "        def rewrite_query_sync(question: str) -> SubqueriesOutput:\n",
        "            return (rewrite_prompt | structured_llm_rewriter).invoke({\"question\": question})\n",
        "\n",
        "        async def rewrite_query_async(question: str) -> SubqueriesOutput:\n",
        "            return await (rewrite_prompt | structured_llm_rewriter).ainvoke({\"question\": question})\n",
        "\n",
        "        class RewriteInput(BaseModel):\n",
        "            question: str = Field(..., description=\"The question to rewrite.\")\n",
        "\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "            func=rewrite_query_sync,\n",
        "            coroutine=rewrite_query_async,\n",
        "            name=\"query_rewriter\",\n",
        "            description=\"Generates optimized search queries.\",\n",
        "            args_schema=RewriteInput,\n",
        "        )\n",
        "\n",
        "\n",
        "    def setup_reflection_tool(self, llm:Any):\n",
        "        \"\"\"Setup reflection tool for assessing document quality.\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert document assessor. Evaluate whether the gathered documents \"\n",
        "            \"provide sufficient information to answer the user's question comprehensively. \"\n",
        "            \"Consider relevance, completeness, and quality of information.\"\n",
        "            \"Return 'refine' if more searching is needed, or 'sufficient' if the results are adequate.\"\n",
        "        )\n",
        "\n",
        "        # Corrected Input Schema for StructuredTool\n",
        "        class ReflectionInput(BaseModel):\n",
        "            question: str = Field(..., description=\"The user's question.\")\n",
        "            documents: List[LangchainDocument] = Field(..., description=\"The documents to assess.\")\n",
        "\n",
        "\n",
        "        structured_llm_reflection = llm.with_structured_output(ReflectionOutput)\n",
        "        reflection_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"Question: {question}\\n\\nDocuments: {documents}\\n\\nAssess whether these documents are sufficient.\"),\n",
        "        ])\n",
        "\n",
        "        # Wrap in a function for StructuredTool\n",
        "        def reflect_on_documents_sync(question: str, documents: List[LangchainDocument]) -> ReflectionOutput:\n",
        "             doc_contents = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "             return (reflection_prompt | structured_llm_reflection).invoke({\"question\": question, \"documents\": doc_contents})\n",
        "\n",
        "        async def reflect_on_documents_async(question: str, documents: List[LangchainDocument]) -> ReflectionOutput:\n",
        "            doc_contents = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "            return await (reflection_prompt | structured_llm_reflection).ainvoke({\"question\": question, \"documents\": doc_contents})\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "            func=reflect_on_documents_sync,\n",
        "            coroutine=reflect_on_documents_async,\n",
        "            name=\"document_reflector\",\n",
        "            description=\"Assesses the quality of gathered documents.\",\n",
        "            args_schema=ReflectionInput,\n",
        "        )\n",
        "\n",
        "\n",
        "    # =============================================================================\n",
        "    # NODE IMPLEMENTATIONS - Corrected to return proper state updates and use instance attributes\n",
        "    # =============================================================================\n",
        "\n",
        "    async def kb_retrieve_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for initial document retrieval from the vector store.\"\"\"\n",
        "        logger.info(\"---EXECUTING RETRIEVAL NODE---\")\n",
        "\n",
        "        # Get the query from state\n",
        "        query = state.get(\"query\", \"\")\n",
        "        if not query and state.get(\"messages\"):\n",
        "            query = state[\"messages\"][-1].content\n",
        "\n",
        "        # Check if retriever was successfully initialized\n",
        "        if not hasattr(self, 'retriever') or self.retriever is None:\n",
        "             logger.error(\"Retriever not initialized.\")\n",
        "             return {\n",
        "                \"error_message\": \"Retriever not initialized.\",\n",
        "                \"kb_lookup_status\": \"miss\",\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True,\n",
        "                \"query\": query,\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            # Use the instance retriever\n",
        "            retrieved_documents: List[LangchainDocument] = await self.retriever.ainvoke(query)\n",
        "\n",
        "            # Filter documents\n",
        "            retrieved_documents = [\n",
        "                doc for doc in retrieved_documents\n",
        "                if isinstance(doc, LangchainDocument) and hasattr(doc, \"page_content\") and doc.page_content.strip()\n",
        "            ]\n",
        "\n",
        "            # Check if documents found\n",
        "            if not retrieved_documents:\n",
        "                logger.info(\"KB Agent: No relevant documents found in the vector store.\")\n",
        "                return {\n",
        "                    \"kb_lookup_status\": \"miss\",\n",
        "                    \"web_search_required\": True,\n",
        "                    \"escalation_required\": True,\n",
        "                    \"query\": query,\n",
        "                    \"retrieved_documents\": [] # Ensure documents list is empty\n",
        "                }\n",
        "\n",
        "            logger.info(f\"Retrieved {len(retrieved_documents)} documents from KB.\")\n",
        "            return {\n",
        "                \"kb_lookup_status\": \"hit\",\n",
        "                \"web_search_required\": False,\n",
        "                \"escalation_required\": False,\n",
        "                \"query\": query,\n",
        "                \"retrieved_documents\": retrieved_documents\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"KB lookup error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"KB lookup failed: {e}\",\n",
        "                \"kb_lookup_status\": \"miss\",\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True,\n",
        "                \"query\": query,\n",
        "                \"retrieved_documents\": [] # Ensure documents list is empty on error\n",
        "            }\n",
        "\n",
        "    async def kb_grade_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Grade the retrieved documents for relevance.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Evaluating Retrieval ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        documents = state.get(\"retrieved_documents\", [])\n",
        "\n",
        "        if not documents:\n",
        "            logger.warning(\"No documents to grade.\")\n",
        "            return {\n",
        "                \"retrieval_grade\": {\"score\": 0, \"confidence\": 0.0, \"verdict\": \"no\", \"rationale\": \"No documents to grade.\"}, # Return a default grade\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True, # Escalate if no docs were found\n",
        "            }\n",
        "\n",
        "        # Check if grader tool was successfully initialized\n",
        "        if not hasattr(self, 'grader') or self.grader is None:\n",
        "             logger.error(\"Grader tool not initialized.\")\n",
        "             return {\n",
        "                \"error_message\": \"Grader tool not initialized.\",\n",
        "                \"retrieval_grade\": {\"score\": 0, \"confidence\": 0.0, \"verdict\": \"no\", \"rationale\": \"Grader tool not initialized.\"},\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True,\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            # Use the instance grader tool with ainvoke\n",
        "            # Pass arguments as a dictionary matching the StructuredTool's args_schema\n",
        "            retrieval_grade_output: RetrievalGrade = await self.grader.ainvoke({\n",
        "                \"question\": query,\n",
        "                \"documents\": documents, # Pass LangchainDocument list\n",
        "                \"context\": \"\" # No additional context for this step\n",
        "            })\n",
        "\n",
        "            # Determine if web search is needed based on grade\n",
        "            # Assuming score=0 or low confidence indicates need for search\n",
        "            web_search_needed = (\n",
        "                retrieval_grade_output.score == 0 or\n",
        "                retrieval_grade_output.confidence < 0.7 or\n",
        "                retrieval_grade_output.verdict.lower() == \"no\"\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Retrieval Grade: Score={retrieval_grade_output.score}, Verdict='{retrieval_grade_output.verdict}', Confidence={retrieval_grade_output.confidence}\")\n",
        "\n",
        "            return {\n",
        "                \"retrieval_grade\": retrieval_grade_output.model_dump(), # Store Pydantic model as dictionary in state\n",
        "                \"web_search_required\": web_search_needed,\n",
        "                \"escalation_required\": web_search_needed # Escalate if initial retrieval was poor\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Grading error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"Grading failed: {e}\",\n",
        "                \"retrieval_grade\": {\"score\": 0, \"confidence\": 0.0, \"verdict\": \"error\", \"rationale\": f\"Grading failed: {e}\"}, # Indicate error in grade\n",
        "                \"web_search_required\": True, # Assume web search is needed on grading error\n",
        "                \"escalation_required\": True,\n",
        "            }\n",
        "\n",
        "    async def kb_update_iteration_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Update iteration counter.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Updating Iteration ---\")\n",
        "        current_iterations = state.get(\"iterations\", 0)\n",
        "        return {\"iterations\": current_iterations + 1}\n",
        "\n",
        "    async def kb_rewrite_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Rewrite query for better search results.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Rewriting Query ---\")\n",
        "\n",
        "        original_query = state.get(\"query\", \"\")\n",
        "\n",
        "        if not original_query:\n",
        "            logger.warning(\"No question found in state. Skipping query rewriting.\")\n",
        "            return {\n",
        "                \"subqueries\": [original_query if original_query else \"\"] # Ensure at least empty string if no query\n",
        "            }\n",
        "\n",
        "        # Check if query_rewriter tool was successfully initialized\n",
        "        if not hasattr(self, 'query_rewriter') or self.query_rewriter is None:\n",
        "             logger.error(\"Query rewriter tool not initialized.\")\n",
        "             return {\n",
        "                 \"error_message\": \"Query rewriter tool not initialized.\",\n",
        "                 \"subqueries\": [original_query] # Fallback to original query\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Rewriting query for: '{original_query}'\")\n",
        "            # Use the instance query_rewriter tool with ainvoke\n",
        "            subquery_output: SubqueriesOutput = await self.query_rewriter.ainvoke({\"question\": original_query})\n",
        "\n",
        "            subqueries = subquery_output.queries\n",
        "            # Ensure original query is included if subqueries list is empty\n",
        "            if not subqueries and original_query:\n",
        "                subqueries = [original_query]\n",
        "\n",
        "            logger.info(f\"Generated subqueries: {subqueries}\")\n",
        "\n",
        "            return {\n",
        "                \"subqueries\": subqueries\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Query rewriting error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"Query rewriting failed: {e}\",\n",
        "                \"subqueries\": [original_query] # Fallback to original query on error\n",
        "            }\n",
        "\n",
        "    # Placeholder for kb_web_search_node - Needs actual implementation\n",
        "    # This node would likely use external search tools (like Tavily, Serper, DDG)\n",
        "    # and potentially a web crawler to get page content.\n",
        "    async def kb_web_search_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for performing web search using generated subqueries.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Performing Web Search ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        subqueries = state.get(\"subqueries\", [])\n",
        "        # Add search tool usage here (e.g., Tavily, Serper, DDG)\n",
        "        # For now, returning empty crawl_results as a placeholder\n",
        "        logger.warning(\"Web search node is a placeholder and not implemented.\")\n",
        "        return {\n",
        "             \"crawl_results\": [],\n",
        "             \"web_search_required\": False, # Mark web search as done (or failed)\n",
        "             \"error_message\": state.get(\"error_message\") or \"Web search node is a placeholder.\"\n",
        "        }\n",
        "\n",
        "    async def kb_rerank_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for reranking retrieved and crawled documents.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Reranking Documents ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        # Combine documents from different sources\n",
        "        all_documents = state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", [])\n",
        "\n",
        "        if not all_documents:\n",
        "            logger.warning(\"No documents to rerank.\")\n",
        "            return {\"rerank_result\": []} # Return empty list if no documents\n",
        "\n",
        "        # Check if reranker tool was successfully initialized and if reranking is enabled in state\n",
        "        use_reranker = state.get(\"use_reranker\", True) # Default to True if not specified in state\n",
        "        if not hasattr(self, 'reranker_tool') or self.reranker_tool is None or not use_reranker:\n",
        "             logger.warning(\"Reranker tool not initialized or reranking disabled. Skipping reranking.\")\n",
        "             # If reranking is skipped, the 'documents' key should still hold the combined documents\n",
        "             # for the next node (compression or generation)\n",
        "             return {\"rerank_result\": all_documents} # Return original combined docs\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Reranking {len(all_documents)} documents.\")\n",
        "            # Use the instance reranker_tool with ainvoke\n",
        "            # The reranker_tool.ainvoke expects {\"query\": ..., \"documents\": ..., \"top_n\": ...}\n",
        "            reranked_docs: List[LangchainDocument] = await self.reranker_tool.ainvoke({\n",
        "                \"query\": query,\n",
        "                \"documents\": all_documents,\n",
        "                \"top_n\": 5 # Define top_n for reranking\n",
        "            })\n",
        "\n",
        "            logger.info(f\"Reranked to {len(reranked_docs)} documents.\")\n",
        "            return {\"rerank_result\": reranked_docs}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reranking error: {e}\")\n",
        "            # Return original combined documents on error\n",
        "            return {\n",
        "                \"error_message\": f\"Reranking failed: {e}\",\n",
        "                \"rerank_result\": all_documents\n",
        "            }\n",
        "\n",
        "\n",
        "    async def kb_compress_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for compressing the top documents using an LLM.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Compressing Documents ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        # Use the reranked documents if available, otherwise the combined documents\n",
        "        documents_to_compress = state.get(\"rerank_result\", []) or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", []))\n",
        "\n",
        "        if not documents_to_compress:\n",
        "            logger.warning(\"No documents to compress.\")\n",
        "            return {\"compress_result\": []} # Return empty list if no documents\n",
        "\n",
        "        # Check if compressor tool was successfully initialized and if compression is enabled in state\n",
        "        use_compressor = state.get(\"use_compressor\", True) # Default to True if not specified\n",
        "        if not hasattr(self, 'compressor_tool') or self.compressor_tool is None or not use_compressor:\n",
        "            logger.warning(\"Compressor tool not initialized or compression disabled. Skipping compression.\")\n",
        "            # If compression is skipped, the 'compress_result' key should hold the full text of the documents\n",
        "            # or this node could be skipped entirely via routing.\n",
        "            # For now, let's return the page_content of the documents as if they were \"compressed\" to their full text.\n",
        "            return {\"compress_result\": [doc.page_content for doc in documents_to_compress]}\n",
        "\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Compressing {len(documents_to_compress)} documents.\")\n",
        "            # Use the instance compressor_tool with ainvoke\n",
        "            # The compressor_tool.ainvoke expects {\"query\": ..., \"documents\": ...}\n",
        "            compressed_docs: List[LangchainDocument] = await self.compressor_tool.ainvoke({\n",
        "                \"query\": query,\n",
        "                \"documents\": documents_to_compress\n",
        "            })\n",
        "\n",
        "            # Extract page_content to match the GraphState definition for compress_result (List[str])\n",
        "            compressed_text_list = [doc.page_content for doc in compressed_docs]\n",
        "            logger.info(f\"Compressed to {len(compressed_text_list)} snippets.\")\n",
        "\n",
        "            return {\"compress_result\": compressed_text_list}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Compression error: {e}\")\n",
        "            # Return original document content on error\n",
        "            return {\n",
        "                \"error_message\": f\"Compression failed: {e}\",\n",
        "                \"compress_result\": [doc.page_content for doc in documents_to_compress]\n",
        "            }\n",
        "\n",
        "\n",
        "    async def kb_assess_and_reflect_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Assess document quality and decide next steps.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Assessing and Reflecting ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        # Use compressed result if available, otherwise combined documents\n",
        "        documents_to_assess = state.get(\"compress_result\", []) or [doc.page_content for doc in (state.get(\"rerank_result\", []) or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", [])))]\n",
        "\n",
        "        if not documents_to_assess:\n",
        "            logger.warning(\"No documents to assess.\")\n",
        "            return {\n",
        "                 \"reflection_output\": {\"decision\": \"sufficient\", \"assessment\": \"No documents to assess.\", \"confidence\": 1.0},\n",
        "                 \"reflection_decision\": \"sufficient\"\n",
        "            }\n",
        "\n",
        "        # Check if reflection tool was successfully initialized\n",
        "        if not hasattr(self, 'reflection_tool') or self.reflection_tool is None:\n",
        "             logger.error(\"Reflection tool not initialized.\")\n",
        "             return {\n",
        "                \"error_message\": \"Reflection tool not initialized.\",\n",
        "                \"reflection_output\": {\"decision\": \"error\", \"assessment\": \"Reflection tool not initialized.\", \"confidence\": 0.0},\n",
        "                \"reflection_decision\": \"sufficient\", # Default to sufficient on tool error\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            logger.info(\"Assessing documents with reflection tool.\")\n",
        "            # Use the instance reflection_tool with ainvoke\n",
        "            # The reflection_tool.ainvoke expects {\"question\": ..., \"documents\": ...}\n",
        "            reflection_result: ReflectionOutput = await self.reflection_tool.ainvoke({\n",
        "                \"question\": query,\n",
        "                \"documents\": documents_to_assess # Pass the list of strings (compressed result) or page_content\n",
        "            })\n",
        "\n",
        "            decision = reflection_result.decision\n",
        "            logger.info(f\"Reflection Decision: '{decision}'\")\n",
        "\n",
        "            return {\n",
        "                \"reflection_output\": reflection_result.model_dump(), # Store Pydantic model as dictionary\n",
        "                \"reflection_decision\": decision\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reflection error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"Reflection failed: {e}\",\n",
        "                 \"reflection_output\": {\"decision\": \"error\", \"assessment\": f\"Reflection failed: {e}\", \"confidence\": 0.0},\n",
        "                \"reflection_decision\": \"sufficient\"  # Default to sufficient on error\n",
        "            }\n",
        "\n",
        "    async def kb_generate_answer_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for generating the final answer using the LLM and retrieved context.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Generating Answer ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        if not query and state.get(\"messages\"):\n",
        "            query = state[\"messages\"][-1].content\n",
        "\n",
        "        # Check if LLM was successfully initialized\n",
        "        if not hasattr(self, 'llm') or self.llm is None:\n",
        "             logger.error(\"LLM not initialized.\")\n",
        "             return {\n",
        "                 \"final_answer\": \"Error: Language model not initialized.\",\n",
        "                 \"error_message\": \"LLM not initialized.\"\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            # Use the compressed results if available, otherwise use the reranked/retrieved docs\n",
        "            # If compression was skipped, compress_result holds the page_content\n",
        "            context_docs = state.get(\"compress_result\") # This is already List[str]\n",
        "            if not context_docs:\n",
        "                 # Fallback to reranked or combined documents if compression didn't produce results\n",
        "                 combined_docs = state.get(\"rerank_result\", []) or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", []))\n",
        "                 context_docs = [doc.page_content for doc in combined_docs] # Extract page_content if not compressed\n",
        "\n",
        "            context = \"\\n\\n\".join(context_docs) # Join snippets with double newline\n",
        "\n",
        "            prompt_template = f\"\"\"\n",
        "            You are an AI assistant tasked with providing comprehensive and accurate answers\n",
        "            based on the following context.\n",
        "\n",
        "            Context:\n",
        "            {context if context.strip() else \"No relevant context found.\"}\n",
        "\n",
        "            Question:\n",
        "            {query}\n",
        "\n",
        "            If the context doesn't contain the answer, state that you cannot answer from the provided information.\n",
        "            Be concise and directly answer the question based *only* on the provided context.\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate response using the instance llm\n",
        "            response = await self.llm.ainvoke(prompt_template)\n",
        "\n",
        "            logger.info(\"Answer generated.\")\n",
        "            return {\"final_answer\": response.content}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Generation error: {e}\")\n",
        "            return {\n",
        "                \"final_answer\": f\"Error generating answer: {e}\",\n",
        "                \"error_message\": f\"Generation failed: {e}\"\n",
        "            }\n",
        "\n",
        "    # =============================================================================\n",
        "    # ROUTING FUNCTIONS - Corrected to use proper conditional logic and state keys\n",
        "    # =============================================================================\n",
        "\n",
        "    def route_after_lookup(self, state: KBState) -> str:\n",
        "        \"\"\"Route after KB lookup based on status and grade.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after KB Lookup (Status: {state.get('kb_lookup_status', 'N/A')}, Web Search Required: {state.get('web_search_required', 'N/A')}) ---\")\n",
        "        if state.get(\"error_message\"):\n",
        "            logger.info(\"Routing to generate due to error.\")\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.get(\"kb_lookup_status\") == \"miss\" or state.get(\"web_search_required\"):\n",
        "             logger.info(\"Routing to rewrite query for web search.\")\n",
        "             return \"kb_rewrite\" # Go to rewrite for web search if KB missed or search is required\n",
        "        else: # KB hit and no web search required from initial check\n",
        "             logger.info(\"Routing to grade retrieved documents.\")\n",
        "             return \"kb_grade\"\n",
        "\n",
        "    def route_after_grade(self, state: KBState) -> str:\n",
        "        \"\"\"Route after grading based on relevance score.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after Grade (Web Search Required: {state.get('web_search_required', 'N/A')}) ---\")\n",
        "        if state.get(\"error_message\"):\n",
        "            logger.info(\"Routing to generate due to error.\")\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.get(\"web_search_required\", False): # web_search_required set by grader\n",
        "            logger.info(\"Routing to rewrite query for web search.\")\n",
        "            return \"kb_rewrite\"\n",
        "        else:\n",
        "            logger.info(\"Routing to rerank documents.\")\n",
        "            return \"kb_rerank\"\n",
        "\n",
        "    def route_after_rewrite(self, state: KBState) -> str:\n",
        "        \"\"\"Route after rewriting query.\"\"\"\n",
        "        logger.info(\"--- ROUTING after Rewrite ---\")\n",
        "        # Assuming rewrite always leads to web search if successful, or generate on error\n",
        "        if state.get(\"error_message\"):\n",
        "             logger.info(\"Routing to generate due to error.\")\n",
        "             return \"kb_generate_answer\"\n",
        "        elif state.get(\"subqueries\"): # Check if subqueries were generated (or original query is there)\n",
        "             logger.info(\"Routing to web search.\")\n",
        "             return \"kb_web_search\"\n",
        "        else: # No subqueries generated, fallback\n",
        "             logger.info(\"No subqueries generated, routing to generate answer (possibly with limited context).\")\n",
        "             return \"kb_generate_answer\"\n",
        "\n",
        "\n",
        "    def route_after_search(self, state: KBState) -> str:\n",
        "        \"\"\"Route after web search based on results and iteration count.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after Web Search (Iterations: {state.get('iterations', 'N/A')}, Max Iterations: {state.get('max_iterations', 'N/A')}) ---\")\n",
        "        max_iterations = state.get(\"max_iterations\", 3)\n",
        "        current_iterations = state.get(\"iterations\", 0)\n",
        "        crawl_results = state.get(\"crawl_results\", [])\n",
        "\n",
        "        if state.get(\"error_message\"):\n",
        "             logger.info(\"Routing to generate due to error.\")\n",
        "             return \"kb_generate_answer\"\n",
        "        elif crawl_results and current_iterations < max_iterations:\n",
        "             # If results found and more iterations allowed, assess and reflect\n",
        "             logger.info(\"Crawl results found and iterations remaining. Routing to update iteration and assess.\")\n",
        "             return \"kb_update_iteration\" # Update iteration before assessing\n",
        "        else:\n",
        "             # If no results, max iterations reached, or error, proceed to rerank/generate\n",
        "             logger.info(\"No crawl results or max iterations reached. Routing to rerank.\")\n",
        "             return \"kb_rerank\"\n",
        "\n",
        "    def route_after_iteration(self, state: KBState) -> str:\n",
        "        \"\"\"Route after iteration update.\"\"\"\n",
        "        logger.info(\"--- ROUTING after Update Iteration ---\")\n",
        "        # After updating iteration, always assess the new set of documents\n",
        "        logger.info(\"Routing to assess and reflect.\")\n",
        "        return \"kb_assess_and_reflect\"\n",
        "\n",
        "\n",
        "    def route_after_reflection(self, state: KBState) -> str:\n",
        "        \"\"\"Route after reflection based on decision.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after Reflection (Decision: {state.get('reflection_decision', 'N/A')}) ---\")\n",
        "        if state.get(\"error_message\"):\n",
        "             logger.info(\"Routing to generate due to error.\")\n",
        "             return \"kb_generate_answer\"\n",
        "        elif state.get(\"reflection_decision\") == \"refine\":\n",
        "            # If refinement needed, rewrite query and search again\n",
        "            logger.info(\"Reflection calls for refinement. Routing to rewrite query.\")\n",
        "            return \"kb_rewrite\"\n",
        "        else: # Decision is \"sufficient\"\n",
        "            # If sufficient, proceed to final processing (rerank, compress, generate)\n",
        "            logger.info(\"Reflection deems documents sufficient. Routing to rerank.\")\n",
        "            return \"kb_rerank\"\n",
        "\n",
        "    # =============================================================================\n",
        "    # GRAPH BUILDER - Corrected to use proper LangGraph patterns and instance methods\n",
        "    # =============================================================================\n",
        "\n",
        "    async def build_rag_graph(self):\n",
        "        \"\"\"Build the RAG graph with corrected syntax using instance methods.\"\"\"\n",
        "\n",
        "        # Define the state machine\n",
        "        # Use the corrected KBState\n",
        "        builder = StateGraph(KBState)\n",
        "\n",
        "        # Add nodes - corrected node names and references to instance methods\n",
        "        builder.add_node(\"kb_retrieve\", self.kb_retrieve_node)\n",
        "        builder.add_node(\"kb_grade\", self.kb_grade_node)\n",
        "        builder.add_node(\"kb_rewrite\", self.kb_rewrite_node)\n",
        "        # Assuming kb_web_search_node is implemented or placeholder exists\n",
        "        builder.add_node(\"kb_web_search\", self.kb_web_search_node) # Add web search node\n",
        "        builder.add_node(\"kb_update_iteration\", self.kb_update_iteration_node)\n",
        "        builder.add_node(\"kb_assess_and_reflect\", self.kb_assess_and_reflect_node)\n",
        "        builder.add_node(\"kb_rerank\", self.kb_rerank_node) # Add rerank node\n",
        "        builder.add_node(\"kb_compress\", self.kb_compress_node) # Add compress node\n",
        "        builder.add_node(\"kb_generate_answer\", self.kb_generate_answer_node)\n",
        "\n",
        "        # Set the entry point - corrected\n",
        "        builder.add_edge(START, \"kb_retrieve\")\n",
        "\n",
        "        # Add conditional edges with corrected routing using instance methods\n",
        "        builder.add_conditional_edges(\"kb_retrieve\", self.route_after_lookup)\n",
        "        builder.add_conditional_edges(\"kb_grade\", self.route_after_grade)\n",
        "        builder.add_conditional_edges(\"kb_rewrite\", self.route_after_rewrite) # Route after rewrite\n",
        "        builder.add_conditional_edges(\"kb_web_search\", self.route_after_search) # Route after web search\n",
        "        builder.add_edge(\"kb_update_iteration\", \"kb_assess_and_reflect\") # Update iteration always leads to assess\n",
        "        builder.add_conditional_edges(\"kb_assess_and_reflect\", self.route_after_reflection) # Route after reflection\n",
        "\n",
        "        # Connect the final processing steps\n",
        "        builder.add_edge(\"kb_rerank\", \"kb_compress\") # Rerank usually precedes compress\n",
        "        builder.add_edge(\"kb_compress\", \"kb_generate_answer\") # Compress usually precedes generate\n",
        "        builder.add_edge(\"kb_generate_answer\", END) # Final answer is the end\n",
        "\n",
        "\n",
        "        # Compile the graph\n",
        "        app = builder.compile()\n",
        "        return app\n"
      ],
      "metadata": {
        "id": "Z7b2N3Rvvjz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CORRECTED KNOWLEDGE BASE AGENT NODES\n",
        "# =============================================================================\n",
        "\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "import hashlib\n",
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "from enum import Enum\n",
        "from datetime import datetime, timezone\n",
        "from pydantic import BaseModel, Field, model_validator, field_validator\n",
        "from langchain_core.tools import StructuredTool\n",
        "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, RetryError\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_core.callbacks import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n",
        "# Ensure CompressorTool and RerankerTool are defined or imported\n",
        "# Assuming CompressorTool and RerankerTool from previous cells are available\n",
        "# from .compressor_tool import CompressorTool # Example if in a different file\n",
        "# from .reranker_tool import RerankerTool # Example if in a different file\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PYDANTIC MODELS - Updated imports and structure (Moved from lI_wH8hWw3jb for clarity)\n",
        "# =============================================================================\n",
        "\n",
        "# Assuming these were defined in a previous cell or imported\n",
        "# from <your_module> import RetrievalGrade, SubqueriesOutput, ReflectionOutput, DocumentResult, VectorStoreResponse, ExtractedURL, SearchResultSchema, DocumentSource, ContentSource, VectorStoreConfig, VectorStoreManager, RerankerTool, CompressorTool\n",
        "class SearchMode(str, Enum):\n",
        "    \"\"\"Search strategy.\"\"\"\n",
        "    SHALLOW = \"shallow\"   # one-off query, no follow-up\n",
        "    DEEP = \"deep\"         # multi-iteration, refinement + crawl\n",
        "\n",
        "\n",
        "class SearchEngine(str, Enum):\n",
        "    \"\"\"Engines / APIs used for search.\"\"\"\n",
        "    TAVILY = \"tavily\"\n",
        "    DUCKDUCKGO = \"duckduckgo\"\n",
        "    WIKIPEDIA = \"wikipedia\"\n",
        "    ARXIV = \"arxiv\"\n",
        "    CRAWL_ENGINE = \"crawl_engine\"  # generic fallback for full-page crawling\n",
        "    OTHER = \"other\"\n",
        "\n",
        "\n",
        "class SearchOutput(BaseModel):\n",
        "    \"\"\"Output schema for a search operation.\"\"\"\n",
        "    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description=\"Unique identifier for the search query.\")\n",
        "    query: str = Field(..., description=\"The query string that was issued.\")\n",
        "    snippet: str = Field(..., description=\"The search result snippet.\")\n",
        "    mode: SearchMode = Field(..., description=\"Whether the search was shallow or deep.\")\n",
        "    engine: SearchEngine = Field(..., description=\"Which engine/tool was used.\")\n",
        "    results: List[DocumentResult] = Field(default_factory=list, description=\"Normalized results (documents/snippets).\")\n",
        "    justification: Optional[str] = Field(None, description=\"Why this engine/mode was selected.\")\n",
        "    iteration: int = Field(default=0, description=\"Iteration number in the search loop.\")\n",
        "    status: str = Field(default=\"success\", description=\"success/failed/timeout/etc.\")\n",
        "    error: Optional[str] = Field(None, description=\"Error message if failed.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n",
        "    metadata: Optional[Dict[str, Any]] = None\n",
        "\n",
        "\n",
        "# If not imported, define them here or ensure they are in a preceding cell\n",
        "class RetrievalGrade(BaseModel):\n",
        "    \"\"\"Grade for retrieval relevance\"\"\"\n",
        "    score: int = Field(..., description=\"Binary relevance: 1 = relevant, 0 = not relevant.\")\n",
        "    verdict: str = Field(..., description=\"'yes' or 'no' (mirror of score).\")\n",
        "    confidence: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0.00-1.00).\")\n",
        "    rationale: str = Field(..., description=\"Step-by-step rationale (max ~120 words).\")\n",
        "    highlights: Optional[List[str]] = Field(None, description=\"Up to 3 short excerpts (≤30 words each).\")\n",
        "    refined_query: Optional[str] = Field(None, description=\"Refined query if score == 0 or low confidence.\")\n",
        "\n",
        "class SubqueriesOutput(BaseModel):\n",
        "    \"\"\"Output schema for subquery generation\"\"\"\n",
        "    queries: List[str] = Field(description=\"A list of distinct, optimized search queries to perform for gathering additional information.\")\n",
        "    justification: Optional[str] = Field(description=\"A brief explanation of why these specific subqueries were generated.\")\n",
        "\n",
        "class ReflectionOutput(BaseModel):\n",
        "    \"\"\"Output schema for reflection\"\"\"\n",
        "    decision: Literal[\"refine\", \"sufficient\"] = Field(description=\"Decision to 'refine' or proceed.\")\n",
        "    assessment: str = Field(description=\"A textual assessment of the gathered documents.\")\n",
        "    confidence: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"Confidence in the assessment.\")\n",
        "\n",
        "# Re-define other necessary Pydantic models if not imported\n",
        "class DocumentSource(str, Enum):\n",
        "    \"\"\"Source types for a document.\"\"\"\n",
        "    KNOWLEDGE_BASE = \"knowledge_base\"\n",
        "    EXTERNAL_API = \"external_api\"\n",
        "    WEB_CRAWL = \"web_crawl\"\n",
        "    SEARCH_SNIPPET = \"search_snippet\"\n",
        "    CACHE = \"cache\"\n",
        "\n",
        "class ContentSource(str, Enum):\n",
        "    \"\"\"Semantic categories assigned by LLM (e.g., academic, blog, forum).\"\"\"\n",
        "    ACADEMIC = \"academic\"\n",
        "    GOVERNMENT = \"government\"\n",
        "    WIKI = \"wikipedia\"\n",
        "    GIT_REPO = \"git_repo\"\n",
        "    NEWS = \"news\"\n",
        "    BLOG = \"blog\"\n",
        "    REPORT = \"report\"\n",
        "    FORUM = \"forum\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "class DocumentResult(BaseModel):\n",
        "    \"\"\"Standardized document result with metadata.\"\"\"\n",
        "    page_content: str = Field(..., description=\"The document content.\")\n",
        "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Metadata associated with the document.\")\n",
        "    content_length: Optional[int] = Field(default=0, ge=0, description=\"Content length in characters.\")\n",
        "    source_type: DocumentSource = Field(default=DocumentSource.SEARCH_SNIPPET, description=\"The origin of the document content.\")\n",
        "    content_source: Optional[ContentSource] = Field(default=ContentSource.OTHER, description=\"The semantic category of the content.\")\n",
        "    relevance_score: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"A score indicating relevance.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    document_id: Optional[str] = Field(default=None, description=\"Stable unique identifier for the document.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def compute_content_length(self) -> \"DocumentResult\":\n",
        "        \"\"\"Ensures content_length is computed from page_content if missing/zero.\"\"\"\n",
        "        if self.page_content and (not self.content_length or self.content_length == 0):\n",
        "            object.__setattr__(self, \"content_length\", len(self.page_content))\n",
        "\n",
        "        #  Always ensure document_id is stable (sha256 of content)\n",
        "        if not self.document_id:\n",
        "            did = hashlib.sha256(self.page_content.encode(\"utf-8\")).hexdigest()\n",
        "            object.__setattr__(self, \"document_id\", did)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def to_langchain(self):\n",
        "        \"\"\"Convert to LangChain Document.\"\"\"\n",
        "        from langchain_core.documents import Document\n",
        "        return Document(page_content=self.page_content, metadata=self.metadata, id=self.document_id)\n",
        "\n",
        "    @classmethod\n",
        "    def from_langchain(cls, doc: LangchainDocument, relevance_score: Optional[float] = None) -> \"DocumentResult\":\n",
        "        \"\"\"Build from a LangChain Document (reverse of to_langchain).\"\"\"\n",
        "        id = doc.id or hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest()\n",
        "        timestamp = doc.metadata.get(\"created_at\") or datetime.now(timezone.utc)\n",
        "        return cls(\n",
        "            page_content=doc.page_content,\n",
        "            metadata=doc.metadata,\n",
        "            relevance_score=relevance_score,\n",
        "            created_at=timestamp,\n",
        "            document_id=id,\n",
        "        )\n",
        "\n",
        "class VectorStoreResponse(BaseModel):\n",
        "    \"\"\"Unified response schema for vector store operations.\"\"\"\n",
        "\n",
        "    query: Optional[str] = None  # retrieval query\n",
        "    results: Optional[List[\"DocumentResult\"]] = None  # retrieval results\n",
        "    status: str = Field(..., description=\"Operation status: success or failed.\")\n",
        "    message: str = Field(..., description=\"Human-readable summary of the result.\")\n",
        "    document_ids: List[str] = Field(default_factory=list, description=\"List of indexed document IDs.\")\n",
        "    indexed_documents: List[\"DocumentResult\"] = Field(default_factory=list, description=\"The full indexed documents.\")\n",
        "    error: Optional[str] = Field(default=None, description=\"Error message if any.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def validate_exclusive_fields(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure that document_ids/indexed_documents and results are mutually exclusive.\"\"\"\n",
        "        if self.results and (self.document_ids or self.indexed_documents):\n",
        "            raise ValueError(\"Response cannot contain both retrieval results and indexing results.\")\n",
        "        return self\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def sync_document_ids(self) -> \"VectorStoreResponse\":\n",
        "        \"\"\"Ensure document_ids matches indexed_documents' IDs if provided.\"\"\"\n",
        "        if self.indexed_documents:\n",
        "            ids = [doc.document_id for doc in self.indexed_documents if doc.document_id]\n",
        "            object.__setattr__(self, \"document_ids\", ids)\n",
        "        return self\n",
        "\n",
        "class TraceStep(BaseModel):\n",
        "    \"\"\"One step in the reasoning trace.\"\"\"\n",
        "    step_type: Literal[\n",
        "        \"kb_lookup\", \"rerank\", \"compress\",\n",
        "        \"shallow_search\", \"deep_search\", \"crawl\",\n",
        "        \"reflection\", \"answer\", \"error\"\n",
        "    ]\n",
        "    description: str  # Human-readable explanation of what happened\n",
        "    outcome: Optional[str] = None  # \"success\", \"miss\", \"insufficient\", \"error\"\n",
        "    engine: Optional[str] = None  # e.g., \"DuckDuckGo\", \"Wikipedia\", \"Arxiv\"\n",
        "    iteration: Optional[int] = None  # Iteration number if applicable\n",
        "    query: Optional[str] = None  # The subquery or refined query used\n",
        "    search_id: Optional[str] = None  # 🔗 Links to SearchOutput (UUID or index)\n",
        "\n",
        "class ReasoningTrace(BaseModel):\n",
        "    \"\"\"Breadcrumb trail of reasoning steps taken by the agent.\"\"\"\n",
        "    steps: List[TraceStep] = Field(default_factory=list)\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def ensure_unique_search_links(self) -> \"ReasoningTrace\":\n",
        "        \"\"\"Ensure no duplicate search_id references in steps.\"\"\"\n",
        "        seen = set()\n",
        "        for step in self.steps:\n",
        "            if step.search_id:\n",
        "                if step.search_id in seen:\n",
        "                    raise ValueError(f\"Duplicate search_id in reasoning trace: {step.search_id}\")\n",
        "                seen.add(step.search_id)\n",
        "        return self\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STATE SCHEMAS - Corrected to use proper LangGraph patterns\n",
        "# =============================================================================\n",
        "\n",
        "class KBState(TypedDict):\n",
        "    \"\"\"Knowledge Base State - Updated to include sources and reasoning trace\"\"\"\n",
        "\n",
        "    query: str  # The user's original query\n",
        "    messages: Annotated[List[BaseMessage], \"add_messages\"]  # Conversation history\n",
        "    session_id: str  # Unique session identifier\n",
        "\n",
        "    # Retrieval & KB lookup\n",
        "    retrieved_documents: Annotated[List[LangchainDocument], \"default_factory=list\"]\n",
        "    kb_lookup_status: Literal[\"hit\", \"miss\", \"not_attempted\"]\n",
        "    web_search_required: bool\n",
        "    escalation_required: bool\n",
        "\n",
        "    # Iteration & reflection\n",
        "    iterations: int\n",
        "    max_iterations: int\n",
        "    retrieval_grade: Optional[dict]\n",
        "    subqueries: Annotated[List[str], \"default_factory=list\"]\n",
        "    crawl_results: Annotated[List[LangchainDocument], \"default_factory=list\"]\n",
        "    reflection_output: Optional[dict]\n",
        "    reflection_decision: Literal[\"refine\", \"sufficient\", \"error\"]\n",
        "\n",
        "    # Error & answer\n",
        "    error_message: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "\n",
        "    # Rerank & compression\n",
        "    use_reranker: bool\n",
        "    use_compressor: bool\n",
        "    rerank_result: Optional[List[LangchainDocument]]\n",
        "    compress_result: Optional[List[str]]\n",
        "\n",
        "    # 🔎 Search tracking\n",
        "    search_outputs: Annotated[List[SearchOutput], \"default_factory=list\"]\n",
        "\n",
        "    # 📄 Sources (normalized doc metadata)\n",
        "    sources: Annotated[List[Dict[str, Any]], \"default_factory=list\"]\n",
        "\n",
        "    # 🧠 Reasoning trace\n",
        "    reasoning_trace: ReasoningTrace\n",
        "\n",
        "    # def add_search_step(\n",
        "    #     self,\n",
        "    #     *,\n",
        "    #     mode: Literal[\"shallow\", \"deep\", \"crawl\"],\n",
        "    #     engine: str,\n",
        "    #     query: str,\n",
        "    #     iteration: int,\n",
        "    #     results: List[Dict[str, Any]],\n",
        "    #     status: Literal[\"success\", \"insufficient\", \"error\"],\n",
        "    #     description: str\n",
        "    # ) -> None:\n",
        "    #     \"\"\"Helper to add a search result + linked reasoning step in one go.\"\"\"\n",
        "    #     search_output = SearchOutput(\n",
        "    #         mode=mode,\n",
        "    #         engine=engine,\n",
        "    #         query=query,\n",
        "    #         iteration=iteration,\n",
        "    #         results=results,\n",
        "    #         status=status,\n",
        "    #     )\n",
        "    #     self.search_outputs.append(search_output)\n",
        "\n",
        "    #     trace_step = TraceStep(\n",
        "    #         step_type=f\"{mode}_search\" if mode != \"crawl\" else \"crawl\",\n",
        "    #         description=description,\n",
        "    #         outcome=status,\n",
        "    #         engine=engine,\n",
        "    #         iteration=iteration,\n",
        "    #         query=query,\n",
        "    #         search_id=search_output.id,\n",
        "    #     )\n",
        "    #     self.reasoning_trace.steps.append(trace_step)\n",
        "\n",
        "class VectorStoreConfig(BaseModel):\n",
        "    chroma_persist_directory: str = Field(default=\"./chroma_db\", description=\"The directory where ChromaDB data will be persisted.\")\n",
        "    collection_name: str = Field(default=\"knowledge_agent_collection\", description=\"The name of the ChromaDB collection.\")\n",
        "    embedding_model_provider: str = Field(default=\"sentence-transformer\", description=\"The provider for the embedding model.\") # Use str instead of Literal for flexibility\n",
        "    embedding_model_name: str = Field(default=\"all-MiniLM-L6-v2\", description=\"The name of the embedding model to use.\")\n",
        "    relevance_threshold: float = Field(default=0.8, description=\"The similarity threshold for filtering relevant documents.\")\n",
        "\n",
        "class VectorStoreManager:\n",
        "    \"\"\"\n",
        "    Manages the lifecycle and tool creation for the vector store.\n",
        "    Encapsulates the setup, indexing, and retrieval logic.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)  # class-level logger\n",
        "\n",
        "    def __init__(self, config: VectorStoreConfig, initial_documents: Optional[List[LangchainDocument]] = None):\n",
        "        self.config = config\n",
        "        self.vectorstore = self._setup_vector_store()\n",
        "        if initial_documents:\n",
        "            # Index initial documents upon creation\n",
        "            self._index_initial_documents(initial_documents)\n",
        "\n",
        "        if not self.logger.hasHandlers():\n",
        "            logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def _get_embedding_model(self, cfg: VectorStoreConfig):\n",
        "        # Assuming SentenceTransformerEmbeddings and HuggingFaceEmbeddings are available\n",
        "        if cfg.embedding_model_provider == \"huggingface\":\n",
        "            return HuggingFaceEmbeddings(model_name=cfg.embedding_model_name)\n",
        "        elif cfg.embedding_model_provider == \"sentence-transformer\":\n",
        "            return SentenceTransformerEmbeddings(model_name=cfg.embedding_model_name)\n",
        "        else:\n",
        "             logger.warning(f\"Unknown embedding provider: {cfg.embedding_model_provider}. Falling back to sentence-transformer.\")\n",
        "             return SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Fallback\n",
        "\n",
        "    @staticmethod\n",
        "    def _prefixed_sha256(x: Union[str, bytes]) -> bytes:\n",
        "        prefix = b\"kb_vectorstore:\"\n",
        "        if isinstance(x, str):\n",
        "            x = x.encode(\"utf-8\")\n",
        "        return hashlib.sha256(prefix + x).digest()\n",
        "\n",
        "    def _setup_vector_store(self) -> Chroma:\n",
        "        try:\n",
        "            embeddings = self._get_embedding_model(self.config)\n",
        "            cache_store = InMemoryStore()\n",
        "            cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
        "                underlying_embeddings=embeddings,\n",
        "                document_embedding_cache=cache_store,\n",
        "                query_embedding_cache=cache_store,\n",
        "                key_encoder=self._prefixed_sha256,\n",
        "            )\n",
        "            # Assuming os is imported\n",
        "            import os\n",
        "            os.makedirs(self.config.chroma_persist_directory, exist_ok=True)\n",
        "            vectorstore = Chroma(\n",
        "                collection_name=self.config.collection_name,\n",
        "                embedding_function=cached_embeddings,\n",
        "                persist_directory=self.config.chroma_persist_directory,\n",
        "            )\n",
        "            self.logger.info(f\"Chroma initialized and ready at {self.config.chroma_persist_directory}\")\n",
        "            return vectorstore\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to initialize vector store: {e}\") from e\n",
        "\n",
        "    def _index_initial_documents(self, documents: List[LangchainDocument]):\n",
        "        \"\"\"Helper to index initial documents synchronously.\"\"\"\n",
        "        docs_to_add = [DocumentResult.from_langchain(doc) for doc in documents]\n",
        "        # Assuming the sync index_documents method is implemented correctly\n",
        "        self.index_documents(docs_to_add)\n",
        "\n",
        "    # Assuming index_documents and retrieve_documents methods are implemented as in the previous cell\n",
        "\n",
        "    def as_index_tool(self) -> StructuredTool:\n",
        "        # This method is likely not used in the current graph, but keep for completeness\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.index_documents,\n",
        "            # coroutine=self.aindex_documents, # Ensure aindex_documents is implemented\n",
        "            name=\"vectorstore_index\",\n",
        "            description=\"Indexes a list of documents into the vector store.\"\n",
        "        )\n",
        "\n",
        "    def as_retrieve_tool(self) -> StructuredTool:\n",
        "        # This is also likely not used directly in the graph as the retriever instance is passed\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.retrieve_documents,\n",
        "            # coroutine=self.aretrieve_documents, # Ensure aretrieve_documents is implemented\n",
        "            name=\"vectorstore_retrieve\",\n",
        "            description=\"Retrieves documents from the vector store based on semantic similarity.\"\n",
        "        )\n",
        "\n",
        "    # Add placeholder sync/async index and retrieve methods if they were not in the previous cell\n",
        "    def index_documents(self, documents: List[DocumentResult]) -> VectorStoreResponse:\n",
        "        \"\"\"Synchronous method to index documents (Placeholder).\"\"\"\n",
        "        logger.info(f\"Indexing {len(documents)} documents (simulated).\")\n",
        "        # Simulate indexing\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Indexed {len(documents)} documents (simulated).\",\n",
        "            indexed_documents=documents,\n",
        "        )\n",
        "\n",
        "    async def aindex_documents(self, documents: List[DocumentResult]) -> VectorStoreResponse:\n",
        "         \"\"\"Asynchronous method to index documents (Placeholder).\"\"\"\n",
        "         logger.info(f\"Indexing {len(documents)} documents asynchronously (simulated).\")\n",
        "         await asyncio.sleep(0.1) # Simulate async work\n",
        "         return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Indexed {len(documents)} documents asynchronously (simulated).\",\n",
        "            indexed_documents=documents,\n",
        "        )\n",
        "\n",
        "    def retrieve_documents(self, query: str, top_k: int = 5) -> VectorStoreResponse:\n",
        "        \"\"\"Synchronous method to retrieve documents (Placeholder).\"\"\"\n",
        "        logger.info(f\"Retrieving documents for query: '{query}' (simulated).\")\n",
        "        # Simulate retrieval - return first few initial documents\n",
        "        simulated_results = [DocumentResult.from_langchain(doc) for doc in self.vectorstore.as_retriever(search_kwargs={\"k\": top_k}).invoke(query)]\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Retrieved {len(simulated_results)} documents (simulated).\",\n",
        "            query=query,\n",
        "            results=simulated_results\n",
        "        )\n",
        "\n",
        "    async def aretrieve_documents(self, query: str, top_k: int = 5) -> VectorStoreResponse:\n",
        "        \"\"\"Asynchronous method to retrieve documents (Placeholder).\"\"\"\n",
        "        logger.info(f\"Retrieving documents asynchronously for query: '{query}' (simulated).\")\n",
        "        await asyncio.sleep(0.1) # Simulate async work\n",
        "        # Simulate async retrieval\n",
        "        simulated_results = [DocumentResult.from_langchain(doc) for doc in await self.vectorstore.as_retriever(search_kwargs={\"k\": top_k}).ainvoke(query)]\n",
        "        return VectorStoreResponse(\n",
        "            status=\"success\",\n",
        "            message=f\"Retrieved {len(simulated_results)} documents asynchronously (simulated).\",\n",
        "            query=query,\n",
        "            results=simulated_results\n",
        "        )\n",
        "\n",
        "# Assuming RerankerTool and CompressorTool classes are defined as in previous successful cells\n",
        "\n",
        "class RerankerTool:\n",
        "    \"\"\"\n",
        "    A class to encapsulate the reranking tool and its dependencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_model: Any, vectorstore: Any, top_n=10, k: int = 20):\n",
        "        self.pipeline = self._setup_reranker_pipeline(embedding_model, vectorstore, top_n, k)\n",
        "        if \"reranker\" not in self.pipeline:\n",
        "             # Fallback if reranker setup fails\n",
        "             logger.warning(\"Reranker not found in pipeline. Using base retriever.\")\n",
        "             self.reranker = None # Indicate reranker is not available\n",
        "             self._base_retriever = self.pipeline # Store the base retriever\n",
        "        else:\n",
        "            self.reranker = self.pipeline[\"reranker\"]\n",
        "            self._base_retriever = self.pipeline[\"pipeline\"] # Store the full compression retriever\n",
        "\n",
        "\n",
        "    def _setup_reranker_pipeline(self, embedding_model, vectorstore, top_n: int = 10, k: int = 20):\n",
        "        \"\"\"\n",
        "        Setup a retriever pipeline that retrieves documents via a vectorstore\n",
        "        and reranks them with a cross-encoder.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "            # Load the cross-encoder model\n",
        "            # Ensure HuggingFaceCrossEncoder is imported\n",
        "            hf_model = HuggingFaceCrossEncoder(\n",
        "                model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "                model_kwargs={\"device\": \"cpu\"}  # or \"cuda\" if available\n",
        "            )\n",
        "            reranker = CrossEncoderReranker(\n",
        "                model=hf_model,\n",
        "                top_n=top_n,\n",
        "            )\n",
        "            pipeline = ContextualCompressionRetriever(\n",
        "                base_compressor=reranker,\n",
        "                base_retriever=base_retriever,\n",
        "            )\n",
        "            return {\"pipeline\": pipeline, \"reranker\": reranker}\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to setup reranker pipeline: {e}\")\n",
        "            # Return just the base retriever as a fallback\n",
        "            return vectorstore.as_retriever(search_kwargs={\"k\": top_n})\n",
        "\n",
        "    # Corrected Input Schema for StructuredTool\n",
        "    class RerankInput(BaseModel):\n",
        "        query: str = Field(..., description=\"The query for reranking.\")\n",
        "        documents: List[LangchainDocument] = Field(..., description=\"The documents to rerank.\")\n",
        "        top_n: int = Field(5, description=\"Number of top documents to return.\")\n",
        "\n",
        "\n",
        "    async def arerank_documents(self, query: str, documents: List[LangchainDocument], top_n: int = 5) -> List[LangchainDocument]:\n",
        "        \"\"\"Async version to be wrapped by StructuredTool.\"\"\"\n",
        "        if not self.reranker:\n",
        "            logger.warning(\"Reranker not available. Returning original documents.\")\n",
        "            return documents[:top_n]\n",
        "\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Use the underlying compressor directly for flexibility or use ainvoke on _base_retriever\n",
        "            # If using _base_retriever (ContextualCompressionRetriever), ainvoke handles retrieval + compression\n",
        "            # But the tool receives pre-retrieved documents. So, use the compressor directly.\n",
        "            reranked_docs = await self.reranker.acompress_documents(\n",
        "                documents=documents,\n",
        "                query=query\n",
        "            )\n",
        "            return reranked_docs[:top_n]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reranker tool failed: {e}\", exc_info=True)\n",
        "            return documents[:top_n] # Return original top_n on failure\n",
        "\n",
        "    def rerank_documents(self, query: str, documents: List[LangchainDocument], top_n: int = 5) -> List[LangchainDocument]:\n",
        "        \"\"\"Sync version to be wrapped by StructuredTool.\"\"\"\n",
        "        if not self.reranker:\n",
        "            logger.warning(\"Reranker not available. Returning original documents.\")\n",
        "            return documents[:top_n]\n",
        "\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            reranked_docs = self.reranker.compress_documents(\n",
        "                documents=documents,\n",
        "                query=query\n",
        "            )\n",
        "            return reranked_docs[:top_n]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reranker tool failed: {e}\", exc_info=True)\n",
        "            return documents[:top_n] # Return original top_n on failure\n",
        "\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        \"\"\"Expose reranking functionality as a StructuredTool.\"\"\"\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.rerank_documents,\n",
        "            coroutine=self.arerank_documents,\n",
        "            name=\"rerank_documents\",\n",
        "            description=\"Reranks a list of documents based on relevance to a query.\",\n",
        "            args_schema=self.RerankInput,\n",
        "            # return_direct=True # Usually False for intermediate tools\n",
        "        )\n",
        "\n",
        "class CompressorTool:\n",
        "    \"\"\"\n",
        "    A class to encapsulate the compressor tool using LLMChainExtractor.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: Any):\n",
        "        self.compressor: BaseDocumentCompressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "    # Corrected Input Schema for StructuredTool\n",
        "    class CompressorInput(BaseModel):\n",
        "        query: str = Field(..., description=\"The user's query for compression.\")\n",
        "        documents: List[LangchainDocument] = Field(..., description=\"The documents to compress.\")\n",
        "\n",
        "    def complex_compress_documents(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[LangchainDocument],\n",
        "    ) -> List[LangchainDocument]:\n",
        "        \"\"\"Synchronous method to be wrapped by StructuredTool.\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "        try:\n",
        "            run_manager = CallbackManagerForRetrieverRun.get_noop_manager()\n",
        "            compressed_docs = self.compressor.compress_documents(\n",
        "                documents=documents,\n",
        "                query=query,\n",
        "                callbacks=run_manager.get_child(),\n",
        "            )\n",
        "            return compressed_docs\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Compressor tool failed (sync): {e}\", exc_info=True)\n",
        "            return [] # Return empty list on failure\n",
        "\n",
        "    async def acomplex_compress_documents(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[LangchainDocument],\n",
        "    ) -> List[LangchainDocument]:\n",
        "        \"\"\"Asynchronous method to be wrapped by StructuredTool.\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "        try:\n",
        "            run_manager = AsyncCallbackManagerForRetrieverRun.get_noop_manager()\n",
        "            compressed_docs = await self.compressor.acompress_documents(\n",
        "                documents=documents,\n",
        "                query=query,\n",
        "                callbacks=run_manager.get_child(),\n",
        "            )\n",
        "            return compressed_docs\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Compressor tool failed (async): {e}\", exc_info=True)\n",
        "            return [] # Return empty list on failure\n",
        "\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        \"\"\"Expose compression functionality as a StructuredTool.\"\"\"\n",
        "        return StructuredTool.from_function(\n",
        "            func=self.complex_compress_documents,\n",
        "            coroutine=self.acomplex_compress_documents,\n",
        "            name=\"llm_compressor\",\n",
        "            description=\"Extracts relevant parts from documents using an LLM.\",\n",
        "            args_schema=self.CompressorInput,\n",
        "            # return_direct=True # Usually False for intermediate tools\n",
        "        )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# KNOWLEDGE BASE AGENT CLASS - Corrected Implementation\n",
        "# =============================================================================\n",
        "\n",
        "class KnowledgeBaseAgent:\n",
        "    \"\"\"Knowledge Base Agent with all node implementations.\"\"\"\n",
        "\n",
        "    def __init__(self, documents_list: List[LangchainDocument], cache_adapter=None):\n",
        "        self.documents_list = documents_list\n",
        "        print(self.documents_list)\n",
        "        self.cache_adapter = cache_adapter\n",
        "        # Initialize components properly\n",
        "        self.setup_components()\n",
        "\n",
        "    def setup_components(self):\n",
        "        \"\"\"Setup all required components as instance attributes.\"\"\"\n",
        "        # --- Initialize components ---\n",
        "        self.config = VectorStoreConfig(\n",
        "            embedding_model_provider=\"sentence-transformer\",\n",
        "            embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "            chroma_persist_directory=\"./chroma_test\",\n",
        "            collection_name=\"test_collection\",\n",
        "        )\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = init_chat_model(\n",
        "            \"gemini-2.5-flash\",\n",
        "            model_provider=\"google-genai\",\n",
        "            temperature=0.2,\n",
        "            # Add rate limiter if needed: rate_limiter=InMemoryRateLimiter(requests_per_second=0.1),\n",
        "        )\n",
        "\n",
        "        # Setup grader, query rewriter, and reflection tool using the instance llm\n",
        "        self.grader = self.setup_grader(self.llm)\n",
        "        self.query_rewriter = self.setup_query_rewriter(self.llm)\n",
        "        self.reflection_tool = self.setup_reflection_tool(self.llm)\n",
        "\n",
        "        # Initialize Vector Store Manager and Retriever using instance config and documents\n",
        "        vectorstore_manager = VectorStoreManager(self.config, initial_documents=self.documents_list)\n",
        "\n",
        "        # Get the retriever instance from the manager\n",
        "        self.retriever = vectorstore_manager.vectorstore.as_retriever(search_kwargs={\"k\": 20}) # Retrieve enough for reranking\n",
        "\n",
        "        # Initialize Reranker Tool using instance embedding model and vectorstore\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Initialize embedding model as instance attribute\n",
        "        reranker_instance = RerankerTool(\n",
        "            embedding_model=self.embedding_model, # Use instance attribute\n",
        "            vectorstore=vectorstore_manager.vectorstore, # Use instance vectorstore from manager\n",
        "            top_n=5 # Rerank to top 5\n",
        "        )\n",
        "        self.reranker_tool = reranker_instance.as_tool() # Get the StructuredTool\n",
        "\n",
        "        # Initialize Compressor Tool using instance llm\n",
        "        compressor_instance = CompressorTool(llm=self.llm) # Use instance attribute\n",
        "        self.compressor_tool = compressor_instance.as_tool() # Get the StructuredTool\n",
        "\n",
        "\n",
        "    def setup_llm(\n",
        "        self,\n",
        "        model: str = \"gemini-2.0-flash\", # Corrected default model name\n",
        "        provider: str = \"google-genai\", # Corrected provider string\n",
        "        temperature: float = 0.2\n",
        "    ):\n",
        "        \"\"\"Initialize LLM with proper error handling.\"\"\"\n",
        "        logger.info(f\"------Setting up LLM: {model} ({provider})------\")\n",
        "        try:\n",
        "            # Assuming dotenv is imported and loaded if needed globally\n",
        "            # from dotenv import load_dotenv\n",
        "            # load_dotenv()\n",
        "            return init_chat_model(\n",
        "                model=model,\n",
        "                model_provider=provider,\n",
        "                temperature=temperature\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize LLM: {e}\")\n",
        "            # Decide on error handling: return None, raise, or handle differently\n",
        "            # For this example, let's raise the exception as the graph depends on the LLM\n",
        "            raise\n",
        "\n",
        "    def setup_grader(self, llm: Any):\n",
        "        \"\"\"Setup document relevance grader.\"\"\"\n",
        "        system_prompt = \"\"\"\n",
        "        You are an expert grader judging whether a retrieved DOCUMENT is sufficiently\n",
        "        relevant, correct, recent, and comprehensive for the USER QUESTION.\n",
        "\n",
        "        PRIORITIES (in order):\n",
        "        1. RELEVANCE: Does the document contain statements directly answering the question?\n",
        "        2. CORRECTNESS: Is the information internally consistent and plausible?\n",
        "        3. RECENCY: If time-sensitive, prefer newer/dated information?\n",
        "        4. COVERAGE: Does it cover core aspects or only peripheral points?\n",
        "\n",
        "        Scoring rules:\n",
        "        - score = 1 (verdict='yes') if document contains relevant statements\n",
        "        - score = 0 (verdict='no') if no relevant statements or contradictory facts\n",
        "\n",
        "        For partial coverage, prefer score=1 but lower confidence and explain gaps.\n",
        "        \"\"\"\n",
        "\n",
        "        # Corrected Input Schema for StructuredTool\n",
        "        class GradeInput(BaseModel):\n",
        "            question: str = Field(..., description=\"The user's question.\")\n",
        "            documents: List[LangchainDocument] = Field(..., description=\"The documents to grade.\")\n",
        "            context: str = Field(..., description=\"Additional context if available.\")\n",
        "\n",
        "        structured_llm_grader = llm.with_structured_output(RetrievalGrade)\n",
        "        grade_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"USER QUESTION:\\n{question}\\n\\nDOCUMENTS:\\n{documents}\\n\\nCONTEXT:\\n{context}\"),\n",
        "        ])\n",
        "\n",
        "        # Need to wrap this in a function that takes the GradeInput and calls the chain\n",
        "        def grade_documents_sync(question: str, documents: List[LangchainDocument], context: str = \"\") -> RetrievalGrade:\n",
        "            doc_contents = [doc.page_content for doc in documents]\n",
        "            return (grade_prompt | structured_llm_grader).invoke({\"question\": question, \"documents\": doc_contents, \"context\": context})\n",
        "\n",
        "        async def grade_documents_async(question: str, documents: List[LangchainDocument], context: str = \"\") -> RetrievalGrade:\n",
        "             doc_contents = [doc.page_content for doc in documents]\n",
        "             return await (grade_prompt | structured_llm_grader).ainvoke({\"question\": question, \"documents\": doc_contents, \"context\": context})\n",
        "\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "             func=grade_documents_sync,\n",
        "             coroutine=grade_documents_async,\n",
        "             name=\"document_grader\",\n",
        "             description=\"Grades the relevance of documents to a user question.\",\n",
        "             args_schema=GradeInput,\n",
        "        )\n",
        "\n",
        "\n",
        "    def setup_query_rewriter(self, llm: Any):\n",
        "        \"\"\"Setup query rewriter for generating subqueries.\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert search query generator. Your task is to analyze a user's question \"\n",
        "            \"and generate a list of distinct search queries that will help find more comprehensive \"\n",
        "            \"and relevant information. Generate only the most useful and focused queries, and keep the list concise.\"\n",
        "        )\n",
        "\n",
        "        structured_llm_rewriter = llm.with_structured_output(SubqueriesOutput)\n",
        "        rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"Question: {question}\"),\n",
        "        ])\n",
        "\n",
        "        # Wrap in a function for StructuredTool\n",
        "        def rewrite_query_sync(question: str) -> SubqueriesOutput:\n",
        "            return (rewrite_prompt | structured_llm_rewriter).invoke({\"question\": question})\n",
        "\n",
        "        async def rewrite_query_async(question: str) -> SubqueriesOutput:\n",
        "            return await (rewrite_prompt | structured_llm_rewriter).ainvoke({\"question\": question})\n",
        "\n",
        "        class RewriteInput(BaseModel):\n",
        "            question: str = Field(..., description=\"The question to rewrite.\")\n",
        "\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "            func=rewrite_query_sync,\n",
        "            coroutine=rewrite_query_async,\n",
        "            name=\"query_rewriter\",\n",
        "            description=\"Generates optimized search queries.\",\n",
        "            args_schema=RewriteInput,\n",
        "        )\n",
        "\n",
        "\n",
        "    def setup_reflection_tool(self, llm:Any):\n",
        "        \"\"\"Setup reflection tool for assessing document quality.\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert document assessor. Evaluate whether the gathered documents \"\n",
        "            \"provide sufficient information to answer the user's question comprehensively. \"\n",
        "            \"Consider relevance, completeness, and quality of information.\"\n",
        "            \"Return 'refine' if more searching is needed, or 'sufficient' if the results are adequate.\"\n",
        "        )\n",
        "\n",
        "        # Corrected Input Schema for StructuredTool\n",
        "        class ReflectionInput(BaseModel):\n",
        "            question: str = Field(..., description=\"The user's question.\")\n",
        "            documents: List[LangchainDocument] = Field(..., description=\"The documents to assess.\")\n",
        "\n",
        "\n",
        "        structured_llm_reflection = llm.with_structured_output(ReflectionOutput)\n",
        "        reflection_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"Question: {question}\\n\\nDocuments: {documents}\\n\\nAssess whether these documents are sufficient.\"),\n",
        "        ])\n",
        "\n",
        "        # Wrap in a function for StructuredTool\n",
        "        def reflect_on_documents_sync(question: str, documents: List[LangchainDocument]) -> ReflectionOutput:\n",
        "             doc_contents = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "             return (reflection_prompt | structured_llm_reflection).invoke({\"question\": question, \"documents\": doc_contents})\n",
        "\n",
        "        async def reflect_on_documents_async(question: str, documents: List[LangchainDocument]) -> ReflectionOutput:\n",
        "            doc_contents = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "            return await (reflection_prompt | structured_llm_reflection).ainvoke({\"question\": question, \"documents\": doc_contents})\n",
        "\n",
        "        return StructuredTool.from_function(\n",
        "            func=reflect_on_documents_sync,\n",
        "            coroutine=reflect_on_documents_async,\n",
        "            name=\"document_reflector\",\n",
        "            description=\"Assesses the quality of gathered documents.\",\n",
        "            args_schema=ReflectionInput,\n",
        "        )\n",
        "\n",
        "\n",
        "    # =============================================================================\n",
        "    # NODE IMPLEMENTATIONS - Corrected to return proper state updates and use instance attributes\n",
        "    # =============================================================================\n",
        "\n",
        "    async def kb_retrieve_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for initial document retrieval from the vector store.\"\"\"\n",
        "        logger.info(\"---EXECUTING RETRIEVAL NODE---\")\n",
        "\n",
        "        # Get the query from state\n",
        "        query = state.get(\"query\", \"\")\n",
        "        if not query and state.get(\"messages\"):\n",
        "            query = state[\"messages\"][-1].content\n",
        "\n",
        "        # Check if retriever was successfully initialized\n",
        "        if not hasattr(self, 'retriever') or self.retriever is None:\n",
        "             logger.error(\"Retriever not initialized.\")\n",
        "             return {\n",
        "                \"error_message\": \"Retriever not initialized.\",\n",
        "                \"kb_lookup_status\": \"miss\",\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True,\n",
        "                \"query\": query,\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            # Use the instance retriever\n",
        "            retrieved_documents: List[LangchainDocument] = await self.retriever.ainvoke(query)\n",
        "\n",
        "            # Filter documents\n",
        "            retrieved_documents = [\n",
        "                doc for doc in retrieved_documents\n",
        "                if isinstance(doc, LangchainDocument) and hasattr(doc, \"page_content\") and doc.page_content.strip()\n",
        "            ]\n",
        "\n",
        "            # Check if documents found\n",
        "            if not retrieved_documents:\n",
        "                logger.info(\"KB Agent: No relevant documents found in the vector store.\")\n",
        "                return {\n",
        "                    \"kb_lookup_status\": \"miss\",\n",
        "                    \"web_search_required\": True,\n",
        "                    \"escalation_required\": True,\n",
        "                    \"query\": query,\n",
        "                    \"retrieved_documents\": [] # Ensure documents list is empty\n",
        "                }\n",
        "\n",
        "            logger.info(f\"Retrieved {len(retrieved_documents)} documents from KB.\")\n",
        "            return {\n",
        "                \"kb_lookup_status\": \"hit\",\n",
        "                \"web_search_required\": False,\n",
        "                \"escalation_required\": False,\n",
        "                \"query\": query,\n",
        "                \"retrieved_documents\": retrieved_documents\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"KB lookup error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"KB lookup failed: {e}\",\n",
        "                \"kb_lookup_status\": \"miss\",\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True,\n",
        "                \"query\": query,\n",
        "                \"retrieved_documents\": [] # Ensure documents list is empty on error\n",
        "            }\n",
        "\n",
        "    async def kb_grade_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Grade the retrieved documents for relevance.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Evaluating Retrieval ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        documents = state.get(\"retrieved_documents\", [])\n",
        "\n",
        "        if not documents:\n",
        "            logger.warning(\"No documents to grade.\")\n",
        "            return {\n",
        "                \"retrieval_grade\": {\"score\": 0, \"confidence\": 0.0, \"verdict\": \"no\", \"rationale\": \"No documents to grade.\"}, # Return a default grade\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True, # Escalate if no docs were found\n",
        "            }\n",
        "\n",
        "        # Check if grader tool was successfully initialized\n",
        "        if not hasattr(self, 'grader') or self.grader is None:\n",
        "             logger.error(\"Grader tool not initialized.\")\n",
        "             return {\n",
        "                \"error_message\": \"Grader tool not initialized.\",\n",
        "                \"retrieval_grade\": {\"score\": 0, \"confidence\": 0.0, \"verdict\": \"no\", \"rationale\": \"Grader tool not initialized.\"},\n",
        "                \"web_search_required\": True,\n",
        "                \"escalation_required\": True,\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            # Use the instance grader tool with ainvoke\n",
        "            # Pass arguments as a dictionary matching the StructuredTool's args_schema\n",
        "            retrieval_grade_output: RetrievalGrade = await self.grader.ainvoke({\n",
        "                \"question\": query,\n",
        "                \"documents\": documents, # Pass LangchainDocument list\n",
        "                \"context\": \"\" # No additional context for this step\n",
        "            })\n",
        "\n",
        "            # Determine if web search is needed based on grade\n",
        "            # Assuming score=0 or low confidence indicates need for search\n",
        "            web_search_needed = (\n",
        "                retrieval_grade_output.score == 0 or\n",
        "                retrieval_grade_output.confidence < 0.7 or\n",
        "                retrieval_grade_output.verdict.lower() == \"no\"\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Retrieval Grade: Score={retrieval_grade_output.score}, Verdict='{retrieval_grade_output.verdict}', Confidence={retrieval_grade_output.confidence}\")\n",
        "\n",
        "            return {\n",
        "                \"retrieval_grade\": retrieval_grade_output.model_dump(), # Store Pydantic model as dictionary in state\n",
        "                \"web_search_required\": web_search_needed,\n",
        "                \"escalation_required\": web_search_needed # Escalate if initial retrieval was poor\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Grading error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"Grading failed: {e}\",\n",
        "                \"retrieval_grade\": {\"score\": 0, \"confidence\": 0.0, \"verdict\": \"error\", \"rationale\": f\"Grading failed: {e}\"}, # Indicate error in grade\n",
        "                \"web_search_required\": True, # Assume web search is needed on grading error\n",
        "                \"escalation_required\": True,\n",
        "            }\n",
        "\n",
        "    async def kb_update_iteration_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Update iteration counter.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Updating Iteration ---\")\n",
        "        current_iterations = state.get(\"iterations\", 0)\n",
        "        return {\"iterations\": current_iterations + 1}\n",
        "\n",
        "    async def kb_rewrite_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Rewrite query for better search results.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Rewriting Query ---\")\n",
        "\n",
        "        original_query = state.get(\"query\", \"\")\n",
        "\n",
        "        if not original_query:\n",
        "            logger.warning(\"No question found in state. Skipping query rewriting.\")\n",
        "            return {\n",
        "                \"subqueries\": [original_query if original_query else \"\"] # Ensure at least empty string if no query\n",
        "            }\n",
        "\n",
        "        # Check if query_rewriter tool was successfully initialized\n",
        "        if not hasattr(self, 'query_rewriter') or self.query_rewriter is None:\n",
        "             logger.error(\"Query rewriter tool not initialized.\")\n",
        "             return {\n",
        "                 \"error_message\": \"Query rewriter tool not initialized.\",\n",
        "                 \"subqueries\": [original_query] # Fallback to original query\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Rewriting query for: '{original_query}'\")\n",
        "            # Use the instance query_rewriter tool with ainvoke\n",
        "            subquery_output: SubqueriesOutput = await self.query_rewriter.ainvoke({\"question\": original_query})\n",
        "\n",
        "            subqueries = subquery_output.queries\n",
        "            # Ensure original query is included if subqueries list is empty\n",
        "            if not subqueries and original_query:\n",
        "                subqueries = [original_query]\n",
        "\n",
        "            logger.info(f\"Generated subqueries: {subqueries}\")\n",
        "\n",
        "            return {\n",
        "                \"subqueries\": subqueries\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Query rewriting error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"Query rewriting failed: {e}\",\n",
        "                \"subqueries\": [original_query] # Fallback to original query on error\n",
        "            }\n",
        "\n",
        "    # Placeholder for kb_web_search_node - Needs actual implementation\n",
        "    # This node would likely use external search tools (like Tavily, Serper, DDG)\n",
        "    # and potentially a web crawler to get page content.\n",
        "    async def kb_web_search_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for performing web search using generated subqueries.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Performing Web Search ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        subqueries = state.get(\"subqueries\", [])\n",
        "        # Add search tool usage here (e.g., Tavily, Serper, DDG)\n",
        "        # For now, returning empty crawl_results as a placeholder\n",
        "        # logger.warning(\"Web search node is a placeholder and not implemented.\")\n",
        "        return {\n",
        "             \"crawl_results\": [],\n",
        "             \"web_search_required\": False, # Mark web search as done (or failed)\n",
        "             \"final_answer\": \"External web search is not available in this environment.\",\n",
        "             \"error_message\": state.get(\"error_message\") or \"Web search node is a placeholder.\"\n",
        "        }\n",
        "\n",
        "    async def kb_rerank_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for reranking retrieved and crawled documents.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Reranking Documents ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        # Combine documents from different sources\n",
        "        all_documents = state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", [])\n",
        "\n",
        "        if not all_documents:\n",
        "            logger.warning(\"No documents to rerank.\")\n",
        "            return {\"rerank_result\": []} # Return empty list if no documents\n",
        "\n",
        "        # Check if reranker tool was successfully initialized and if reranking is enabled in state\n",
        "        use_reranker = state.get(\"use_reranker\", True) # Default to True if not specified in state\n",
        "        if not hasattr(self, 'reranker_tool') or self.reranker_tool is None or not use_reranker:\n",
        "             logger.warning(\"Reranker tool not initialized or reranking disabled. Skipping reranking.\")\n",
        "             # If reranking is skipped, the 'documents' key should still hold the combined documents\n",
        "             # for the next node (compression or generation)\n",
        "             return {\"rerank_result\": all_documents} # Return original combined docs\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Reranking {len(all_documents)} documents.\")\n",
        "            # Use the instance reranker_tool with ainvoke\n",
        "            # The reranker_tool.ainvoke expects {\"query\": ..., \"documents\": ..., \"top_n\": ...}\n",
        "            reranked_docs: List[LangchainDocument] = await self.reranker_tool.ainvoke({\n",
        "                \"query\": query,\n",
        "                \"documents\": all_documents,\n",
        "                \"top_n\": 5 # Define top_n for reranking\n",
        "            })\n",
        "\n",
        "            logger.info(f\"Reranked to {len(reranked_docs)} documents.\")\n",
        "            return {\"rerank_result\": reranked_docs}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reranking error: {e}\")\n",
        "            # Return original combined documents on error\n",
        "            return {\n",
        "                \"error_message\": f\"Reranking failed: {e}\",\n",
        "                \"rerank_result\": all_documents\n",
        "            }\n",
        "\n",
        "\n",
        "    async def kb_compress_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for compressing the top documents using an LLM.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Compressing Documents ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        # Use the reranked documents if available, otherwise the combined documents\n",
        "        documents_to_compress = state.get(\"rerank_result\", []) or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", []))\n",
        "\n",
        "        if not documents_to_compress:\n",
        "            logger.warning(\"No documents to compress.\")\n",
        "            return {\"compress_result\": []} # Return empty list if no documents\n",
        "\n",
        "        # Check if compressor tool was successfully initialized and if compression is enabled in state\n",
        "        use_compressor = state.get(\"use_compressor\", True) # Default to True if not specified\n",
        "        if not hasattr(self, 'compressor_tool') or self.compressor_tool is None or not use_compressor:\n",
        "            logger.warning(\"Compressor tool not initialized or compression disabled. Skipping compression.\")\n",
        "            # If compression is skipped, the 'compress_result' key should hold the full text of the documents\n",
        "            # or this node could be skipped entirely via routing.\n",
        "            # For now, let's return the page_content of the documents as if they were \"compressed\" to their full text.\n",
        "            return {\"compress_result\": [doc.page_content for doc in documents_to_compress]}\n",
        "\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Compressing {len(documents_to_compress)} documents.\")\n",
        "            # Use the instance compressor_tool with ainvoke\n",
        "            # The compressor_tool.ainvoke expects {\"query\": ..., \"documents\": ...}\n",
        "            compressed_docs: List[LangchainDocument] = await self.compressor_tool.ainvoke({\n",
        "                \"query\": query,\n",
        "                \"documents\": documents_to_compress\n",
        "            })\n",
        "\n",
        "            # Extract page_content to match the GraphState definition for compress_result (List[str])\n",
        "            compressed_text_list = [doc.page_content for doc in compressed_docs]\n",
        "            logger.info(f\"Compressed to {len(compressed_text_list)} snippets.\")\n",
        "\n",
        "            return {\"compress_result\": compressed_text_list}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Compression error: {e}\")\n",
        "            # Return original document content on error\n",
        "            return {\n",
        "                \"error_message\": f\"Compression failed: {e}\",\n",
        "                \"compress_result\": [doc.page_content for doc in documents_to_compress]\n",
        "            }\n",
        "\n",
        "\n",
        "    async def kb_assess_and_reflect_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Assess document quality and decide next steps.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Assessing and Reflecting ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        # Use compressed result if available, otherwise combined documents\n",
        "        documents_to_assess = state.get(\"compress_result\", []) or [doc.page_content for doc in (state.get(\"rerank_result\", []) or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", [])))]\n",
        "\n",
        "        if not documents_to_assess:\n",
        "            logger.warning(\"No documents to assess.\")\n",
        "            return {\n",
        "                 \"reflection_output\": {\"decision\": \"sufficient\", \"assessment\": \"No documents to assess.\", \"confidence\": 1.0},\n",
        "                 \"reflection_decision\": \"sufficient\"\n",
        "            }\n",
        "\n",
        "        # Check if reflection tool was successfully initialized\n",
        "        if not hasattr(self, 'reflection_tool') or self.reflection_tool is None:\n",
        "             logger.error(\"Reflection tool not initialized.\")\n",
        "             return {\n",
        "                \"error_message\": \"Reflection tool not initialized.\",\n",
        "                \"reflection_output\": {\"decision\": \"error\", \"assessment\": \"Reflection tool not initialized.\", \"confidence\": 0.0},\n",
        "                \"reflection_decision\": \"sufficient\", # Default to sufficient on tool error\n",
        "             }\n",
        "\n",
        "        try:\n",
        "            logger.info(\"Assessing documents with reflection tool.\")\n",
        "            # Use the instance reflection_tool with ainvoke\n",
        "            # The reflection_tool.ainvoke expects {\"question\": ..., \"documents\": ...}\n",
        "            reflection_result: ReflectionOutput = await self.reflection_tool.ainvoke({\n",
        "                \"question\": query,\n",
        "                \"documents\": documents_to_assess # Pass the list of strings (compressed result) or page_content\n",
        "            })\n",
        "\n",
        "            decision = reflection_result.decision\n",
        "            logger.info(f\"Reflection Decision: '{decision}'\")\n",
        "\n",
        "            return {\n",
        "                \"reflection_output\": reflection_result.model_dump(), # Store Pydantic model as dictionary\n",
        "                \"reflection_decision\": decision\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Reflection error: {e}\")\n",
        "            return {\n",
        "                \"error_message\": f\"Reflection failed: {e}\",\n",
        "                 \"reflection_output\": {\"decision\": \"error\", \"assessment\": f\"Reflection failed: {e}\", \"confidence\": 0.0},\n",
        "                \"reflection_decision\": \"sufficient\"  # Default to sufficient on error\n",
        "            }\n",
        "\n",
        "    def _state_as_context(state: KBState) -> str:\n",
        "        \"\"\"Convert agent state into readable context for the LLM.\"\"\"\n",
        "        state_lines = []\n",
        "        for key, value in state.items():\n",
        "            if isinstance(value, list):\n",
        "                state_lines.append(f\"{key}: {len(value)} items\")\n",
        "            elif isinstance(value, dict):\n",
        "                state_lines.append(f\"{key}: {list(value.keys())}\")\n",
        "            else:\n",
        "                state_lines.append(f\"{key}: {str(value)[:200]}\")  # truncate if long\n",
        "        return \"\\n\".join(state_lines)\n",
        "\n",
        "    def _docs_as_context(docs: List[LangchainDocument]) -> str:\n",
        "        context_docs = []\n",
        "        for i, doc in enumerate(docs, start=1):\n",
        "            title = doc.metadata.get(\"title\", \"Unknown Title\")\n",
        "            source = doc.metadata.get(\"source\", \"Unknown Source\")\n",
        "            date = doc.metadata.get(\"date\", \"Unknown Date\")\n",
        "            context_docs.append(\n",
        "                f\"Document {i}:\\n\"\n",
        "                f\"Title: {title}\\nSource: {source}\\nDate: {date}\\n\"\n",
        "                f\"Content: {doc.page_content[:500]}...\\n\"\n",
        "            )\n",
        "        return \"\\n\\n\".join(context_docs)\n",
        "\n",
        "\n",
        "    # async def kb_generate_answer_node(self, state: KBState) -> Dict[str, Any]:\n",
        "    #     \"\"\"Node for generating the final answer using the LLM, state, and retrieved context.\"\"\"\n",
        "    #     logger.info(\"--- Knowledge Base Agent Node: Generating Answer ---\")\n",
        "\n",
        "    #     query = state.get(\"query\", \"\")\n",
        "    #     if not query and state.get(\"messages\"):\n",
        "    #         query = state[\"messages\"][-1].content\n",
        "\n",
        "    #     # Check if LLM was successfully initialized\n",
        "    #     if not hasattr(self, 'llm') or self.llm is None:\n",
        "    #         logger.error(\"LLM not initialized.\")\n",
        "    #         return {\n",
        "    #             \"final_answer\": \"Error: Language model not initialized.\",\n",
        "    #             \"error_message\": \"LLM not initialized.\"\n",
        "    #         }\n",
        "\n",
        "    #     try:\n",
        "    #         # Prefer compressed results; fallback to reranked/retrieved/crawled docs\n",
        "    #         context_docs = state.get(\"compress_result\")  # Already List[str] if compression worked\n",
        "    #         if not context_docs:\n",
        "    #             combined_docs = (\n",
        "    #                 state.get(\"rerank_result\", [])\n",
        "    #                 or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", []))\n",
        "    #             )\n",
        "    #             # Enrich docs with metadata if available\n",
        "    #             context_docs = []\n",
        "    #             for i, doc in enumerate(combined_docs, start=1):\n",
        "    #                 title = getattr(doc, \"metadata\", {}).get(\"title\", \"Unknown Title\")\n",
        "    #                 source = getattr(doc, \"metadata\", {}).get(\"source\", \"Unknown Source\")\n",
        "    #                 date = getattr(doc, \"metadata\", {}).get(\"date\", \"Unknown Date\")\n",
        "    #                 content = getattr(doc, \"page_content\", str(doc))[:500]  # truncate long docs\n",
        "    #                 context_docs.append(\n",
        "    #                     f\"Document {i}:\\nTitle: {title}\\nSource: {source}\\nDate: {date}\\nContent: {content}\"\n",
        "    #                 )\n",
        "\n",
        "    #         # Build state summary for LLM self-awareness\n",
        "    #         state_summary = []\n",
        "    #         for key, value in state.items():\n",
        "    #             if isinstance(value, list):\n",
        "    #                 state_summary.append(f\"{key}: {len(value)} items\")\n",
        "    #             elif isinstance(value, dict):\n",
        "    #                 state_summary.append(f\"{key}: {list(value.keys())}\")\n",
        "    #             else:\n",
        "    #                 state_summary.append(f\"{key}: {str(value)[:200]}\")  # truncate long values\n",
        "    #         state_context = \"\\n\".join(state_summary)\n",
        "\n",
        "    #         # Merge into final context\n",
        "    #         context = \"\\n\\n\".join(context_docs)\n",
        "\n",
        "    #         prompt_template = f\"\"\"\n",
        "    #         You are an assistant that answers strictly from the provided context.\n",
        "\n",
        "    #         <agent_state>\n",
        "    #         {state_context if state_context.strip() else \"No state information available.\"}\n",
        "    #         </agent_state>\n",
        "\n",
        "    #         <documents>\n",
        "    #         {context if context.strip() else \"No relevant documents found.\"}\n",
        "    #         </documents>\n",
        "\n",
        "    #         <question>\n",
        "    #         {query}\n",
        "    #         </question>\n",
        "\n",
        "    #         Instructions:\n",
        "    #         - If the question is about chapters, titles, or sources → use <documents>.\n",
        "    #         - If the question is about current state, retrieved docs, or workflow → use <agent_state>.\n",
        "    #         - If the answer cannot be found in either, reply exactly: \"I cannot answer from the provided context.\"\n",
        "    #         \"\"\"\n",
        "\n",
        "    #         # Generate response using the instance llm\n",
        "    #         response = await self.llm.ainvoke(prompt_template)\n",
        "\n",
        "    #         logger.info(\"Answer generated.\")\n",
        "    #         return {\"final_answer\": response.content}\n",
        "\n",
        "    #     except Exception as e:\n",
        "    #         logger.error(f\"Generation error: {e}\")\n",
        "    #         return {\n",
        "    #             \"final_answer\": f\"Error generating answer: {e}\",\n",
        "    #             \"error_message\": f\"Generation failed: {e}\"\n",
        "    #         }\n",
        "    # async def kb_generate_answer_node(self, state: KBState) -> Dict[str, Any]:\n",
        "    #     \"\"\"Node for generating the final answer using the LLM, state, and retrieved context.\"\"\"\n",
        "    #     logger.info(\"--- Knowledge Base Agent Node: Generating Answer ---\")\n",
        "\n",
        "    #     query = state.get(\"query\", \"\")\n",
        "    #     if not query and state.get(\"messages\"):\n",
        "    #         query = state[\"messages\"][-1].content\n",
        "\n",
        "    #     # Handle greetings directly\n",
        "    #     greetings = {\"hello\", \"hi\", \"hey\", \"good morning\", \"good evening\"}\n",
        "    #     if query.lower().strip() in greetings:\n",
        "    #         return {\"final_answer\": \"Hello! I’m here and ready to help. What would you like to know?\"}\n",
        "\n",
        "    #     # Handle explicit state requests with trigger phrase\n",
        "    #     if \"#state_dump\" in query.lower():\n",
        "    #         state_summary = []\n",
        "    #         for key, value in state.items():\n",
        "    #             if isinstance(value, list):\n",
        "    #                 state_summary.append(f\"{key}: {len(value)} items\")\n",
        "    #             elif isinstance(value, dict):\n",
        "    #                 state_summary.append(f\"{key}: {list(value.keys())}\")\n",
        "    #             else:\n",
        "    #                 state_summary.append(f\"{key}: {str(value)[:200]}\")\n",
        "    #         return {\"final_answer\": \"Here is my detailed internal state:\\n\\n\" + \"\\n\".join(state_summary)}\n",
        "\n",
        "    #     # If casually asked \"what is your state?\" → soft answer\n",
        "    #     if \"state\" in query.lower():\n",
        "    #         return {\"final_answer\": \"I’m active, holding some documents in memory, and ready to continue assisting you.\"}\n",
        "\n",
        "    #     # Fallback to normal document/context answering\n",
        "    #     try:\n",
        "    #         context_docs = state.get(\"compress_result\")\n",
        "    #         if not context_docs:\n",
        "    #             combined_docs = (\n",
        "    #                 state.get(\"rerank_result\", [])\n",
        "    #                 or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", []))\n",
        "    #             )\n",
        "    #             context_docs = []\n",
        "    #             for i, doc in enumerate(combined_docs, start=1):\n",
        "    #                 title = getattr(doc, \"metadata\", {}).get(\"title\", \"Unknown Title\")\n",
        "    #                 source = getattr(doc, \"metadata\", {}).get(\"source\", \"Unknown Source\")\n",
        "    #                 date = getattr(doc, \"metadata\", {}).get(\"date\", \"Unknown Date\")\n",
        "    #                 content = getattr(doc, \"page_content\", str(doc))[:500]\n",
        "    #                 context_docs.append(\n",
        "    #                     f\"Document {i}:\\nTitle: {title}\\nSource: {source}\\nDate: {date}\\nContent: {content}\"\n",
        "    #                 )\n",
        "\n",
        "    #         context = \"\\n\\n\".join(context_docs)\n",
        "\n",
        "    #         prompt_template = f\"\"\"\n",
        "    #         You are an assistant that answers strictly from the provided context.\n",
        "\n",
        "    #         <documents>\n",
        "    #         {context if context.strip() else \"No relevant documents found.\"}\n",
        "    #         </documents>\n",
        "\n",
        "    #         <question>\n",
        "    #         {query}\n",
        "    #         </question>\n",
        "\n",
        "    #         Instructions:\n",
        "    #         - Answer clearly and directly if the context contains the answer.\n",
        "    #         - If the answer is not in the context, reply exactly: \"I cannot answer from the provided context.\"\n",
        "    #         - Do not expose your full internal state unless the query includes '#state_dump'.\n",
        "    #         \"\"\"\n",
        "\n",
        "    #         response = await self.llm.ainvoke(prompt_template)\n",
        "    #         logger.info(\"Answer generated.\")\n",
        "    #         return {\"final_answer\": response.content}\n",
        "\n",
        "    #     except Exception as e:\n",
        "    #         logger.error(f\"Generation error: {e}\")\n",
        "    #         return {\n",
        "    #             \"final_answer\": f\"Error generating answer: {e}\",\n",
        "    #             \"error_message\": f\"Generation failed: {e}\"\n",
        "    #         }\n",
        "\n",
        "    async def kb_generate_answer_node(self, state: KBState) -> Dict[str, Any]:\n",
        "        \"\"\"Node for generating the final answer using the LLM, state, and retrieved context.\"\"\"\n",
        "        logger.info(\"--- Knowledge Base Agent Node: Generating Answer ---\")\n",
        "\n",
        "        query = state.get(\"query\", \"\")\n",
        "        if not query and state.get(\"messages\"):\n",
        "            query = state[\"messages\"][-1].content\n",
        "\n",
        "        # Handle greetings directly\n",
        "        greetings = {\"hello\", \"hi\", \"hey\", \"good morning\", \"good evening\"}\n",
        "        if query.lower().strip() in greetings:\n",
        "            return {\"final_answer\": \"Hello! I’m here and ready to help. What would you like to know?\"}\n",
        "\n",
        "        # Handle explicit state dump request\n",
        "        if \"#state_dump\" in query.lower():\n",
        "            state_summary = []\n",
        "            for key, value in state.items():\n",
        "                if isinstance(value, list):\n",
        "                    state_summary.append(f\"{key}: {len(value)} items\")\n",
        "                elif isinstance(value, dict):\n",
        "                    state_summary.append(f\"{key}: {list(value.keys())}\")\n",
        "                else:\n",
        "                    state_summary.append(f\"{key}: {str(value)[:200]}\")\n",
        "            return {\n",
        "                \"final_answer\": \"Here is my detailed internal state:\\n\\n\" + \"\\n\".join(state_summary),\n",
        "                \"sources\": [],\n",
        "                \"reasoning_trace\": \"Direct state dump requested.\",\n",
        "                \"search_outputs\": state.get(\"search_outputs\", []),\n",
        "            }\n",
        "\n",
        "        # Handle casual \"what is your state?\"\n",
        "        if \"state\" in query.lower():\n",
        "            return {\n",
        "                \"final_answer\": \"I’m active, holding some documents in memory, and ready to continue assisting you.\",\n",
        "                \"sources\": [],\n",
        "                \"reasoning_trace\": \"User casually asked about agent state.\",\n",
        "                \"search_outputs\": state.get(\"search_outputs\", []),\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Collect documents from compression or fallback\n",
        "            context_docs = state.get(\"compress_result\")\n",
        "            combined_docs = (\n",
        "                state.get(\"rerank_result\", [])\n",
        "                or (state.get(\"retrieved_documents\", []) + state.get(\"crawl_results\", []))\n",
        "            )\n",
        "\n",
        "            if not context_docs:\n",
        "                context_docs = []\n",
        "                for i, doc in enumerate(combined_docs, start=1):\n",
        "                    title = doc.metadata.get(\"title\", \"Unknown Title\")\n",
        "                    source = doc.metadata.get(\"source\", \"Unknown Source\")\n",
        "                    date = doc.metadata.get(\"date\", \"Unknown Date\")\n",
        "                    content = getattr(doc, \"page_content\", str(doc))[:500]\n",
        "                    context_docs.append(\n",
        "                        f\"Document {i}:\\nTitle: {title}\\nSource: {source}\\nDate: {date}\\nContent: {content}\"\n",
        "                    )\n",
        "\n",
        "            context = \"\\n\\n\".join(context_docs)\n",
        "\n",
        "            prompt_template = f\"\"\"\n",
        "            You are an assistant that answers strictly from the provided context.\n",
        "\n",
        "            <documents>\n",
        "            {context if context.strip() else \"No relevant documents found.\"}\n",
        "            </documents>\n",
        "\n",
        "            <question>\n",
        "            {query}\n",
        "            </question>\n",
        "\n",
        "            Instructions:\n",
        "            - Answer clearly and directly if the context contains the answer.\n",
        "            - If the answer is not in the context, reply exactly: \"I cannot answer from the provided context.\"\n",
        "            - Do not expose your full internal state unless the query includes '#state_dump'.\n",
        "            \"\"\"\n",
        "\n",
        "            response = await self.llm.ainvoke(prompt_template)\n",
        "            logger.info(\"Answer generated.\")\n",
        "\n",
        "            # Normalize sources\n",
        "            sources = []\n",
        "            for doc in combined_docs:\n",
        "                sources.append({\n",
        "                    \"title\": doc.metadata.get(\"title\", \"Unknown Title\"),\n",
        "                    \"source\": doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"date\": doc.metadata.get(\"date\", \"Unknown Date\"),\n",
        "                    \"document_id\": getattr(doc, \"id\", None) or doc.metadata.get(\"document_id\")\n",
        "                })\n",
        "\n",
        "            # Build reasoning trace\n",
        "            reasoning_trace = []\n",
        "            if state.get(\"kb_lookup_status\"):\n",
        "                reasoning_trace.append(f\"KB lookup: {state['kb_lookup_status']}\")\n",
        "            if state.get(\"retrieval_grade\"):\n",
        "                reasoning_trace.append(f\"Retrieval grade: {state['retrieval_grade']}\")\n",
        "            if state.get(\"reflection_output\"):\n",
        "                reasoning_trace.append(f\"Reflection: {state['reflection_output']}\")\n",
        "            if state.get(\"search_outputs\"):\n",
        "                reasoning_trace.append(f\"Search steps: {len(state['search_outputs'])} performed\")\n",
        "\n",
        "            return {\n",
        "                \"final_answer\": response.content,\n",
        "                \"sources\": sources,\n",
        "                \"reasoning_trace\": \" → \".join(reasoning_trace) if reasoning_trace else \"No reasoning trace available.\",\n",
        "                \"search_outputs\": [so.dict() for so in state.get(\"search_outputs\", [])],\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Generation error: {e}\")\n",
        "            return {\n",
        "                \"final_answer\": f\"Error generating answer: {e}\",\n",
        "                \"error_message\": f\"Generation failed: {e}\",\n",
        "                \"sources\": [],\n",
        "                \"reasoning_trace\": f\"Error during generation: {e}\",\n",
        "                \"search_outputs\": state.get(\"search_outputs\", []),\n",
        "            }\n",
        "\n",
        "\n",
        "    # =============================================================================\n",
        "    # ROUTING FUNCTIONS - Corrected to use proper conditional logic and state keys\n",
        "    # =============================================================================\n",
        "    @staticmethod\n",
        "    def generate_fallback_reply(state: \"KBState\") -> str:\n",
        "        \"\"\"\n",
        "        Generate a smart fallback when no sufficient answer was found,\n",
        "        citing the reasoning trace.\n",
        "        \"\"\"\n",
        "        query = state.get(\"query\", \"\").strip() or \"your request\"\n",
        "        logger.info(f\"--- Generating Fallback Reply for Query: '{query}' ---\")\n",
        "\n",
        "            # Case 1: Knowledge Base miss (no retrieval attempt or failed lookup)\n",
        "        if state.get(\"kb_lookup_status\") == \"miss\" and not state.get(\"search_outputs\"):\n",
        "            return (\n",
        "                f'I understood your request about \"{query}\", but '\n",
        "                \"I couldn’t find anything relevant in my knowledge base.\"\n",
        "            )\n",
        "        # If we have a reasoning trace with steps, compress into a breadcrumb string\n",
        "        reasoning = None\n",
        "        if state.get(\"reasoning_trace\") and getattr(state[\"reasoning_trace\"], \"steps\", None):\n",
        "            steps = state[\"reasoning_trace\"].steps\n",
        "            # compress steps into: \"KB miss → shallow search insufficient → deep crawl error\"\n",
        "            reasoning = \" → \".join(\n",
        "                f\"{step.step_type.replace('_', ' ')} {step.outcome}\"\n",
        "                for step in steps\n",
        "            )\n",
        "\n",
        "        # Build the fallback message\n",
        "        if reasoning:\n",
        "            return (\n",
        "                f'I looked for information about \"{query}\", but I couldn’t find enough context.\\n'\n",
        "                f'Reasoning path: {reasoning}.'\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                f'I looked for information about \"{query}\", but I don’t have enough context to answer right now.'\n",
        "            )\n",
        "\n",
        "    def agent_reply_node(self, state: \"KBState\") -> str:\n",
        "        if state.get(\"final_answer\"):\n",
        "            return state[\"final_answer\"]\n",
        "        if state.get(\"error_message\"):\n",
        "            return f\"⚠️ An error occurred: {state['error_message']}\"\n",
        "        return self.generate_fallback_reply(state)\n",
        "\n",
        "\n",
        "    def route_after_lookup(self, state: KBState) -> str:\n",
        "        \"\"\"Route after KB lookup based on status and grade.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after KB Lookup (Status: {state.get('kb_lookup_status', 'N/A')}, Web Search Required: {state.get('web_search_required', 'N/A')}) ---\")\n",
        "        if state.get(\"error_message\"):\n",
        "            logger.info(\"Routing to generate due to error.\")\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.get(\"kb_lookup_status\") == \"miss\" or state.get(\"web_search_required\"):\n",
        "             logger.info(\"Routing to rewrite query for web search.\")\n",
        "             return \"kb_rewrite\" # Go to rewrite for web search if KB missed or search is required\n",
        "        else: # KB hit and no web search required from initial check\n",
        "             logger.info(\"Routing to grade retrieved documents.\")\n",
        "             return \"kb_grade\"\n",
        "\n",
        "    def route_after_grade(self, state: KBState) -> str:\n",
        "        \"\"\"Route after grading based on relevance score.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after Grade (Web Search Required: {state.get('web_search_required', 'N/A')}) ---\")\n",
        "        if state.get(\"error_message\"):\n",
        "            logger.info(\"Routing to generate due to error.\")\n",
        "            return \"kb_generate_answer\"\n",
        "        elif state.get(\"web_search_required\", False): # web_search_required set by grader\n",
        "            logger.info(\"Routing to rewrite query for web search.\")\n",
        "            return \"kb_rewrite\"\n",
        "        else:\n",
        "            logger.info(\"Routing to rerank documents.\")\n",
        "            return \"kb_rerank\"\n",
        "\n",
        "    def route_after_rewrite(self, state: KBState) -> str:\n",
        "        \"\"\"Route after rewriting query.\"\"\"\n",
        "        logger.info(\"--- ROUTING after Rewrite ---\")\n",
        "        # Assuming rewrite always leads to web search if successful, or generate on error\n",
        "        if state.get(\"error_message\"):\n",
        "             logger.info(\"Routing to generate due to error.\")\n",
        "             return \"kb_generate_answer\"\n",
        "        elif state.get(\"subqueries\"): # Check if subqueries were generated (or original query is there)\n",
        "             logger.info(\"Routing to web search.\")\n",
        "             return \"kb_web_search\"\n",
        "        else: # No subqueries generated, fallback\n",
        "             logger.info(\"No subqueries generated, routing to generate answer (possibly with limited context).\")\n",
        "             return \"kb_generate_answer\"\n",
        "\n",
        "\n",
        "    def route_after_search(self, state: KBState) -> str:\n",
        "        \"\"\"Route after web search based on results and iteration count.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after Web Search (Iterations: {state.get('iterations', 'N/A')}, Max Iterations: {state.get('max_iterations', 'N/A')}) ---\")\n",
        "        max_iterations = state.get(\"max_iterations\", 3)\n",
        "        current_iterations = state.get(\"iterations\", 0)\n",
        "        crawl_results = state.get(\"crawl_results\", [])\n",
        "\n",
        "        if state.get(\"error_message\"):\n",
        "             logger.info(\"Routing to generate due to error.\")\n",
        "             return \"kb_generate_answer\"\n",
        "        elif crawl_results and current_iterations < max_iterations:\n",
        "             # If results found and more iterations allowed, assess and reflect\n",
        "             logger.info(\"Crawl results found and iterations remaining. Routing to update iteration and assess.\")\n",
        "             return \"kb_update_iteration\" # Update iteration before assessing\n",
        "        else:\n",
        "             # If no results, max iterations reached, or error, proceed to rerank/generate\n",
        "             logger.info(\"No crawl results or max iterations reached. Routing to rerank.\")\n",
        "             return \"kb_rerank\"\n",
        "\n",
        "    def route_after_iteration(self, state: KBState) -> str:\n",
        "        \"\"\"Route after iteration update.\"\"\"\n",
        "        logger.info(\"--- ROUTING after Update Iteration ---\")\n",
        "        # After updating iteration, always assess the new set of documents\n",
        "        logger.info(\"Routing to assess and reflect.\")\n",
        "        return \"kb_assess_and_reflect\"\n",
        "\n",
        "\n",
        "    def route_after_reflection(self, state: KBState) -> str:\n",
        "        \"\"\"Route after reflection based on decision.\"\"\"\n",
        "        logger.info(f\"--- ROUTING after Reflection (Decision: {state.get('reflection_decision', 'N/A')}) ---\")\n",
        "        if state.get(\"error_message\"):\n",
        "             logger.info(\"Routing to generate due to error.\")\n",
        "             return \"kb_generate_answer\"\n",
        "        elif state.get(\"reflection_decision\") == \"refine\":\n",
        "            # If refinement needed, rewrite query and search again\n",
        "            logger.info(\"Reflection calls for refinement. Routing to rewrite query.\")\n",
        "            return \"kb_rewrite\"\n",
        "        else: # Decision is \"sufficient\"\n",
        "            # If sufficient, proceed to final processing (rerank, compress, generate)\n",
        "            logger.info(\"Reflection deems documents sufficient. Routing to rerank.\")\n",
        "            return \"kb_rerank\"\n",
        "\n",
        "    # =============================================================================\n",
        "    # GRAPH BUILDER - Corrected to use proper LangGraph patterns and instance methods\n",
        "    # =============================================================================\n",
        "\n",
        "    async def build_rag_graph(self):\n",
        "        \"\"\"Build the RAG graph with corrected syntax using instance methods.\"\"\"\n",
        "\n",
        "        # Define the state machine\n",
        "        # Use the corrected KBState\n",
        "        builder = StateGraph(KBState)\n",
        "\n",
        "        # Add nodes - corrected node names and references to instance methods\n",
        "        builder.add_node(\"kb_retrieve\", self.kb_retrieve_node)\n",
        "        builder.add_node(\"kb_grade\", self.kb_grade_node)\n",
        "        builder.add_node(\"kb_rewrite\", self.kb_rewrite_node)\n",
        "        # Assuming kb_web_search_node is implemented or placeholder exists\n",
        "        builder.add_node(\"kb_web_search\", self.kb_web_search_node) # Add web search node\n",
        "        builder.add_node(\"kb_update_iteration\", self.kb_update_iteration_node)\n",
        "        builder.add_node(\"kb_assess_and_reflect\", self.kb_assess_and_reflect_node)\n",
        "        builder.add_node(\"kb_rerank\", self.kb_rerank_node) # Add rerank node\n",
        "        builder.add_node(\"kb_compress\", self.kb_compress_node) # Add compress node\n",
        "        builder.add_node(\"kb_generate_answer\", self.kb_generate_answer_node)\n",
        "\n",
        "        # Set the entry point - corrected\n",
        "        builder.add_edge(START, \"kb_retrieve\")\n",
        "\n",
        "        # Add conditional edges with corrected routing using instance methods\n",
        "        builder.add_conditional_edges(\"kb_retrieve\", self.route_after_lookup)\n",
        "        builder.add_conditional_edges(\"kb_grade\", self.route_after_grade)\n",
        "        builder.add_conditional_edges(\"kb_rewrite\", self.route_after_rewrite) # Route after rewrite\n",
        "        builder.add_conditional_edges(\"kb_web_search\", self.route_after_search) # Route after web search\n",
        "        builder.add_edge(\"kb_update_iteration\", \"kb_assess_and_reflect\") # Update iteration always leads to assess\n",
        "        builder.add_conditional_edges(\"kb_assess_and_reflect\", self.route_after_reflection) # Route after reflection\n",
        "\n",
        "        # Connect the final processing steps\n",
        "        builder.add_edge(\"kb_rerank\", \"kb_compress\") # Rerank usually precedes compress\n",
        "        builder.add_edge(\"kb_compress\", \"kb_generate_answer\") # Compress usually precedes generate\n",
        "        builder.add_edge(\"kb_generate_answer\", END) # Final answer is the end\n",
        "\n",
        "\n",
        "        # Compile the graph\n",
        "        app = builder.compile()\n",
        "        return app"
      ],
      "metadata": {
        "id": "3Zkaly4tw2ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# USAGE EXAMPLE - Corrected invocation pattern\n",
        "# =============================================================================\n",
        "\n",
        "async def run_kb_agent_example(documents_list: List[LangchainDocument]):\n",
        "    \"\"\"Example of how to run the corrected KB agent\"\"\"\n",
        "\n",
        "    # Initialize the agent\n",
        "    # Pass documents_list during initialization\n",
        "    agent = KnowledgeBaseAgent(\n",
        "        documents_list=documents_list,\n",
        "        cache_adapter=None # Add cache adapter if you have one\n",
        "    )\n",
        "\n",
        "    # Build the graph using the instance method\n",
        "    app = await agent.build_rag_graph()\n",
        "\n",
        "    # Example invocation\n",
        "    initial_state: KBState = {\n",
        "        \"query\": \"What effect did the plasma fire have on the silver veins?\",\n",
        "        \"messages\": [HumanMessage(content=\"What effect did the plasma fire have on the silver veins?\")],\n",
        "        \"session_id\": \"test_session_1\",\n",
        "        \"iterations\": 0,\n",
        "        \"max_iterations\": 2, # Allow a couple of search iterations\n",
        "        \"retrieved_documents\": [], # Will be populated by kb_retrieve\n",
        "        \"kb_lookup_status\": \"not_attempted\",\n",
        "        \"web_search_required\": False,\n",
        "        \"escalation_required\": False,\n",
        "        \"retrieval_grade\": None,\n",
        "        \"subqueries\": [],\n",
        "        \"crawl_results\": [],\n",
        "        \"reflection_output\": None,\n",
        "        \"reflection_decision\": \"sufficient\", # Default decision\n",
        "        \"error_message\": None,\n",
        "        \"final_answer\": None,\n",
        "        \"use_reranker\": True, # Enable reranking\n",
        "        \"use_compressor\": True, # Enable compression\n",
        "        \"rerank_result\": [],\n",
        "        \"compress_result\": [],\n",
        "    }\n",
        "\n",
        "    print(\"--- Running KB Agent ---\")\n",
        "    try:\n",
        "        # Run the graph\n",
        "        # Pass the initial state dictionary\n",
        "        result = await app.ainvoke(initial_state)\n",
        "        print(\"\\nFinal Answer:\", result.get('final_answer', 'No answer generated'))\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error running KB agent: {e}\")\n",
        "        return {\"error_message\": str(e)} # Return state with error message\n",
        "\n",
        "\n",
        "# Run the example with your documents_list\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(run_kb_agent_example(documents_list))"
      ],
      "metadata": {
        "id": "j0YZzyqlsK7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "998e2b94"
      },
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "async def interactive_kb_agent(documents_list):\n",
        "    \"\"\"Allows interactive communication with the KB agent.\"\"\"\n",
        "\n",
        "    # Initialize the agent\n",
        "    agent = KnowledgeBaseAgent(documents_list=documents_list)\n",
        "\n",
        "    # Build the graph\n",
        "    app = await agent.build_rag_graph()\n",
        "\n",
        "    print(\"--- Interactive Knowledge Base Agent ---\")\n",
        "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
        "\n",
        "    session_id = \"interactive_session\" # Use a fixed session ID for simplicity\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query: \")\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Ending session.\")\n",
        "            break\n",
        "\n",
        "        # Prepare initial state\n",
        "        initial_state: KBState = {\n",
        "            \"query\": query,\n",
        "            \"messages\": [HumanMessage(content=query)],\n",
        "            \"session_id\": session_id,\n",
        "            \"iterations\": 0,\n",
        "            \"max_iterations\": 2, # Allow a couple of search iterations\n",
        "            \"retrieved_documents\": [],\n",
        "            \"kb_lookup_status\": \"not_attempted\",\n",
        "            \"web_search_required\": False,\n",
        "            \"escalation_required\": False,\n",
        "            \"retrieval_grade\": None,\n",
        "            \"subqueries\": [],\n",
        "            \"crawl_results\": [],\n",
        "            \"reflection_output\": None,\n",
        "            \"reflection_decision\": \"sufficient\", # Default decision\n",
        "            \"error_message\": None,\n",
        "            \"final_answer\": None,\n",
        "            \"use_reranker\": True, # Enable reranking by default\n",
        "            \"use_compressor\": True, # Enable compression by default\n",
        "            \"rerank_result\": [],\n",
        "            \"compress_result\": [],\n",
        "        }\n",
        "\n",
        "        print(\"Processing query...\")\n",
        "        try:\n",
        "            # Run the graph\n",
        "            result = await app.ainvoke(initial_state)\n",
        "            print(\"\\nAgent Response:\", result.get('final_answer', 'No answer generated'))\n",
        "            if result.get('error_message'):\n",
        "                print(\"Error:\", result['error_message'])\n",
        "            print(\"-\" * 30) # Separator for next query\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during processing: {e}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "# To run the interactive session, you would call:\n",
        "# asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f4ced85"
      },
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1e3dcf8"
      },
      "source": [
        "documents = {\"🌌 The Signal from Titan\": \"\"\"\n",
        "\n",
        "In the year 2145, humanity had colonies on Mars, mining stations in the asteroid belt, and research outposts circling Jupiter. But Titan, Saturn’s largest moon, was still a mystery. Its thick orange atmosphere hid secrets that no probe had ever fully explained.\n",
        "\n",
        "Captain Liora Vega was chosen to lead the first human crew to Titan. Her ship, the Aurora, carried six explorers, each trained in different sciences. The journey took three long years, but they were ready. Humanity wanted answers: Was Titan dead rock, or could something live in its frozen seas?\n",
        "\n",
        "When the Aurora entered Titan’s orbit, the crew gasped. The orange clouds swirled like painted fire. Rivers of liquid methane reflected Saturn’s pale light. It was beautiful but also alien—nothing like Earth.\n",
        "\n",
        "On the second day, while setting up their base, the crew received a signal. It was faint, rhythmic, and unlike any natural sound. “It’s not background noise,” said Arjun, the communications officer. “Someone—or something—is transmitting.”\n",
        "\n",
        "The crew debated. Some thought it was an echo from Saturn’s magnetosphere. Others whispered about aliens. Captain Vega decided to follow the signal. It led them to a frozen lake, covered in cracked ice. Underneath the surface, faint lights pulsed in perfect rhythm with the signal.\n",
        "\n",
        "Dr. Kiera, the biologist, lowered a drone beneath the ice. The camera showed strange glowing shapes, moving like schools of fish. “They’re alive,” she whispered. “And they’re intelligent.” The glowing beings changed patterns in response to the drone’s light, as if trying to talk.\n",
        "\n",
        "Suddenly, the ship’s computer detected a translation. The pulses formed repeating sequences, close to mathematical primes. It wasn’t just life—it was communication.\n",
        "\n",
        "For days, the crew worked to respond. They sent pulses of light, simple patterns, then sequences of numbers. The beings replied, faster each time. Arjun grew pale. “They’re smarter than we are. They learn quicker than our AI.”\n",
        "\n",
        "Then the beings sent an image—burning cities, dark skies, and oceans turning black. It wasn’t Titan. It looked like Earth.\n",
        "\n",
        "“Are they warning us?” Captain Vega asked. “Or is this what they did to their own world?”\n",
        "\n",
        "The crew argued deep into the night. Some said the beings were friends, offering protection. Others feared they were predators, showing power. The tension nearly tore the team apart.\n",
        "\n",
        "On the tenth day, the beings sent another image: a single human figure standing with glowing creatures beside them, facing the stars. It felt like an invitation.\n",
        "\n",
        "Captain Vega made her decision. “We came to explore, not to fear. If they wish to speak, we will answer.” She stepped onto the frozen lake, carrying a light beacon. The ice glowed under her boots. The beings rose, surrounding her in a circle of radiant blue.\n",
        "\n",
        "The signal grew louder, filling the air like a song. Vega’s body shook as the beacon pulsed in her hand. Then her voice came through the radio, calm but strange:\n",
        "\n",
        "“They are not from Titan. They travel between stars. They see us, and they want us to join. But they warn us: Earth is close to the same mistake they made. If we continue, our oceans will die too.”\n",
        "\n",
        "The crew was silent. Humanity had reached out into the stars—and the stars had answered, not with weapons, but with a warning.\n",
        "\n",
        "The Aurora left Titan months later, carrying data, recordings, and one message to all of Earth: Change, or perish.\n",
        "\n",
        "Captain Vega stood at the window of the ship, watching Saturn shrink into the black. She knew this was not the end, but the beginning of something greater—an alliance written in light, across the universe.\n",
        "\"\"\",\n",
        "\"🌌 The Signal from Titan — Episode 2: The Choice\":\"\"\"\n",
        "When the Aurora returned to Earth in 2149, the message from Titan spread faster than light across human networks. At first, leaders called it a hoax. Some claimed Captain Vega’s crew had gone mad. But the recordings were undeniable: glowing creatures, mathematical signals, and warnings of oceans turning black.\n",
        "\n",
        "The United Earth Council gathered in New Geneva. Scientists, politicians, and generals argued for weeks. “If they’re telling the truth,” said Dr. Alana Cho, “then Earth is closer to collapse than we think. Rising seas, poisoned air—it’s already happening.”\n",
        "\n",
        "But others were afraid. “What if the signal is a trick?” demanded Admiral Hart. “What if these beings lure us into a trap? We cannot risk the safety of humanity on glowing phantoms.”\n",
        "\n",
        "Meanwhile, ordinary people were divided too. Some marched in the streets, chanting “Listen to Titan!” Others carried signs saying “No alien lies!” Humanity, as always, struggled to agree.\n",
        "\n",
        "Captain Liora Vega and her crew became both heroes and targets. Newsfeeds replayed her every word, her calm face surrounded by blue light. Children drew glowing creatures in their notebooks. But dark rumors spread as well: that Vega was no longer human, that the beings had taken control of her mind.\n",
        "\n",
        "Despite the chaos, the message carried weight. Within months, the Council launched Project Horizon—a massive effort to clean Earth’s oceans, cut toxic fuels, and rebuild cities with alien efficiency. It was humanity’s first true attempt to heal its world.\n",
        "\n",
        "But then came a second signal.\n",
        "\n",
        "It was picked up by a listening post near Neptune. Unlike Titan’s calm rhythm, this one was sharp, erratic, almost like a warning siren. The data matched nothing known. The pulses carried one repeating sequence: a symbol that resembled a shattered star.\n",
        "\n",
        "When Vega saw it, her skin went cold. “It’s not them,” she whispered. “It’s someone else.”\n",
        "\n",
        "The Council panicked. Was this new signal a threat? An enemy of the Titan beings? Or perhaps the very reason they had warned Earth in the first place?\n",
        "\n",
        "The Aurora was ordered back into service. Vega and her crew were told to return to the outer system, track the new signal, and find the truth. Some of the crew hesitated. “We nearly tore ourselves apart last time,” Arjun said. “What if this new contact isn’t peaceful?”\n",
        "\n",
        "But Vega was resolute. “We sought the stars, and now the stars are answering. If we run, we will never stop running. If we face it, maybe we survive.”\n",
        "\n",
        "The Aurora launched once more, this time carrying not just explorers but diplomats, scientists, and even weapons—just in case. Humanity was learning that the universe was wider, stranger, and far more dangerous than they had ever imagined.\n",
        "\n",
        "Weeks later, as Saturn grew small behind them, the crew detected something vast near Neptune’s orbit. It wasn’t a planet, or a moon, or even a ship in the human sense. It was a structure—a web of black spires stretching across thousands of kilometers, absorbing sunlight like a wound in space.\\n\\nThe erratic signal came from its center.\\n\\n“Is it alive?” Kiera whispered.\\n\\n“No,” Vega said softly. “It’s a machine. And it’s waiting for us.”\\n\\nThe crew fell silent as the Aurora drifted closer. Lights flickered across the black web, like eyes opening in the dark.\\n\\nThe beings on Titan had offered a warning, a chance for survival. But this structure felt different. Cold. Hungry.\\n\\nFor the first time, Captain Vega wondered if humanity had been too late to listen.\\n\"\"\",\n",
        "\"🌌 The Signal from Titan — Episode 3: The Black Web\":\"\"\"\n",
        "The Aurora drifted into the shadow of the strange machine near Neptune. Its spires were blacker than space itself, absorbing starlight until they seemed to cut holes in reality.\\n\\n“Readings?” Vega asked, her voice steady though her chest was tight.\\n\\nArjun scanned his console. “No heat. No emissions except the signal. It’s… it’s not just broadcasting, it’s pulling. Like it’s searching for listeners.”\\n\\nThe ship shuddered. A ripple of static swept across every screen. The signal grew louder, not through the speakers but inside their minds—sharp pulses that made teeth ache and thoughts stutter.\\n\\n“It’s in my head,” Kiera whispered, gripping her temples.\\n\\n“Shut it down!” Vega ordered, but the Aurora’s systems were already flickering. Lights dimmed. Gravity wavered. Something vast had reached across space and taken hold of their ship.\\n\\nThen, suddenly, silence.\\n\\nOn the main screen, the black web shifted. Its spires folded like the ribs of a giant beast, and a circular opening appeared, glowing faintly with blue light. An invitation. Or a trap.\\n\\nDr. Cho’s hands trembled as she spoke. “This architecture is beyond anything we’ve imagined. If it’s a weapon, it could shatter Earth in seconds. But if it’s… communication…”\\n\\n“Captain,” Arjun said, “we can leave now. Report back, let the Council decide.”\\n\\nVega stared at the opening. In her mind, she saw Titan’s glowing creatures, their patient warning: Others are coming. The oceans will burn. Prepare.\\n\\nWhat if this was what they had feared?\\n\\n“No,” Vega said at last. “We go in.”\\n\\nThe Aurora inched forward. As they passed through the glowing circle, gravity shifted again. Outside, the stars vanished. They were no longer in Neptune’s orbit but inside a hollow space larger than any city on Earth. The walls of the black web curved upward like an endless cathedral, pulsing faintly with silver veins.\\n\\nAnd at the center floated a construct: a sphere of glass and shadow, rotating slowly, covered in symbols that shifted like living text.\\n\\n“It’s a library,” Dr. Cho breathed.\\n\\nOr a prison, Vega thought.\\n\\nThe sphere pulsed. A voice—no, not sound, but thought—spread through the crew:\\n\\nYOU ARE NOT THE FIRST.\\n\\nEveryone froze. The words weren’t in English, but they carried meaning, heavy and undeniable.\\n\\n“Who are you?” Vega whispered.\\n\\nWE ARE THE KEEPERS OF THE LAST LIGHT.\\n\\nThe crew exchanged nervous glances.\\n\\nWE HARVEST WHAT REMAINS.\\n\\nKiera’s eyes widened. “Harvest?” she mouthed.\\n\\nThe symbols across the sphere shifted again, replaying scenes that chilled them: oceans boiling, cities falling into ash, alien worlds torn apart. Species after species vanishing into silence.\\n\\nAnd each time, the black web grew larger.\\n\\n“They’re not warning us,” Arjun said, his voice breaking. “They’re feeding on endings.”\\n\\nThe thought-voice returned, heavier now:\\n\\nEARTH WILL JOIN THE PATTERN.\\n\\nThe crew reeled. Dr. Cho clutched Vega’s arm. “This is what Titan tried to warn us about! These things don’t save worlds—they consume them.”\\n\\nVega’s heart pounded. Titan had given them a choice: prepare or perish. And now she understood why. This machine wasn’t just a relic—it was the predator that followed civilizations to their graves.\\n\\nOn the viewscreen, the black spires began to fold inward. The opening was closing.\\n\\n“Captain,” Arjun shouted, “if we don’t move now, we’ll never get out!”\\n\\nVega’s mind raced. They could flee and warn Earth… but would Earth believe them this time? Or should they strike now, while inside the beast, risking everything to wound it before it reached the home they loved?\\n\\nThe ship trembled as the web shifted, hungry and closing. The crew waited for Vega’s command.\\n\\nShe took a breath.\\n\\nAnd made her choice.\\n\"\"\",\n",
        "\"🌌 The Signal from Titan — Episode 4: Fire in the Web\":\"\"\"\n",
        "The Aurora shook as the black spires closed around them. Vega gripped her chair, every nerve screaming at her to flee. But she forced herself to speak the words that sealed their path.\\n\\n“We strike.”\\n\\nThe crew looked at her in shock. Arjun’s hands froze above his console. “Captain, we’re in the heart of this thing! A single mistake and—”\\n\\n“Then we don’t make mistakes,” Vega snapped. “Target the veins. The silver lines—they’re alive. They feed the structure. If we cut them, maybe we can cripple it.”\\n\\nKiera’s face was pale, but she nodded and prepared the targeting system. Dr. Cho whispered, “If Titan’s warning was true, this is the predator. If we fall here, maybe Earth has a chance to fight back.”\\n\\nThe Aurora’s weapons hummed to life.\\n\\nThe black sphere pulsed with thought again, heavier now, shaking their minds like a storm:\\n\\nYOU CANNOT KILL THE PATTERN. YOU ARE ALREADY PART OF IT.\\n\\nVega ignored it. “Fire!”\\n\\nTwin lances of plasma shot from the Aurora, striking the silver veins. The chamber shook violently. For the first time, the alien structure screamed—not in sound, but in a pressure that twisted their bones. The silver veins flared white-hot, rupturing into cascading sparks.\\n\\nThe spires convulsed. The opening to space began to widen again, but not in invitation—this was pain, collapse.\\n\\n“They’re destabilizing!” Kiera shouted.\\n\\n“Again!” Vega ordered.\\n\\nArjun fired a second barrage. The central sphere flickered, its shifting symbols stuttering. Images of dying worlds dissolved into static.\\n\\nThen the thought-voice came again, but weaker, distorted:\\n\\nYOU… WILL… JOIN… US…\\n\\nThe sphere cracked. Shards of glass-like shadow floated outward, each one glowing with fragments of alien script.\\n\\nSuddenly, the Aurora was caught in a pull—an intense gravity dragging them toward the ruptured core.\\n\\n“We’re being sucked in!” Arjun cried.\\n\\n“Full thrusters!” Vega barked.\\n\\nEngines roared, but the pull was stronger. The Aurora tilted toward the collapsing sphere, alarms shrieking. Crew members were thrown from their seats. Sparks rained from the ceiling.\\n\\n“We won’t make it!” Kiera screamed.\\n\\nThen, from the chaos, a ripple of blue light cut across the chamber. The crew gasped as a familiar vision appeared—shapes like the glowing creatures from Titan, swimming through the void, their forms like waves of living starlight.\\n\\nThe Titan beings had returned.\\n\\nThey swirled around the Aurora, weaving currents of energy that pushed against the collapsing core. The ship lurched forward, breaking free from the pull.\\n\\n“They’re helping us!” Dr. Cho shouted.\\n\\nVega’s chest tightened. Titan had warned them. Titan had saved them. But why?\\n\\nThe alien lights pulsed once, like a heartbeat, then vanished into the collapsing structure.\\n\\nThe Aurora shot through the widening opening, escaping into the black sea of space. Behind them, the web folded inward, spires collapsing like a dying star, until nothing remained but silence.\\n\\nThe crew sat in stunned quiet, breathing hard, sweat dripping down their faces.\\n\\n“It’s gone,” Kiera whispered.\\n\\n“No,” Vega said softly, staring at the empty void. “Not gone. Wounded. And if it feeds on civilizations… there will be more of them. Somewhere out there.”\\n\\nThe thought chilled them more than the battle.\\n\\nDr. Cho finally broke the silence. “We have to tell Earth. Not just the Council—everyone. Humanity has to prepare. The Titan beings gave us time. We can’t waste it.”\\n\\nVega nodded slowly. Her choice had bought them survival, but the war had only begun.\\n\\nShe turned to her crew. “Set course for Earth. It’s time to wake the world.”\\n\"\"\"\n",
        "}\n",
        "\n",
        "# Convert the dictionary of documents into a list of Document objects\n",
        "documents_list = [LangchainDocument(page_content=text, metadata={\"title\": title}) for title, text in documents.items()]\n",
        "\n",
        "# Display the first document to show the format\n",
        "print(documents_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fead22c0"
      },
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09b11c0d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45c71795"
      },
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c16a975b"
      },
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "metadata": {
        "id": "Kiej_yqnPYd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "metadata": {
        "id": "-KM4QujgPaqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "metadata": {
        "id": "oa3lPrMwQafg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "# Assuming documents_list is already defined and populated from previous cells\n",
        "# Assuming KnowledgeBaseAgent and KBState classes are defined in previous cells\n",
        "asyncio.run(interactive_kb_agent(documents_list))"
      ],
      "metadata": {
        "id": "Y6Zqp3HmSI4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langgraph --quiet\n",
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import uuid\n",
        "import hashlib\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Optional, Set, Dict, Any, Literal, TypedDict, Annotated, Sequence\n",
        "from enum import Enum\n",
        "from operator import add\n",
        "\n",
        "from langchain_core.tools import structured_tool\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, model_validator\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper, GoogleSerperAPIWrapper\n",
        "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain_community.document_loaders import WebBaseLoader, AsyncChromiumLoader\n",
        "from langchain_community.document_transformers import Html2TextTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
        "from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun\n",
        "from langchain_community.embeddings import Embeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpointer import InMemorySaver\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "import httpx\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "load_dotenv()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -- Pydantic Schemas for Tool I/O --\n",
        "class DocumentSource(str, Enum):\n",
        "    KNOWLEDGE_BASE = \"knowledge_base\"\n",
        "    EXTERNAL_API = \"external_api\"\n",
        "    WEB_CRAWL = \"web_crawl\"\n",
        "    SEARCH_SNIPPET = \"search_snippet\"\n",
        "    CACHE = \"cache\"\n",
        "\n",
        "class ContentSource(str, Enum):\n",
        "    ACADEMIC = \"academic\"\n",
        "    GOVERNMENT = \"government\"\n",
        "    WIKI = \"wikipedia\"\n",
        "    GIT_REPO = \"git_repo\"\n",
        "    NEWS = \"news\"\n",
        "    BLOG = \"blog\"\n",
        "    REPORT = \"report\"\n",
        "    FORUM = \"forum\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "class DocumentResult(BaseModel):\n",
        "    page_content: str = Field(..., description=\"The document content.\")\n",
        "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Metadata associated with the document.\")\n",
        "    content_length: Optional[int] = Field(default=0, ge=0, description=\"Content length in characters.\")\n",
        "    source_type: DocumentSource = Field(default=DocumentSource.SEARCH_SNIPPET, description=\"The origin of the document content.\")\n",
        "    content_source: Optional[ContentSource] = Field(default=ContentSource.OTHER, description=\"The semantic category of the content.\")\n",
        "    relevance_score: Optional[float] = Field(default=None, ge=0.0, le=1.0, description=\"A score indicating relevance.\")\n",
        "    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
        "    document_id: Optional[str] = Field(default=None, description=\"Stable unique identifier for the document.\")\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def compute_content_length_and_id(self) -> \"DocumentResult\":\n",
        "        if self.page_content and (not self.content_length or self.content_length == 0):\n",
        "            object.__setattr__(self, \"content_length\", len(self.page_content))\n",
        "        if not self.document_id:\n",
        "            did = hashlib.sha256(self.page_content.encode(\"utf-8\")).hexdigest()\n",
        "            object.__setattr__(self, \"document_id\", did)\n",
        "        return self\n",
        "\n",
        "    def to_langchain(self):\n",
        "        from langchain_core.documents import Document\n",
        "        return Document(page_content=self.page_content, metadata=self.metadata, id=self.document_id)\n",
        "\n",
        "    @classmethod\n",
        "    def from_langchain(cls, doc: LangchainDocument, relevance_score: Optional[float] = None) -> \"DocumentResult\":\n",
        "        id_ = doc.id or hashlib.sha256(doc.page_content.encode(\"utf-8\")).hexdigest()\n",
        "        timestamp = doc.metadata.get(\"created_at\") or datetime.now(timezone.utc)\n",
        "        return cls(\n",
        "            page_content=doc.page_content,\n",
        "            metadata=doc.metadata,\n",
        "            relevance_score=relevance_score,\n",
        "            created_at=timestamp,\n",
        "            document_id=id_,\n",
        "        )\n",
        "\n",
        "class SearchMode(str, Enum):\n",
        "    SHALLOW = \"shallow\"\n",
        "    DEEP = \"deep\"\n",
        "\n",
        "class SearchEngine(str, Enum):\n",
        "    TAVILY = \"tavily\"\n",
        "    DUCKDUCKGO = \"duckduckgo\"\n",
        "    SERPER = \"serper\"\n",
        "    WIKIPEDIA = \"wikipedia\"\n",
        "    ARXIV = \"arxiv\"\n",
        "    SEMANTIC_SCHOLAR = \"semantic_scholar\"\n",
        "    CRAWL_ENGINE = \"crawl_engine\"\n",
        "    OTHER = \"other\"\n",
        "\n",
        "class SearchOutput(BaseModel):\n",
        "    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n",
        "    query: str = Field(...)\n",
        "    mode: SearchMode = Field(...)\n",
        "    engine: SearchEngine = Field(...)\n",
        "    results: List[DocumentResult] = Field(default_factory=list)\n",
        "    status: str = Field(default=\"success\")\n",
        "    error: Optional[str] = Field(None)\n",
        "    iteration: int = Field(default=0)\n",
        "    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n",
        "\n",
        "# -- Client Configuration --\n",
        "tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
        "tavily_client = TavilySearchAPIWrapper(tavily_api_key=tavily_api_key) if tavily_api_key else None\n",
        "ddg_tool = DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper())\n",
        "serper_api_key = os.environ.get(\"SERPER_API_KEY\")\n",
        "serper_client = GoogleSerperAPIWrapper(serper_api_key=serper_api_key) if serper_api_key else None\n",
        "\n",
        "# -- Toolbox Class with all tools --\n",
        "class SearchToolbox:\n",
        "    def __init__(self, embedding_model: Optional[Embeddings] = None):\n",
        "        self.seen_urls: Set[str] = set()\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    @structured_tool\n",
        "    async def web_search(self, queries: List[str], max_results_per_query: int = 5, mode: SearchMode = SearchMode.SHALLOW, iteration: int = 0) -> List[SearchOutput]:\n",
        "        async def _tavily_search_async(query: str, max_results: int):\n",
        "            if not tavily_client: return []\n",
        "            try:\n",
        "                results = await tavily_client.ainvoke({\"query\": query, \"max_results\": max_results})\n",
        "                return [dict(url=res.get(\"url\"), snippet=res.get(\"snippet\"), title=res.get(\"title\"), engine=\"TAVILY\", query=query) for res in results]\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Tavily search error: {e}\")\n",
        "                return []\n",
        "\n",
        "        async def _ddg_search_async(query: str):\n",
        "            try:\n",
        "                result_string = await ddg_tool.ainvoke(query)\n",
        "                data = json.loads(result_string)\n",
        "                return [dict(url=item.get(\"link\"), snippet=item.get(\"snippet\"), title=item.get(\"title\"), engine=\"DUCKDUCKGO\", query=query) for item in data]\n",
        "            except Exception as e:\n",
        "                logger.error(f\"DuckDuckGo search error: {e}\")\n",
        "                return []\n",
        "\n",
        "        async def _serper_search_async(query: str, max_results: int):\n",
        "            if not serper_client: return []\n",
        "            try:\n",
        "                results = await serper_client.ainvoke(query)\n",
        "                if isinstance(results, dict) and 'organic' in results:\n",
        "                    return [dict(url=item.get(\"link\"), snippet=item.get(\"snippet\"), title=item.get(\"title\"), engine=\"SERPER\", query=query) for item in results['organic'][:max_results]]\n",
        "                return []\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Serper search error: {e}\")\n",
        "                return []\n",
        "\n",
        "        all_search_outputs = []\n",
        "        for query in queries:\n",
        "            tasks = []\n",
        "            if tavily_client: tasks.append(_tavily_search_async(query, max_results_per_query))\n",
        "            tasks.append(_ddg_search_async(query))\n",
        "            if serper_client: tasks.append(_serper_search_async(query, max_results_per_query))\n",
        "\n",
        "            raw_results_list = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "            all_search_results = [res for sublist in raw_results_list if isinstance(sublist, list) for res in sublist]\n",
        "\n",
        "            document_results = [\n",
        "                DocumentResult(\n",
        "                    page_content=res['snippet'],\n",
        "                    metadata={\"title\": res['title'], \"url\": res['url'], \"engine\": res['engine']},\n",
        "                    source_type=DocumentSource.SEARCH_SNIPPET\n",
        "                ) for res in all_search_results if res['url'] not in self.seen_urls\n",
        "            ]\n",
        "            for doc in document_results:\n",
        "                self.seen_urls.add(doc.metadata.get(\"url\"))\n",
        "\n",
        "            for engine in {res['engine'] for res in all_search_results}:\n",
        "                engine_results = [doc for doc in document_results if doc.metadata.get('engine') == engine]\n",
        "                if engine_results:\n",
        "                    all_search_outputs.append(SearchOutput(query=query, mode=mode, engine=SearchEngine[engine], results=engine_results, iteration=iteration))\n",
        "\n",
        "        return all_search_outputs\n",
        "\n",
        "    @structured_tool\n",
        "    async def wikipedia_search(self, query: str) -> List[SearchOutput]:\n",
        "        api_wrapper = WikipediaAPIWrapper(top_k_results=1)\n",
        "        wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "        try:\n",
        "            lc_docs = await wikipedia_tool.ainvoke(query)\n",
        "            docs = [DocumentResult.from_langchain(doc) for doc in lc_docs]\n",
        "            for doc in docs: self.seen_urls.add(doc.metadata.get(\"url\", \"\"))\n",
        "            return [SearchOutput(query=query, mode=SearchMode.SHALLOW, engine=SearchEngine.WIKIPEDIA, results=docs)]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Wikipedia search failed: {e}\")\n",
        "            return [SearchOutput(query=query, mode=SearchMode.SHALLOW, engine=SearchEngine.WIKIPEDIA, status=\"failed\", error=str(e))]\n",
        "\n",
        "    @structured_tool\n",
        "    async def arxiv_search(self, query: str, load_max_docs: int = 3) -> List[SearchOutput]:\n",
        "        api_wrapper = ArxivAPIWrapper(load_max_docs=load_max_docs)\n",
        "        arxiv_tool = ArxivQueryRun(api_wrapper=api_wrapper)\n",
        "        try:\n",
        "            lc_docs = await arxiv_tool.ainvoke(query)\n",
        "            docs = [DocumentResult.from_langchain(doc) for doc in lc_docs]\n",
        "            for doc in docs: self.seen_urls.add(doc.metadata.get(\"url\", \"\"))\n",
        "            return [SearchOutput(query=query, mode=SearchMode.SHALLOW, engine=SearchEngine.ARXIV, results=docs)]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Arxiv search failed: {e}\")\n",
        "            return [SearchOutput(query=query, mode=SearchMode.SHALLOW, engine=SearchEngine.ARXIV, status=\"failed\", error=str(e))]\n",
        "\n",
        "    @structured_tool\n",
        "    async def semantic_scholar_search(self, query: str, top_k_results: int = 3) -> List[SearchOutput]:\n",
        "        api_wrapper = SemanticScholarAPIWrapper(top_k_results=top_k_results)\n",
        "        semantic_scholar_tool = SemanticScholarQueryRun(api_wrapper=api_wrapper)\n",
        "        try:\n",
        "            lc_docs = await semantic_scholar_tool.ainvoke(query)\n",
        "            docs = [DocumentResult.from_langchain(doc) for doc in lc_docs]\n",
        "            for doc in docs: self.seen_urls.add(doc.metadata.get(\"url\", \"\"))\n",
        "            return [SearchOutput(query=query, mode=SearchMode.SHALLOW, engine=SearchEngine.SEMANTIC_SCHOLAR, results=docs)]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Semantic Scholar search failed: {e}\")\n",
        "            return [SearchOutput(query=query, mode=SearchMode.SHALLOW, engine=SearchEngine.SEMANTIC_SCHOLAR, status=\"failed\", error=str(e))]\n",
        "\n",
        "    @structured_tool\n",
        "    async def web_crawl(self, urls: List[str], chunk_size: int = 2000, chunk_overlap: int = 200, mode: SearchMode = SearchMode.DEEP, iteration: int = 0) -> List[SearchOutput]:\n",
        "        to_crawl_urls = [url for url in urls if url not in self.seen_urls]\n",
        "        if not to_crawl_urls: return []\n",
        "\n",
        "        async def _crawl_single_url(url: str):\n",
        "            try:\n",
        "                loader = AsyncChromiumLoader([url]) if 'dynamic' in url else WebBaseLoader([url])\n",
        "                docs = await loader.aload()\n",
        "                if docs and docs[0].page_content:\n",
        "                    docs[0].metadata[\"loader\"] = \"chromium\" if 'dynamic' in url else \"web_base\"\n",
        "                    return docs[0]\n",
        "            except (httpx.TimeoutException, httpx.ConnectError, Exception) as e:\n",
        "                logger.error(f\"Failed to crawl {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "        tasks = [_crawl_single_url(url) for url in to_crawl_urls]\n",
        "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "        crawled_documents = [DocumentResult.from_langchain(doc) for doc in results if isinstance(doc, LangchainDocument)]\n",
        "        for doc in crawled_documents: self.seen_urls.add(doc.metadata.get(\"url\"))\n",
        "\n",
        "        if not crawled_documents: return [SearchOutput(query=\"[Crawl]\", mode=mode, engine=SearchEngine.CRAWL_ENGINE, status=\"failed\")]\n",
        "\n",
        "        chunks = await self.clean_html(crawled_documents, chunk_size, chunk_overlap)\n",
        "\n",
        "        return [SearchOutput(query=\"[Crawl]\", mode=mode, engine=SearchEngine.CRAWL_ENGINE, results=chunks, iteration=iteration)]\n",
        "\n",
        "    @structured_tool\n",
        "    async def clean_html(self, documents: List[DocumentResult], chunk_size: int = 2000, chunk_overlap: int = 200, use_semantic_chunking: bool = False) -> List[DocumentResult]:\n",
        "        if not documents: return []\n",
        "        lc_docs = [doc.to_langchain() for doc in documents]\n",
        "\n",
        "        def _sync_clean_and_chunk(docs, chunk_size, chunk_overlap, use_semantic_chunking, embedding_model):\n",
        "            try:\n",
        "                stripped_docs = Html2TextTransformer().transform_documents(docs)\n",
        "                if use_semantic_chunking and embedding_model:\n",
        "                    text_splitter = SemanticChunker(embedding_model)\n",
        "                    all_content = \" \".join([doc.page_content for doc in stripped_docs])\n",
        "                    chunks = text_splitter.create_documents([all_content])\n",
        "                else:\n",
        "                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "                    chunks = text_splitter.split_documents(stripped_docs)\n",
        "                return [DocumentResult.from_langchain(doc) for doc in chunks]\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Cleaning and chunking failed: {e}\")\n",
        "                return []\n",
        "\n",
        "        return await asyncio.to_thread(_sync_clean_and_chunk, lc_docs, chunk_size, chunk_overlap, use_semantic_chunking, self.embedding_model)\n",
        "\n",
        "# --- LangGraph Integration ---\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "toolbox = SearchToolbox(embedding_model=embedding_model)\n",
        "\n",
        "tools = [\n",
        "    toolbox.web_search,\n",
        "    toolbox.wikipedia_search,\n",
        "    toolbox.arxiv_search,\n",
        "    toolbox.semantic_scholar_search,\n",
        "    toolbox.web_crawl,\n",
        "    toolbox.clean_html,\n",
        "]\n",
        "model = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add]\n",
        "    search_results: Annotated[List[SearchOutput], add]\n",
        "\n",
        "def call_model(state: AgentState):\n",
        "    try:\n",
        "        response = model.invoke(state[\"messages\"])\n",
        "        return {\"messages\": [response]}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LLM invocation failed: {e}\")\n",
        "        return {\"messages\": [AIMessage(content=f\"Error: {e}\")]}\n",
        "\n",
        "tool_node = ToolNode(tools, handle_tool_errors=True)\n",
        "\n",
        "def process_tool_output(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    search_outputs = []\n",
        "    if last_message.tool_calls:\n",
        "        # Assuming tool outputs are directly attached to the ToolMessage\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            # The tool output is the value of the 'output' key in a ToolMessage\n",
        "            tool_output = tool_call.get('output')\n",
        "            if isinstance(tool_output, list) and all(isinstance(o, SearchOutput) for o in tool_output):\n",
        "                search_outputs.extend(tool_output)\n",
        "    return {\"search_results\": search_outputs}\n",
        "\n",
        "def should_continue(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    return \"tools\" if last_message.tool_calls else \"end\"\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"llm\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "workflow.add_node(\"process_output\", process_tool_output)\n",
        "\n",
        "workflow.add_edge(START, \"llm\")\n",
        "workflow.add_edge(\"tools\", \"llm\")\n",
        "workflow.add_edge(\"process_output\", \"llm\")\n",
        "\n",
        "workflow.add_conditional_edges(\"llm\", should_continue)\n",
        "workflow.set_finish_point(\"end\")\n",
        "\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# --- Example Usage ---\n",
        "async def main():\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "    response_stream = app.stream(\n",
        "        {\"messages\": [HumanMessage(content=\"Explain the recent trends in AI research, including a summary from a relevant Arxiv paper, and use semantic chunking to process the resulting text.\")]},\n",
        "        config,\n",
        "        stream_mode=\"values\"\n",
        "    )\n",
        "\n",
        "    async for state in response_stream:\n",
        "        print(state)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "dE0wmRGXUnnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\\n\\n\".join(doc.metadata.get(\"title\", \"Untitled\") + \"\\n\" + doc.page_content for doc in documents_list)"
      ],
      "metadata": {
        "id": "e46PVX_0P4bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDg0iAN5g1P1UxHPcVyZeJ3y_EpnrbJ8lQ\"\n",
        "import base64\n",
        "import mimetypes\n",
        "import os\n",
        "import re\n",
        "import struct\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def save_binary_file(file_name, data):\n",
        "    f = open(file_name, \"wb\")\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "    print(f\"File saved to to: {file_name}\")\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    model = \"gemini-2.5-flash-preview-tts\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\\n\\n\".join(doc.metadata.get(\"title\", \"Untitled\") + \"\\n\" + doc.page_content for doc in documents_list[1:4])),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        response_modalities=[\n",
        "            \"audio\",\n",
        "        ],\n",
        "        speech_config=types.SpeechConfig(\n",
        "            voice_config=types.VoiceConfig(\n",
        "                prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                    voice_name=\"Puck\"\n",
        "                )\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    file_index = 0\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        if (\n",
        "            chunk.candidates is None\n",
        "            or chunk.candidates[0].content is None\n",
        "            or chunk.candidates[0].content.parts is None\n",
        "        ):\n",
        "            continue\n",
        "        if chunk.candidates[0].content.parts[0].inline_data and chunk.candidates[0].content.parts[0].inline_data.data:\n",
        "            file_name = f\"ENTER_FILE_NAME_{file_index}\"\n",
        "            file_index += 1\n",
        "            inline_data = chunk.candidates[0].content.parts[0].inline_data\n",
        "            data_buffer = inline_data.data\n",
        "            file_extension = mimetypes.guess_extension(inline_data.mime_type)\n",
        "            if file_extension is None:\n",
        "                file_extension = \".wav\"\n",
        "                data_buffer = convert_to_wav(inline_data.data, inline_data.mime_type)\n",
        "            save_binary_file(f\"{file_name}{file_extension}\", data_buffer)\n",
        "        else:\n",
        "            print(chunk.text)\n",
        "\n",
        "def convert_to_wav(audio_data: bytes, mime_type: str) -> bytes:\n",
        "    \"\"\"Generates a WAV file header for the given audio data and parameters.\n",
        "\n",
        "    Args:\n",
        "        audio_data: The raw audio data as a bytes object.\n",
        "        mime_type: Mime type of the audio data.\n",
        "\n",
        "    Returns:\n",
        "        A bytes object representing the WAV file header.\n",
        "    \"\"\"\n",
        "    parameters = parse_audio_mime_type(mime_type)\n",
        "    bits_per_sample = parameters[\"bits_per_sample\"]\n",
        "    sample_rate = parameters[\"rate\"]\n",
        "    num_channels = 1\n",
        "    data_size = len(audio_data)\n",
        "    bytes_per_sample = bits_per_sample // 8\n",
        "    block_align = num_channels * bytes_per_sample\n",
        "    byte_rate = sample_rate * block_align\n",
        "    chunk_size = 36 + data_size  # 36 bytes for header fields before data chunk size\n",
        "\n",
        "    # http://soundfile.sapp.org/doc/WaveFormat/\n",
        "\n",
        "    header = struct.pack(\n",
        "        \"<4sI4s4sIHHIIHH4sI\",\n",
        "        b\"RIFF\",          # ChunkID\n",
        "        chunk_size,       # ChunkSize (total file size - 8 bytes)\n",
        "        b\"WAVE\",          # Format\n",
        "        b\"fmt \",          # Subchunk1ID\n",
        "        16,               # Subchunk1Size (16 for PCM)\n",
        "        1,                # AudioFormat (1 for PCM)\n",
        "        num_channels,     # NumChannels\n",
        "        sample_rate,      # SampleRate\n",
        "        byte_rate,        # ByteRate\n",
        "        block_align,      # BlockAlign\n",
        "        bits_per_sample,  # BitsPerSample\n",
        "        b\"data\",          # Subchunk2ID\n",
        "        data_size         # Subchunk2Size (size of audio data)\n",
        "    )\n",
        "    return header + audio_data\n",
        "\n",
        "def parse_audio_mime_type(mime_type: str) -> dict[str, int | None]:\n",
        "    \"\"\"Parses bits per sample and rate from an audio MIME type string.\n",
        "\n",
        "    Assumes bits per sample is encoded like \"L16\" and rate as \"rate=xxxxx\".\n",
        "\n",
        "    Args:\n",
        "        mime_type: The audio MIME type string (e.g., \"audio/L16;rate=24000\").\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with \"bits_per_sample\" and \"rate\" keys. Values will be\n",
        "        integers if found, otherwise None.\n",
        "    \"\"\"\n",
        "    bits_per_sample = 16\n",
        "    rate = 24000\n",
        "\n",
        "    # Extract rate from parameters\n",
        "    parts = mime_type.split(\";\")\n",
        "    for param in parts: # Skip the main type part\n",
        "        param = param.strip()\n",
        "        if param.lower().startswith(\"rate=\"):\n",
        "            try:\n",
        "                rate_str = param.split(\"=\", 1)[1]\n",
        "                rate = int(rate_str)\n",
        "            except (ValueError, IndexError):\n",
        "                # Handle cases like \"rate=\" with no value or non-integer value\n",
        "                pass # Keep rate as default\n",
        "        elif param.startswith(\"audio/L\"):\n",
        "            try:\n",
        "                bits_per_sample = int(param.split(\"L\", 1)[1])\n",
        "            except (ValueError, IndexError):\n",
        "                pass # Keep bits_per_sample as default if conversion fails\n",
        "\n",
        "    return {\"bits_per_sample\": bits_per_sample, \"rate\": rate}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate()\n"
      ],
      "metadata": {
        "id": "GGqcEictBcKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQM2xgMjRD5S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}